
# ê¸°ì¡´ì†ŒìŠ¤ - IndexFlatL2 ì‚¬ìš©ìœ¼ë¡œ ë‹¨ìˆœí•œ ë²¡í„° ì¸ë±ìŠ¤ì„. LangChainê³¼ í¬ë§·ì˜¤ë¥˜ ë°œìƒí•˜ì—¬ ì‹ ê·œ ì†ŒìŠ¤ ì‘ì„±í•¨ build_faiss_with_metadata.py

import os
import fitz  # PyMuPDF
from docx import Document
from sentence_transformers import SentenceTransformer
import faiss
import numpy as np
from dotenv import load_dotenv
import pickle

# í˜„ì¬ ìŠ¤í¬ë¦½íŠ¸ ê¸°ì¤€ ì ˆëŒ€ ê²½ë¡œ ì„¤ì •
BASE_DIR = os.path.dirname(os.path.abspath(__file__))
DOCS_DIR = os.path.join(BASE_DIR, "docs")
DB_DIR = os.path.join(BASE_DIR, "db")

# db ë””ë ‰í† ë¦¬ ì—†ìœ¼ë©´ ìƒì„±
os.makedirs(DB_DIR, exist_ok=True)


# 1. ë¬¸ì„œ ì½ê¸°
def read_docx(path):
    doc = Document(path)
    return "\n".join([para.text for para in doc.paragraphs if para.text.strip()])

def read_pdf(path):
    doc = fitz.open(path)
    text = ""
    for page_num, page in enumerate(doc, start=1):
        page_text = page.get_text()
        text += f"\n[Page {page_num}]\n{page_text}"
    return text

# 2. ë¬¸ì„œ ë¶ˆëŸ¬ì˜¤ê¸°
def load_documents(folder_path):
    documents = []
    for file in os.listdir(folder_path):
        file_path = os.path.join(folder_path, file)
        if file.endswith(".pdf"):
            text = read_pdf(file_path)
        elif file.endswith(".docx"):
            text = read_docx(file_path)
        else:
            continue
        documents.append({"filename": file, "text": text})
    return documents

# 3. ë¬¸ì¥ ë‹¨ìœ„ ë¶„í•  (ì¤„ë°”ê¿ˆ ê¸°ì¤€)
def split_documents(documents):
    chunks = []
    for doc in documents:
        lines = [line.strip() for line in doc["text"].split("\n") if len(line.strip()) > 20]
        for line in lines:
            chunks.append({
                "text": line,
                "source": doc["filename"]
            })
    return chunks

# 4. ë²¡í„° DB êµ¬ì¶•
def build_vector_index():
    documents = load_documents(DOCS_DIR)
    chunks = split_documents(documents)

    model = SentenceTransformer("all-MiniLM-L6-v2")
    texts = [chunk["text"] for chunk in chunks]
    embeddings = model.encode(texts, convert_to_numpy=True)

    index = faiss.IndexFlatL2(embeddings.shape[1])
    index.add(embeddings)

    # ì €ì¥ ê²½ë¡œë¥¼ db/ í•˜ìœ„ë¡œ ë³€ê²½
    faiss_index_path = os.path.join(DB_DIR, "faiss_index.idx")
    chunks_path = os.path.join(DB_DIR, "chunks.pkl")

    faiss.write_index(index, faiss_index_path)
    with open(chunks_path, "wb") as f:
        pickle.dump(chunks, f)

    print(f" FAISS ì¸ë±ìŠ¤ ì €ì¥ ì™„ë£Œ: {faiss_index_path}")
    print(f" ë¬¸ì¥ ë©”íƒ€ë°ì´í„° ì €ì¥ ì™„ë£Œ: {chunks_path}")


    print("ğŸ‰ ë²¡í„° DB ìƒì„± ì™„ë£Œ!")

# 5. ê²€ìƒ‰ ì˜ˆì‹œ
def search_query(query, top_k=3):
    # ëª¨ë¸, ì¸ë±ìŠ¤, chunks ë¶ˆëŸ¬ì˜¤ê¸°
    print(f"\nğŸ” ê²€ìƒ‰ì–´: {query}")
    model = SentenceTransformer("all-MiniLM-L6-v2")
    index = faiss.read_index(os.path.join(DB_DIR, "faiss_index.idxs"))
    with open(os.path.join(DB_DIR, "chunks.pkl"), "rb") as f:
        chunks = pickle.load(f)

    query_vec = model.encode([query], convert_to_numpy=True)
    distances, indices = index.search(query_vec, top_k)

    print("\nğŸ“Œ ê²€ìƒ‰ ê²°ê³¼:")
    for rank, idx in enumerate(indices[0], start=1):
        print(f"[{rank}] {chunks[idx]['text']} (ì¶œì²˜: {chunks[idx]['source']})")

# ë©”ì¸ ì‹¤í–‰
if __name__ == "__main__":
    build_vector_index()
    search_query("ê¸°ì¤€ì—´ëŸ‰ì´ ë¬´ì—‡ì¸ê°€ìš”?")