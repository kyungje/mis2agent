import os
import fitz  # PyMuPDF
from docx import Document
from langchain_community.vectorstores import FAISS
from langchain_community.embeddings import HuggingFaceEmbeddings
from langchain_core.documents import Document as LCDocument
import gc
import time
import re

# === ê²½ë¡œ ì„¤ì • ===
BASE_DIR = os.path.dirname(os.path.abspath(__file__))
DOCS_DIR = os.path.join(BASE_DIR, "docs")
DB_DIR = os.path.join(BASE_DIR, "db")
INDEX_FAISS_PATH = os.path.join(DB_DIR, "index.faiss")
INDEX_PKL_PATH = os.path.join(DB_DIR, "index.pkl")

# í•„ìš”í•œ í´ë” ìƒì„±
os.makedirs(DB_DIR, exist_ok=True)

# === ë¬¸ì„œ ì½ê¸° ===
def read_docx(path):
    doc = Document(path)
    text = "\n".join([para.text for para in doc.paragraphs if para.text.strip()])
    return text

def read_pdf(path):
    doc = fitz.open(path)
    text_by_page = []
    
    # í˜ì´ì§€ë³„ë¡œ ì²˜ë¦¬
    for page_num, page in enumerate(doc, start=1):
        try:
            # ë³´ë‹¤ êµ¬ì¡°í™”ëœ í…ìŠ¤íŠ¸ ì¶”ì¶œ ë°©ì‹ìœ¼ë¡œ ë³€ê²½
            blocks = page.get_text("blocks")
            page_text = ""
            for block in blocks:
                if block[6] == 0:  # í…ìŠ¤íŠ¸ ë¸”ë¡ë§Œ ì„ íƒ (ì´ë¯¸ì§€ ë¸”ë¡ ì œì™¸)
                    # ë¸”ë¡ í…ìŠ¤íŠ¸ì— ê³µë°± ì¶”ê°€
                    block_text = block[4]
                    # í•œê¸€ í…ìŠ¤íŠ¸ ì •ê·œí™”
                    block_text = normalize_korean_text(block_text)
                    page_text += block_text + "\n"
                    
            text_by_page.append((page_num, page_text))
        except Exception as e:
            print(f"í˜ì´ì§€ {page_num} ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {str(e)}")
    
    # ë©”ëª¨ë¦¬ í•´ì œ
    doc.close()
    return text_by_page

def normalize_korean_text(text):
    """í•œê¸€ í…ìŠ¤íŠ¸ ì •ê·œí™”: ë„ì–´ì“°ê¸° ë° ë¬¸ì¥ êµ¬ì¡° ê°œì„ """
    import re
    
    # 1. ê¸°ë³¸ ì •ê·œí™”
    text = re.sub(r'\s+', ' ', text).strip()
    
    # 2. í•œê¸€ ë‹¨ì–´ ì‚¬ì´ì— ë„ì–´ì“°ê¸° ì¶”ê°€
    # í•œê¸€ ììŒ+ëª¨ìŒ íŒ¨í„´ (ê°€-í£)
    text = re.sub(r'([ê°€-í£])([A-Za-z0-9])', r'\1 \2', text)  # í•œê¸€ ë’¤ì— ì˜ìˆ«ì
    text = re.sub(r'([A-Za-z0-9])([ê°€-í£])', r'\1 \2', text)  # ì˜ìˆ«ì ë’¤ì— í•œê¸€
    
    # 3. ë¶™ì–´ìˆëŠ” í•œê¸€ ë‹¨ì–´ë“¤ ì‚¬ì´ì— ë„ì–´ì“°ê¸° ì¶”ê°€ (íŒ¨í„´ ê¸°ë°˜)
    patterns = [
        (r'([.!?])([ê°€-í£])', r'\1 \2'),  # ë¬¸ì¥ ë¶€í˜¸ ë’¤ì— ë„ì–´ì“°ê¸°
        (r'([ê°€-í£])([(){}[\]<>])', r'\1 \2'),  # í•œê¸€ê³¼ ê´„í˜¸ ì‚¬ì´
        (r'([(){}[\]<>])([ê°€-í£])', r'\1 \2'),  # ê´„í˜¸ì™€ í•œê¸€ ì‚¬ì´
    ]
    
    for pattern, repl in patterns:
        text = re.sub(pattern, repl, text)
    
    # 4. íŠ¹ìˆ˜ íŒ¨í„´ ì²˜ë¦¬ (ì˜ˆ: ê°€ìŠ¤ê³µê¸‰ê·œì • -> ê°€ìŠ¤ ê³µê¸‰ ê·œì •)
    # 6ê¸€ì ì´ìƒ ì—°ì†ëœ í•œê¸€ì„ í™•ì¸í•˜ì—¬ 3~4ê¸€ì ë‹¨ìœ„ë¡œ ë¶„ë¦¬ ì‹œë„
    def split_long_korean(match):
        long_word = match.group(0)
        if len(long_word) >= 6:
            # ê¸´ ë‹¨ì–´ë¥¼ 3ê¸€ì ë‹¨ìœ„ë¡œ ë¶„ë¦¬ ì‹œë„
            parts = []
            i = 0
            while i < len(long_word):
                if i + 3 <= len(long_word):
                    parts.append(long_word[i:i+3])
                else:
                    parts.append(long_word[i:])
                i += 3
            return ' '.join(parts)
        return long_word
    
    text = re.sub(r'[ê°€-í£]{6,}', split_long_korean, text)
    
    return text

# === ë¬¸ì„œ ë¶ˆëŸ¬ì˜¤ê¸° ===
def load_document(file_path):
    """ë‹¨ì¼ ë¬¸ì„œë¥¼ ë¡œë“œí•©ë‹ˆë‹¤."""
    if file_path.endswith(".pdf"):
        return read_pdf(file_path)
    elif file_path.endswith(".docx"):
        text = read_docx(file_path)
        # DOCXëŠ” í˜ì´ì§€ ì •ë³´ê°€ ì—†ìœ¼ë¯€ë¡œ í•˜ë‚˜ì˜ í˜ì´ì§€ë¡œ ì·¨ê¸‰
        return [(1, text)]
    return []

# === ë¬¸ì¥ ë¶„í•  - ê°„ì†Œí™” ë²„ì „ ===
def split_text_to_chunks(text_by_page, source_name, chunk_size=100):
    """
    ë¬¸ì„œë¥¼ ë” ì‘ì€ ì²­í¬ë¡œ ë¶„í• í•©ë‹ˆë‹¤(ë‹¨ìˆœí™”ëœ ë²„ì „).
    """
    chunks = []
    
    for page_num, page_text in text_by_page:
        # í…ìŠ¤íŠ¸ ì •ê·œí™”: ë¶ˆí•„ìš”í•œ ê³µë°± ì œê±° ë° ì¤„ë°”ê¿ˆ í‘œì¤€í™”
        page_text = re.sub(r'\s+', ' ', page_text)
        
        # ì¤„ë°”ê¿ˆìœ¼ë¡œ ë¶„í• 
        lines = [line.strip() for line in page_text.split('\n') if line.strip()]
        
        current_chunk = ""
        
        # ê° ì¤„ì„ ì²˜ë¦¬
        for line in lines:
            # ì¤„ ì •ê·œí™”: ë‹¨ì–´ ì‚¬ì´ì— ê³µë°±ì´ ì—†ëŠ” ê²½ìš° ì¶”ê°€
            line = re.sub(r'([ê°€-í£])([A-Za-z0-9])', r'\1 \2', line)  # í•œê¸€ ë’¤ì— ì˜ìˆ«ì
            line = re.sub(r'([A-Za-z0-9])([ê°€-í£])', r'\1 \2', line)  # ì˜ìˆ«ì ë’¤ì— í•œê¸€
            
            # í˜„ì¬ ì¤„ ì¶”ê°€ ì‹œ chunk_sizeë¥¼ ì´ˆê³¼í•˜ëŠ”ì§€ í™•ì¸
            if len(current_chunk) + len(line) > chunk_size and current_chunk:
                # ì²­í¬ ì¶”ê°€ í›„ ìƒˆ ì²­í¬ ì‹œì‘
                chunks.append({
                    "text": current_chunk,
                    "source": source_name,
                    "page": page_num
                })
                current_chunk = line
            else:
                # í˜„ì¬ ì²­í¬ì— ì¤„ ì¶”ê°€
                if current_chunk:
                    current_chunk += " " + line
                else:
                    current_chunk = line
        
        # ë§ˆì§€ë§‰ ì²­í¬ ì¶”ê°€
        if current_chunk:
            chunks.append({
                "text": current_chunk,
                "source": source_name,
                "page": page_num
            })
    
    return chunks

# === ì¸ë±ìŠ¤ ìƒì„± - ë°°ì¹˜ ì²˜ë¦¬ ===
def build_langchain_vector_index():
    print("ğŸ“‚ ë¬¸ì„œ ë¡œë”© ë° ì¸ë±ì‹± ì‹œì‘...")
    
    # ë¬¸ì„œ ëª©ë¡ ê°€ì ¸ì˜¤ê¸°
    files = [f for f in os.listdir(DOCS_DIR) if f.endswith(('.pdf', '.docx'))]
    if not files:
        print("âŒ docs í´ë”ì— ìœ íš¨í•œ ë¬¸ì„œê°€ ì—†ìŠµë‹ˆë‹¤.")
        return
    
    print(f"ì´ {len(files)}ê°œ ë¬¸ì„œ ë°œê²¬")
    
    # ì„ë² ë”© ëª¨ë¸ ì´ˆê¸°í™”
    print("ì„ë² ë”© ëª¨ë¸ ì´ˆê¸°í™” ì¤‘...")
    embedding_model = HuggingFaceEmbeddings(model_name="sentence-transformers/all-MiniLM-L6-v2")
    
    # ê¸°ì¡´ ì¸ë±ìŠ¤ ì¡´ì¬ ì‹œ ë¡œë“œ, ì—†ìœ¼ë©´ ìƒˆë¡œ ìƒì„±
    if os.path.exists(os.path.join(DB_DIR, "index.faiss")):
        print("ê¸°ì¡´ ì¸ë±ìŠ¤ ë¡œë“œ ì¤‘...")
        vectorstore = FAISS.load_local(DB_DIR, embedding_model, allow_dangerous_deserialization=True)
    else:
        print("ìƒˆ ì¸ë±ìŠ¤ ìƒì„± ì¤‘...")
        # ë¹ˆ ë¬¸ì„œ ë¦¬ìŠ¤íŠ¸ë¡œ ì´ˆê¸°í™”
        vectorstore = FAISS.from_documents([
            LCDocument(page_content="ì´ˆê¸°í™” ë¬¸ì„œ", metadata={"source": "ì´ˆê¸°í™”", "page": 0})
        ], embedding_model)
    
    # íŒŒì¼ë³„ë¡œ ì²˜ë¦¬ (ë°°ì¹˜ ì²˜ë¦¬)
    total_chunks = 0
    for i, file_name in enumerate(files, 1):
        print(f"\nì²˜ë¦¬ ì¤‘: {file_name} ({i}/{len(files)})")
        file_path = os.path.join(DOCS_DIR, file_name)
        
        # ë‹¨ì¼ ë¬¸ì„œ ë¡œë“œ
        text_by_page = load_document(file_path)
        print(f"  - {len(text_by_page)}í˜ì´ì§€ ë¡œë“œë¨")
        
        # ë©”ëª¨ë¦¬ ê´€ë¦¬ë¥¼ ìœ„í•´ í…ìŠ¤íŠ¸ë¥¼ ì²­í¬ë¡œ ë¶„í• 
        chunks = split_text_to_chunks(text_by_page, file_name, chunk_size=100)
        print(f"  - {len(chunks)}ê°œ ì²­í¬ë¡œ ë¶„í• ë¨")
        total_chunks += len(chunks)
        
        # ë©”ëª¨ë¦¬ ìµœì í™”: í…ìŠ¤íŠ¸ í˜ì´ì§€ ë°ì´í„° í•´ì œ
        del text_by_page
        gc.collect()
        
        # ì²­í¬ë¥¼ ì‘ì€ ë°°ì¹˜ë¡œ ì²˜ë¦¬
        batch_size = 50  # í•œ ë²ˆì— ì²˜ë¦¬í•  ì²­í¬ ìˆ˜
        for batch_start in range(0, len(chunks), batch_size):
            batch_end = min(batch_start + batch_size, len(chunks))
            current_batch = chunks[batch_start:batch_end]
            
            print(f"  - ë°°ì¹˜ ì²˜ë¦¬ ì¤‘: {batch_start+1}~{batch_end}/{len(chunks)}")
            
            # í˜„ì¬ ë°°ì¹˜ë¥¼ LangChain ë¬¸ì„œë¡œ ë³€í™˜
            lc_documents = [
                LCDocument(
                    page_content=chunk["text"],
                    metadata={"source": chunk["source"], "page": chunk["page"]}
                )
                for chunk in current_batch
            ]
            
            # ê¸°ì¡´ ì¸ë±ìŠ¤ì— ì¶”ê°€
            vectorstore.add_documents(lc_documents)
            
            # ë©”ëª¨ë¦¬ ìµœì í™”
            del lc_documents
            gc.collect()
            
            # ì ì‹œ ëŒ€ê¸°í•˜ì—¬ ì‹œìŠ¤í…œì— ì—¬ìœ  ë¶€ì—¬
            time.sleep(0.1)
        
        # í˜„ì¬ íŒŒì¼ ì²˜ë¦¬ ì™„ë£Œ í›„ ì¸ë±ìŠ¤ ì €ì¥
        print("  - í˜„ì¬ê¹Œì§€ì˜ ì¸ë±ìŠ¤ ì €ì¥ ì¤‘...")
        vectorstore.save_local(DB_DIR)
        
        # ë°°ì¹˜ ì²˜ë¦¬ í›„ ë©”ëª¨ë¦¬ ì •ë¦¬
        del chunks
        gc.collect()
    
    print(f"\nâœ… ëª¨ë“  ë¬¸ì„œ ì²˜ë¦¬ ì™„ë£Œ. ì´ {total_chunks}ê°œ ì²­í¬ê°€ ì¸ë±ì‹±ë¨")
    print(f"ğŸ’¾ ìµœì¢… ì¸ë±ìŠ¤ ì €ì¥ ì™„ë£Œ â†’ {INDEX_FAISS_PATH}")
    print(f"ğŸ’¾ ë©”íƒ€ë°ì´í„° ì €ì¥ ì™„ë£Œ â†’ {INDEX_PKL_PATH}")
    print("ğŸ‰ LangChain í˜¸í™˜ ë²¡í„° DB ìƒì„± ì™„ë£Œ")

# === ê²€ìƒ‰ ì˜ˆì‹œ ===
def search_query(query, top_k=5):
    print(f"\nğŸ” ê²€ìƒ‰ì–´: {query}")
    embedding_model = HuggingFaceEmbeddings(model_name="sentence-transformers/all-MiniLM-L6-v2")
    
    # pickle íŒŒì¼ ë¡œë”© í—ˆìš©
    vectorstore = FAISS.load_local(DB_DIR, embedding_model, allow_dangerous_deserialization=True)

    results = vectorstore.similarity_search(query, k=top_k)

    print("\nğŸ“Œ ê²€ìƒ‰ ê²°ê³¼:")
    for i, doc in enumerate(results, start=1):
        print(f"[{i}] {doc.page_content}")
        
        # ë©”íƒ€ë°ì´í„° ì•ˆì „í•˜ê²Œ ì¶œë ¥
        source = doc.metadata.get('source', 'ì•Œ ìˆ˜ ì—†ìŒ')
        page = doc.metadata.get('page', 'ì•Œ ìˆ˜ ì—†ìŒ')
        print(f"     â¤· ì¶œì²˜: {source}, í˜ì´ì§€: {page}")
        print("-" * 60)


# === ë©”ì¸ ì‹¤í–‰ ===
if __name__ == "__main__":
    build_langchain_vector_index()
    
    # ì¸ë±ìŠ¤ ìƒì„± ì™„ë£Œ í›„ ì˜ˆì‹œ ê²€ìƒ‰ ìˆ˜í–‰
    search_query("ë„ì‹œê°€ìŠ¤ ê³µê¸‰ ê·œì •ì—ì„œ ë„ì‹œê°€ìŠ¤íšŒì‚¬ì˜ ì˜ë¬´ëŠ” ë¬´ì—‡ì¸ê°€ìš”?")
    search_query("ê°€ìŠ¤ìš”ê¸ˆì€ ì–´ë–»ê²Œ ê³„ì‚°ë˜ë‚˜ìš”?")
