import os
import fitz  # PyMuPDF
from docx import Document
from langchain_community.vectorstores import FAISS
from langchain_openai import OpenAIEmbeddings
from langchain_core.documents import Document as LCDocument
from langchain_text_splitters import RecursiveCharacterTextSplitter
import gc
import time
import re
from typing import List
from dotenv import load_dotenv

# .env íŒŒì¼ ë¡œë“œ
load_dotenv()

# === ê²½ë¡œ ì„¤ì • ===
BASE_DIR = os.path.dirname(os.path.abspath(__file__))
DOCS_DIR = os.path.join(BASE_DIR, "docs")
DB_DIR = os.path.join(BASE_DIR, "db")

# ì¸ë±ìŠ¤ ë¶„ë¥˜ë³„ ê²½ë¡œ ì„¤ì •
GAS_INDEX_DIR = os.path.join(DB_DIR, "gas_index")
POWER_INDEX_DIR = os.path.join(DB_DIR, "power_index")
OTHER_INDEX_DIR = os.path.join(DB_DIR, "other_index")

# í•„ìš”í•œ í´ë” ìƒì„±
os.makedirs(DB_DIR, exist_ok=True)
os.makedirs(GAS_INDEX_DIR, exist_ok=True)
os.makedirs(POWER_INDEX_DIR, exist_ok=True)
os.makedirs(OTHER_INDEX_DIR, exist_ok=True)

# === OpenAI ì„ë² ë”© ëª¨ë¸ ì´ˆê¸°í™” ===
def create_embedding_model():
    """ë™ì ìœ¼ë¡œ ë°°ì¹˜ í¬ê¸°ë¥¼ ì¡°ì •í•˜ì—¬ ì„ë² ë”© ëª¨ë¸ì„ ìƒì„±í•©ë‹ˆë‹¤."""
    # ì´ˆê¸° ë°°ì¹˜ í¬ê¸°
    chunk_size = 1000
    
    try:
        embedding_model = OpenAIEmbeddings(
            model="text-embedding-3-small",
            chunk_size=chunk_size,
            max_retries=3
        )
        return embedding_model
    except Exception as e:
        if "max_tokens_per_request" in str(e):
            # í† í° ì œí•œ ì˜¤ë¥˜ ì‹œ ë°°ì¹˜ í¬ê¸° ì¤„ì„
            chunk_size = 500
            print(f"í† í° ì œí•œìœ¼ë¡œ ì¸í•´ ë°°ì¹˜ í¬ê¸°ë¥¼ {chunk_size}ë¡œ ì¡°ì •í•©ë‹ˆë‹¤.")
            return OpenAIEmbeddings(
                model="text-embedding-3-small",
                chunk_size=chunk_size,
                max_retries=3
            )
        else:
            raise e

embedding_model = create_embedding_model()

# === í…ìŠ¤íŠ¸ ë¶„í• ê¸° ì´ˆê¸°í™” ===
text_splitter = RecursiveCharacterTextSplitter(
    chunk_size=400,
    chunk_overlap=80,
    length_function=len,
    separators=["\n\n", "\n", " ", ""]
)

# === ë¬¸ì„œ ì½ê¸° ===
def read_docx(path):
    """DOCX ë¬¸ì„œë¥¼ ì½ì–´ì„œ í…ìŠ¤íŠ¸ë¥¼ ì¶”ì¶œí•©ë‹ˆë‹¤."""
    doc = Document(path)
    text = "\n".join([para.text for para in doc.paragraphs if para.text.strip()])
    print(f"[DOCX] {path} ì¶”ì¶œ í…ìŠ¤íŠ¸ ì˜ˆì‹œ:", text[:100])
    return text

def read_pdf(path):
    """PDF ë¬¸ì„œë¥¼ ì½ì–´ì„œ ì „ì²´ í…ìŠ¤íŠ¸ë¥¼ ì¶”ì¶œí•©ë‹ˆë‹¤."""
    doc = fitz.open(path)
    full_text = ""
    
    # í˜ì´ì§€ë³„ë¡œ ì²˜ë¦¬
    for page_num, page in enumerate(doc, start=1):
        try:
            # ë³´ë‹¤ êµ¬ì¡°í™”ëœ í…ìŠ¤íŠ¸ ì¶”ì¶œ ë°©ì‹ìœ¼ë¡œ ë³€ê²½
            blocks = page.get_text("blocks")
            page_text = ""
            for block in blocks:
                if block[6] == 0:  # í…ìŠ¤íŠ¸ ë¸”ë¡ë§Œ ì„ íƒ (ì´ë¯¸ì§€ ë¸”ë¡ ì œì™¸)
                    # ë¸”ë¡ í…ìŠ¤íŠ¸ì— ê³µë°± ì¶”ê°€
                    block_text = block[4]
                    # í•œê¸€ í…ìŠ¤íŠ¸ ì •ê·œí™”
                    block_text = normalize_text(block_text)
                    page_text += block_text + "\n"
                    
            full_text += page_text + "\n"
        except Exception as e:
            print(f"í˜ì´ì§€ {page_num} ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {str(e)}")
    
    # ë©”ëª¨ë¦¬ í•´ì œ
    doc.close()
    if full_text:
        print(f"[PDF] {path} í…ìŠ¤íŠ¸ ì˜ˆì‹œ:", full_text[:100])
    return full_text

# === í…ìŠ¤íŠ¸ ì •ê·œí™” ===
def normalize_text(text):
    """í…ìŠ¤íŠ¸ ì •ê·œí™” (í•œê¸€-ì˜ë¬¸ ê³µë°±, íŠ¹ìˆ˜ë¬¸ì ì •ë¦¬)"""
    # í•œê¸€-ì˜ë¬¸ ê³µë°± ì •ë¦¬
    text = re.sub(r'([ê°€-í£])([A-Za-z0-9])', r'\1 \2', text)
    text = re.sub(r'([A-Za-z0-9])([ê°€-í£])', r'\1 \2', text)
    
    # ì—°ì†ëœ ê³µë°± ì •ë¦¬
    text = re.sub(r'\s+', ' ', text)
    
    # íŠ¹ìˆ˜ë¬¸ì ì •ë¦¬ (ë¬¸ì¥ë¶€í˜¸, ê´„í˜¸ ë“± ìœ ì§€)
    text = re.sub(r'[^\w\s\.\,\!\?\;\:\-\(\)\[\]\{\}]', '', text)
    
    return text.strip()

# === íŒŒì¼ ë¶„ë¥˜ í•¨ìˆ˜ ===
def classify_file(file_name):
    """íŒŒì¼ëª…ì— ë”°ë¼ ë¶„ë¥˜ë¥¼ ê²°ì •í•©ë‹ˆë‹¤."""
    if 'ë„ì‹œê°€ìŠ¤' in file_name:
        return 'gas'
    elif 'ì „ë ¥' in file_name:
        return 'power'
    else:
        return 'other'

def get_index_dir(category):
    """ë¶„ë¥˜ì— ë”°ë¥¸ ì¸ë±ìŠ¤ ë””ë ‰í† ë¦¬ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤."""
    if category == 'gas':
        return GAS_INDEX_DIR
    elif category == 'power':
        return POWER_INDEX_DIR
    else:
        return OTHER_INDEX_DIR

# === ë¬¸ì„œ ì²˜ë¦¬ ë° ì²­í‚¹ ===
def process_document(file_path):
    """ë¬¸ì„œë¥¼ ì½ê³  LangChain í…ìŠ¤íŠ¸ ë¶„í• ê¸°ë¥¼ ì‚¬ìš©í•˜ì—¬ ì²­í‚¹í•©ë‹ˆë‹¤."""
    filename = os.path.basename(file_path)
    
    # ë¬¸ì„œ ì½ê¸°
    if file_path.endswith(".pdf"):
        text = read_pdf(file_path)
    elif file_path.endswith(".docx"):
        text = read_docx(file_path)
    else:
        return []
    
    # í…ìŠ¤íŠ¸ ì •ê·œí™”
    text = normalize_text(text)
    
    # LangChain í…ìŠ¤íŠ¸ ë¶„í• ê¸° ì‚¬ìš©
    chunks = text_splitter.split_text(text)
    
    # ë©”íƒ€ë°ì´í„°ê°€ í¬í•¨ëœ LangChain ë¬¸ì„œë¡œ ë³€í™˜
    documents = []
    for i, chunk in enumerate(chunks):
        if len(chunk.strip()) > 50:  # ìµœì†Œ ê¸¸ì´ í•„í„° (50ì ì´ìƒ)
            doc = LCDocument(
                page_content=chunk,
                metadata={
                    "source": filename,
                    "chunk_id": i,
                    "file_path": file_path
                }
            )
            documents.append(doc)
    
    print(f"[CHUNKS] {filename} â†’ {len(documents)}ê°œ ìƒì„±")
    return documents

# === ë²¡í„° ì¸ë±ìŠ¤ êµ¬ì¶• ===
def build_langchain_vector_index():
    """ë¬¸ì„œë“¤ì„ ì½ì–´ì„œ ë¶„ë¥˜ë³„ë¡œ ë²¡í„° ì¸ë±ìŠ¤ë¥¼ êµ¬ì¶•í•©ë‹ˆë‹¤."""
    print("ğŸ“‚ ë¬¸ì„œ ì¸ë±ì‹± ì‹œì‘")

    # ë¬¸ì„œ ëª©ë¡ ë¶ˆëŸ¬ì˜¤ê¸°
    files = [f for f in os.listdir(DOCS_DIR) if f.endswith((".pdf", ".docx"))]
    if not files:
        print("âŒ ì¸ë±ì‹±í•  ë¬¸ì„œ ì—†ìŒ")
        return

    print(f"ì´ {len(files)}ê°œ ë¬¸ì„œ ë°œê²¬")

    # ë¶„ë¥˜ë³„ë¡œ ë¬¸ì„œ ê·¸ë£¹í™”
    gas_documents = []
    power_documents = []
    other_documents = []
    
    total_files = len(files)

    # ë¬¸ì„œë³„ë¡œ ì²˜ë¦¬ ë° ë¶„ë¥˜
    for i, file_name in enumerate(files, 1):
        file_path = os.path.join(DOCS_DIR, file_name)
        print(f"[{i}/{total_files}] ì²˜ë¦¬ ì¤‘: {file_name}")
        
        # íŒŒì¼ ë¶„ë¥˜
        category = classify_file(file_name)
        print(f"  â†’ ë¶„ë¥˜: {category}")
        
        documents = process_document(file_path)
        
        # ë¶„ë¥˜ë³„ë¡œ ë¬¸ì„œ ì¶”ê°€
        if category == 'gas':
            gas_documents.extend(documents)
        elif category == 'power':
            power_documents.extend(documents)
        else:
            other_documents.extend(documents)
        
        # ë©”ëª¨ë¦¬ ìµœì í™”
        del documents
        gc.collect()
        time.sleep(0.1)  # ì‹œìŠ¤í…œ ì—¬ìœ  ì£¼ê¸°

    print(f"\nğŸ“Š ì „ì²´ ë¬¸ì„œ ìˆ˜: {len(files)}ê°œ")
    print(f"ğŸ”– Gas ë¬¸ì„œ: {len(gas_documents)}ê°œ")
    print(f"ğŸ”– Power ë¬¸ì„œ: {len(power_documents)}ê°œ")
    print(f"ğŸ”– Other ë¬¸ì„œ: {len(other_documents)}ê°œ")

    # ë¶„ë¥˜ë³„ë¡œ ì¸ë±ìŠ¤ ìƒì„±
    categories = [
        ('gas', gas_documents, 'Gas'),
        ('power', power_documents, 'Power'),
        ('other', other_documents, 'Other')
    ]
    
    for category, documents, category_name in categories:
        if len(documents) == 0:
            print(f"âš ï¸ {category_name} ë¬¸ì„œê°€ ì—†ì–´ ì¸ë±ìŠ¤ë¥¼ ìƒì„±í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")
            continue
            
        print(f"\nğŸ”§ {category_name} ì¸ë±ìŠ¤ ìƒì„± ì¤‘... (ë¬¸ì„œ ìˆ˜: {len(documents)}ê°œ)")
        
        # ë°°ì¹˜ ë‹¨ìœ„ë¡œ ì„ë² ë”© ì²˜ë¦¬
        try:
            vectorstore = FAISS.from_documents(documents, embedding_model)
        except Exception as e:
            if "max_tokens_per_request" in str(e):
                print(f"í† í° ì œí•œìœ¼ë¡œ ì¸í•´ ë°°ì¹˜ í¬ê¸°ë¥¼ 500ìœ¼ë¡œ ì¡°ì •í•©ë‹ˆë‹¤...")
                # ë‘ ë²ˆì§¸ ì‹œë„: 500
                try:
                    medium_embedding_model = OpenAIEmbeddings(
                        model="text-embedding-3-small",
                        chunk_size=500,
                        max_retries=3
                    )
                    vectorstore = FAISS.from_documents(documents, medium_embedding_model)
                except Exception as e2:
                    if "max_tokens_per_request" in str(e2):
                        print(f"í† í° ì œí•œìœ¼ë¡œ ì¸í•´ ë°°ì¹˜ í¬ê¸°ë¥¼ 100ìœ¼ë¡œ ì¡°ì •í•©ë‹ˆë‹¤...")
                        # ìµœí›„ ìˆ˜ë‹¨: 100
                        small_embedding_model = OpenAIEmbeddings(
                            model="text-embedding-3-small",
                            chunk_size=100,
                            max_retries=3
                        )
                        vectorstore = FAISS.from_documents(documents, small_embedding_model)
                    else:
                        raise e2
            else:
                raise e

        # ì¸ë±ìŠ¤ ì €ì¥
        index_dir = get_index_dir(category)
        try:
            vectorstore.save_local(index_dir)
            print(f"ğŸ’¾ {category_name} ì¸ë±ìŠ¤ ì €ì¥ ì™„ë£Œ: {index_dir}")
        except Exception as e:
            print(f"âŒ {category_name} ì¸ë±ìŠ¤ ì €ì¥ ì˜¤ë¥˜: {e}")

    print("\nğŸ‰ ëª¨ë“  ë¶„ë¥˜ë³„ ì¸ë±ìŠ¤ ìƒì„± ì™„ë£Œ")

# === ê²€ìƒ‰ ì‹œìŠ¤í…œ ===
def search_query(query, top_k=10, category=None):
    """ë²¡í„° ì¸ë±ìŠ¤ì—ì„œ ìœ ì‚¬í•œ ë¬¸ì„œë¥¼ ê²€ìƒ‰í•©ë‹ˆë‹¤."""
    print(f"\nğŸ” ê²€ìƒ‰ì–´: {query}")
    
    # ê²€ìƒ‰í•  ì¸ë±ìŠ¤ ê²°ì •
    if category is None:
        # ëª¨ë“  ì¸ë±ìŠ¤ì—ì„œ ê²€ìƒ‰
        search_categories = ['gas', 'power', 'other']
        all_results = []
        
        for cat in search_categories:
            index_dir = get_index_dir(cat)
            index_faiss_path = os.path.join(index_dir, "index.faiss")
            
            if os.path.exists(index_faiss_path):
                try:
                    print(f"ğŸ“‚ {cat} ì¸ë±ìŠ¤ì—ì„œ ê²€ìƒ‰ ì¤‘...")
                    vectorstore = FAISS.load_local(index_dir, embedding_model, allow_dangerous_deserialization=True)
                    results = vectorstore.similarity_search_with_score(query, k=5)
                    all_results.extend(results)
                    print(f"  â†’ {cat} ì¸ë±ìŠ¤ì—ì„œ {len(results)}ê°œ ê²°ê³¼ ë°œê²¬")
                except Exception as e:
                    print(f"âš ï¸ {cat} ì¸ë±ìŠ¤ ë¡œë“œ ì‹¤íŒ¨: {e}")
            else:
                print(f"â„¹ï¸ {cat} ì¸ë±ìŠ¤ê°€ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤. (ê±´ë„ˆëœ€)")
        
        # ëª¨ë“  ê²°ê³¼ë¥¼ ì ìˆ˜ë¡œ ì •ë ¬
        if all_results:
            all_results = sorted(all_results, key=lambda x: x[1])
            results = all_results[:top_k]
        else:
            print("âŒ ê²€ìƒ‰ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return
        
    else:
        # íŠ¹ì • ì¹´í…Œê³ ë¦¬ì—ì„œë§Œ ê²€ìƒ‰
        index_dir = get_index_dir(category)
        index_faiss_path = os.path.join(index_dir, "index.faiss")
        
        if not os.path.exists(index_faiss_path):
            print(f"âŒ {category} ì¸ë±ìŠ¤ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {index_faiss_path}")
            return
            
        try:
            vectorstore = FAISS.load_local(index_dir, embedding_model, allow_dangerous_deserialization=True)
            results = vectorstore.similarity_search_with_score(query, k=top_k)
            results = sorted(results, key=lambda x: x[1])  # ë‚®ì€ ì ìˆ˜ì¼ìˆ˜ë¡ ìœ ì‚¬ë„ ë†’ìŒ
        except Exception as e:
            print(f"âŒ {category} ì¸ë±ìŠ¤ ë¡œë“œ ì‹¤íŒ¨: {e}")
            return

    print(f"âœ… ê²€ìƒ‰ ì™„ë£Œ (ì´ {len(results)}ê°œ ê²°ê³¼)")

    print("\nğŸ“Œ ê²€ìƒ‰ ê²°ê³¼:")
    for i, (doc, score) in enumerate(results, start=1):
        print(f"[{i}] ì ìˆ˜: {score:.4f}")
        # ê²€ìƒ‰ì–´ ì£¼ë³€ ì»¨í…ìŠ¤íŠ¸ë¥¼ í¬í•¨í•˜ì—¬ ì¶œë ¥
        content = doc.page_content
        if query in content:
            idx = content.find(query)
            start = max(0, idx - 150)  # ì»¨í…ìŠ¤íŠ¸ ë²”ìœ„ í™•ì¥
            end = min(len(content), idx + 150)  # ì»¨í…ìŠ¤íŠ¸ ë²”ìœ„ í™•ì¥
            print(f"...{content[start:end]}...")
        else:
            # ê²€ìƒ‰ì–´ê°€ ì—†ëŠ” ê²½ìš°, ë¬¸ì¥ ë‹¨ìœ„ë¡œ ì¶œë ¥
            sentences = content.split('.')
            print('. '.join(sentences[:3]) + '.')  # ì²˜ìŒ 3ê°œ ë¬¸ì¥ë§Œ ì¶œë ¥
        print(f"     â¤· ì¶œì²˜: {doc.metadata.get('source')} / ì²­í¬ ID: {doc.metadata.get('chunk_id')}")
        print("-" * 60)

# === ë©”ì¸ ì‹¤í–‰ ===
if __name__ == "__main__":
    build_langchain_vector_index()
    
    # ì¸ë±ìŠ¤ ìƒì„± ì™„ë£Œ í›„ ì˜ˆì‹œ ê²€ìƒ‰ ìˆ˜í–‰
    print("\n" + "="*60)
    print("ğŸ” ê²€ìƒ‰ í…ŒìŠ¤íŠ¸")
    print("="*60)
    
    # ì „ì²´ ê²€ìƒ‰
    search_query("ê¸°ì¤€ì—´ëŸ‰")
    
    # ë¶„ë¥˜ë³„ ê²€ìƒ‰ ì˜ˆì‹œ
    #search_query("ë„ì‹œê°€ìŠ¤ ê³µê¸‰ ê·œì •ì—ì„œ ë„ì‹œê°€ìŠ¤íšŒì‚¬ì˜ ì˜ë¬´ëŠ” ë¬´ì—‡ì¸ê°€ìš”?", category='gas')
    #search_query("ê°€ìŠ¤ìš”ê¸ˆì€ ì–´ë–»ê²Œ ê³„ì‚°ë˜ë‚˜ìš”?", category='gas')
    #search_query("ì „ë ¥ ê³µê¸‰ ê·œì •", category='power')
    #search_query("ê¸°íƒ€ ë¬¸ì„œ ê²€ìƒ‰", category='other')
