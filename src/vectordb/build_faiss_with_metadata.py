import os
import fitz  # PyMuPDF
from docx import Document
from langchain_community.vectorstores import FAISS
from langchain_community.embeddings import HuggingFaceEmbeddings
from langchain_core.documents import Document as LCDocument
import gc
import time
import re

# === ê²½ë¡œ ì„¤ì • ===
BASE_DIR = os.path.dirname(os.path.abspath(__file__))
DOCS_DIR = os.path.join(BASE_DIR, "docs")
DB_DIR = os.path.join(BASE_DIR, "db")
INDEX_FAISS_PATH = os.path.join(DB_DIR, "index.faiss")
INDEX_PKL_PATH = os.path.join(DB_DIR, "index.pkl")

# í•„ìš”í•œ í´ë” ìƒì„±
os.makedirs(DB_DIR, exist_ok=True)

# === ë¬¸ì„œ ì½ê¸° ===
def read_docx(path):
    doc = Document(path)
    text = "\n".join([para.text for para in doc.paragraphs if para.text.strip()])
    return [(1, text)]  # í˜ì´ì§€ ë²ˆí˜¸ë¥¼ 1ë¡œ ì§€ì •í•œ íŠœí”Œ ë¦¬ìŠ¤íŠ¸ ë°˜í™˜

def read_pdf(path):
    doc = fitz.open(path)
    text_by_page = []
    
    # í˜ì´ì§€ë³„ë¡œ ì²˜ë¦¬
    for page_num, page in enumerate(doc, start=1):
        try:
            # ë³´ë‹¤ êµ¬ì¡°í™”ëœ í…ìŠ¤íŠ¸ ì¶”ì¶œ ë°©ì‹ìœ¼ë¡œ ë³€ê²½
            blocks = page.get_text("blocks")
            page_text = ""
            for block in blocks:
                if block[6] == 0:  # í…ìŠ¤íŠ¸ ë¸”ë¡ë§Œ ì„ íƒ (ì´ë¯¸ì§€ ë¸”ë¡ ì œì™¸)
                    # ë¸”ë¡ í…ìŠ¤íŠ¸ì— ê³µë°± ì¶”ê°€
                    block_text = block[4]
                    # í•œê¸€ í…ìŠ¤íŠ¸ ì •ê·œí™”
                    block_text = normalize_korean_text(block_text)
                    page_text += block_text + "\n"
                    
            text_by_page.append((page_num, page_text))
        except Exception as e:
            print(f"í˜ì´ì§€ {page_num} ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {str(e)}")
    
    # ë©”ëª¨ë¦¬ í•´ì œ
    doc.close()
    return text_by_page

def normalize_korean_text(text):
    """í•œê¸€ í…ìŠ¤íŠ¸ ì •ê·œí™”: ë„ì–´ì“°ê¸° ë° ë¬¸ì¥ êµ¬ì¡° ê°œì„ """
    import re
    
    # 1. ê¸°ë³¸ ì •ê·œí™”
    text = re.sub(r'\s+', ' ', text).strip()
    
    # 2. í•œê¸€ ë‹¨ì–´ ì‚¬ì´ì— ë„ì–´ì“°ê¸° ì¶”ê°€
    # í•œê¸€ ììŒ+ëª¨ìŒ íŒ¨í„´ (ê°€-í£)
    text = re.sub(r'([ê°€-í£])([A-Za-z0-9])', r'\1 \2', text)  # í•œê¸€ ë’¤ì— ì˜ìˆ«ì
    text = re.sub(r'([A-Za-z0-9])([ê°€-í£])', r'\1 \2', text)  # ì˜ìˆ«ì ë’¤ì— í•œê¸€
    
    # 3. ë¶™ì–´ìˆëŠ” í•œê¸€ ë‹¨ì–´ë“¤ ì‚¬ì´ì— ë„ì–´ì“°ê¸° ì¶”ê°€ (íŒ¨í„´ ê¸°ë°˜)
    patterns = [
        (r'([.!?])([ê°€-í£])', r'\1 \2'),  # ë¬¸ì¥ ë¶€í˜¸ ë’¤ì— ë„ì–´ì“°ê¸°
        (r'([ê°€-í£])([(){}[\]<>])', r'\1 \2'),  # í•œê¸€ê³¼ ê´„í˜¸ ì‚¬ì´
        (r'([(){}[\]<>])([ê°€-í£])', r'\1 \2'),  # ê´„í˜¸ì™€ í•œê¸€ ì‚¬ì´
    ]
    
    for pattern, repl in patterns:
        text = re.sub(pattern, repl, text)
    
    # 4. íŠ¹ìˆ˜ íŒ¨í„´ ì²˜ë¦¬ (ì˜ˆ: ê°€ìŠ¤ê³µê¸‰ê·œì • -> ê°€ìŠ¤ ê³µê¸‰ ê·œì •)
    # 6ê¸€ì ì´ìƒ ì—°ì†ëœ í•œê¸€ì„ í™•ì¸í•˜ì—¬ 3~4ê¸€ì ë‹¨ìœ„ë¡œ ë¶„ë¦¬ ì‹œë„
    def split_long_korean(match):
        long_word = match.group(0)
        if len(long_word) >= 6:
            # ê¸´ ë‹¨ì–´ë¥¼ 3ê¸€ì ë‹¨ìœ„ë¡œ ë¶„ë¦¬ ì‹œë„
            parts = []
            i = 0
            while i < len(long_word):
                if i + 3 <= len(long_word):
                    parts.append(long_word[i:i+3])
                else:
                    parts.append(long_word[i:])
                i += 3
            return ' '.join(parts)
        return long_word
    
    text = re.sub(r'[ê°€-í£]{6,}', split_long_korean, text)
    
    return text

# === ë¬¸ì„œ ë¶ˆëŸ¬ì˜¤ê¸° ===
def load_document(file_path):
    """ë‹¨ì¼ ë¬¸ì„œë¥¼ ë¡œë“œí•©ë‹ˆë‹¤."""
    if file_path.endswith(".pdf"):
        return read_pdf(file_path)
    elif file_path.endswith(".docx"):
        text = read_docx(file_path)
        # DOCXëŠ” í˜ì´ì§€ ì •ë³´ê°€ ì—†ìœ¼ë¯€ë¡œ í•˜ë‚˜ì˜ í˜ì´ì§€ë¡œ ì·¨ê¸‰
        return [(1, text)]
    return []

# === ë¬¸ì¥ ë¶„í•  - ê°„ì†Œí™” ë²„ì „ ===
def split_text_to_chunks(text_by_page, source_name, chunk_size=200):
    """
    ë¬¸ì„œë¥¼ ë” ì‘ì€ ì²­í¬ë¡œ ë¶„í• í•©ë‹ˆë‹¤(ì¤„ ê¸°ì¤€ + ì˜ë¯¸ ë‹¨ìœ„ ìœ ì§€ + ê³µë°± ì •ë¦¬).
    """
    chunks = []

    for page_num, page_text in text_by_page:
        #print(f"--- í˜ì´ì§€ {page_num} ---")
        #print(page_text[:300])  # ë””ë²„ê¹…ìš©: ì•ë¶€ë¶„ ë¯¸ë¦¬ë³´ê¸°

        # 1. ì¤„ë°”ê¿ˆ ìœ ì§€í•˜ë©° ì¤„ ë‹¨ìœ„ ë¶„ë¦¬
        lines = [line.strip() for line in page_text.split('\n') if line.strip()]
        # 2. ê° ì¤„ì˜ ë‚´ë¶€ ê³µë°± ì •ë¦¬
        lines = [re.sub(r'\s+', ' ', line) for line in lines]

        current_chunk = ""

        for line in lines:
            # í•œê¸€-ì˜ë¬¸ ê³µë°± ì •ë¦¬
            line = re.sub(r'([ê°€-í£])([A-Za-z0-9])', r'\1 \2', line)
            line = re.sub(r'([A-Za-z0-9])([ê°€-í£])', r'\1 \2', line)

            if len(current_chunk) + len(line) > chunk_size and current_chunk:
                chunks.append({
                    "text": current_chunk,
                    "source": source_name,
                    "page": page_num
                })
                current_chunk = line
            else:
                current_chunk = current_chunk + " " + line if current_chunk else line

        if current_chunk:
            chunks.append({
                "text": current_chunk,
                "source": source_name,
                "page": page_num
            })

        # ì•ë¶€ë¶„ì´ ë¹ˆ í˜ì´ì§€ì¼ ë•Œ ìµœì†Œ 1ê°œ ì²­í¬ë¼ë„ ìƒì„±í•˜ë„ë¡ ì²˜ë¦¬
        if not chunks:
            chunks.append({
                "text": page_text[:chunk_size],
                "source": source_name,
                "page": page_num
            })

    print(f"â†’ ìƒì„±ëœ ì²­í¬ ìˆ˜: {len(chunks)}")
    return chunks

# === ì¸ë±ìŠ¤ ìƒì„± - ë°°ì¹˜ ì²˜ë¦¬ ===
# === ë²¡í„° ì¸ë±ìŠ¤ êµ¬ì¶• ===
def build_langchain_vector_index():
    print("ğŸ“‚ ë¬¸ì„œ ì¸ë±ì‹± ì‹œì‘")

    # ë¬¸ì„œ ëª©ë¡ ë¶ˆëŸ¬ì˜¤ê¸°
    files = [f for f in os.listdir(DOCS_DIR) if f.endswith((".pdf", ".docx"))]
    if not files:
        print("âŒ ì¸ë±ì‹±í•  ë¬¸ì„œ ì—†ìŒ")
        return

    print(f"ì´ {len(files)}ê°œ ë¬¸ì„œ ë°œê²¬")

    # ì„ë² ë”© ëª¨ë¸ ì´ˆê¸°í™” (ë°°ì¹˜ ì‚¬ì´ì¦ˆ ì¶•ì†Œë¡œ ë©”ëª¨ë¦¬ ì ˆì•½)
    embedding_model = HuggingFaceEmbeddings(
        model_name="sentence-transformers/all-MiniLM-L6-v2",
        encode_kwargs={"batch_size": 32}  # ê¸°ë³¸ì€ 32 â†’ 16ìœ¼ë¡œ ì¤„ì—¬ ë©”ëª¨ë¦¬ ì‚¬ìš© ê°ì†Œ
    )

    all_chunks = []
    total_pages = 0

    # ë¬¸ì„œë³„ë¡œ ì²­í¬ ìˆ˜ ì¶”ì  ë° ë¡œê·¸ ì¶œë ¥
    for file_name in files:
        file_path = os.path.join(DOCS_DIR, file_name)
        if file_name.endswith(".pdf"):
            text_by_page = read_pdf(file_path)
        else:
            text_by_page = read_docx(file_path)

        total_pages += len(text_by_page)

        chunks = split_text_to_chunks(text_by_page, file_name)
        all_chunks.extend(chunks)
        print(f" {file_name} â†’ {len(chunks)}ê°œ ì²­í¬")
        
        # ë©”ëª¨ë¦¬ ìµœì í™”
        del text_by_page, chunks
        gc.collect()
        time.sleep(0.1)  # ì‹œìŠ¤í…œ ì—¬ìœ  ì£¼ê¸°

    print(f"\nğŸ“Š ì „ì²´ ë¬¸ì„œ ìˆ˜: {len(files)}ê°œ")
    print(f"ğŸ“„ ì „ì²´ í˜ì´ì§€ ìˆ˜: {total_pages}")
    print(f"ğŸ”– ì „ì²´ ì²­í¬ ìˆ˜: {len(all_chunks)}")

    #if len(all_chunks) > 100_000:
    #    print("âš ï¸ ì²­í¬ ìˆ˜ê°€ ë§¤ìš° ë§ìŠµë‹ˆë‹¤. ë©”ëª¨ë¦¬ ë¶€ì¡± ê°€ëŠ¥ì„±ì´ ìˆìœ¼ë¯€ë¡œ ë°°ì¹˜ ì²˜ë¦¬ë¥¼ ê³ ë ¤í•˜ì„¸ìš”.")

    # LangChain ë¬¸ì„œ í˜•ì‹ìœ¼ë¡œ ë³€í™˜
    lc_documents = [
        LCDocument(page_content=chunk["text"], metadata={"source": chunk["source"], "page": chunk["page"]})
        for chunk in all_chunks
    ]

    # ì „ì²´ ì²­í¬ë¡œë¶€í„° ì¸ë±ìŠ¤ ìƒì„±
    print("\nğŸ”§ ì¸ë±ìŠ¤ ìƒì„± ì¤‘...")
    vectorstore = FAISS.from_documents(lc_documents, embedding_model)

   
    # ì¸ë±ìŠ¤ ì €ì¥
    try:
        vectorstore.save_local(DB_DIR)
    except Exception as e:
        print(f"ì¸ë±ìŠ¤ ì €ì¥ ì˜¤ë¥˜: {e}")

    print(f"\nğŸ’¾ ì¸ë±ìŠ¤ ì €ì¥ ì™„ë£Œ: {INDEX_FAISS_PATH}")
    print(f"ğŸ’¾ ë©”íƒ€ë°ì´í„° ì €ì¥ ì™„ë£Œ: {INDEX_PKL_PATH}")
    print("ğŸ‰ ëª¨ë“  ë¬¸ì„œ ì¸ë±ì‹± ì™„ë£Œ")


# === ê²€ìƒ‰ ì˜ˆì‹œ ===
def search_query(query, top_k=5):
    print(f"\nğŸ” ê²€ìƒ‰ì–´: {query}")
    embedding_model = HuggingFaceEmbeddings(model_name="sentence-transformers/all-MiniLM-L6-v2")
    
    # pickle íŒŒì¼ ë¡œë”© í—ˆìš©
    #vectorstore = FAISS.load_local(DB_DIR, embedding_model, allow_dangerous_deserialization=True)
    print("ğŸ“ ë²¡í„°ìŠ¤í† ì–´ ë¡œë”© ì¤‘...")
    vectorstore = FAISS.load_local(DB_DIR, embedding_model, allow_dangerous_deserialization=True)
    print("âœ… ë²¡í„°ìŠ¤í† ì–´ ë¡œë”© ì™„ë£Œ")

    # ì‹¤ì œ ë²¡í„° ê°œìˆ˜ í™•ì¸
    if hasattr(vectorstore, "index") and hasattr(vectorstore.index, "ntotal"):
        print(f"ğŸ” ì¸ë±ìŠ¤ì— ì €ì¥ëœ ë²¡í„° ìˆ˜: {vectorstore.index.ntotal}")
    else:
        print("âš ï¸ ì¸ë±ìŠ¤ ì •ë³´ ì—†ìŒ")

    results = vectorstore.similarity_search(query, k=top_k)

    print("\nğŸ“Œ ê²€ìƒ‰ ê²°ê³¼:")
    for i, doc in enumerate(results, start=1):
        print(f"[{i}] {doc.page_content}")
        
        # ë©”íƒ€ë°ì´í„° ì•ˆì „í•˜ê²Œ ì¶œë ¥
        source = doc.metadata.get('source', 'ì•Œ ìˆ˜ ì—†ìŒ')
        page = doc.metadata.get('page', 'ì•Œ ìˆ˜ ì—†ìŒ')
        print(f"     â¤· ì¶œì²˜: {source}, í˜ì´ì§€: {page}")
        print("-" * 60)


# === ë©”ì¸ ì‹¤í–‰ ===
if __name__ == "__main__":
    build_langchain_vector_index()
    
    # ì¸ë±ìŠ¤ ìƒì„± ì™„ë£Œ í›„ ì˜ˆì‹œ ê²€ìƒ‰ ìˆ˜í–‰
    search_query("ë„ì‹œê°€ìŠ¤ ê³µê¸‰ ê·œì •ì—ì„œ ë„ì‹œê°€ìŠ¤íšŒì‚¬ì˜ ì˜ë¬´ëŠ” ë¬´ì—‡ì¸ê°€ìš”?")
    search_query("ê°€ìŠ¤ìš”ê¸ˆì€ ì–´ë–»ê²Œ ê³„ì‚°ë˜ë‚˜ìš”?")
