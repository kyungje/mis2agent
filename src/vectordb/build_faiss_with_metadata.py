import os
import fitz  # PyMuPDF
from docx import Document
from langchain_community.vectorstores import FAISS
from langchain_community.embeddings import HuggingFaceEmbeddings
from langchain_core.documents import Document as LCDocument

# === ê²½ë¡œ ì„¤ì • ===
BASE_DIR = os.path.dirname(os.path.abspath(__file__))
DOCS_DIR = os.path.join(BASE_DIR, "docs")
DB_DIR = os.path.join(BASE_DIR, "db")
INDEX_FAISS_PATH = os.path.join(DB_DIR, "index.faiss")
INDEX_PKL_PATH = os.path.join(DB_DIR, "index.pkl")

# í•„ìš”í•œ í´ë” ìƒì„±
os.makedirs(DB_DIR, exist_ok=True)

# === ë¬¸ì„œ ì½ê¸° ===
def read_docx(path):
    doc = Document(path)
    return "\n".join([para.text for para in doc.paragraphs if para.text.strip()])

def read_pdf(path):
    doc = fitz.open(path)
    text = ""
    for page_num, page in enumerate(doc, start=1):
        text += f"\n[Page {page_num}]\n" + page.get_text()
    return text

# === ë¬¸ì„œ ë¶ˆëŸ¬ì˜¤ê¸° ===
def load_documents(folder_path):
    documents = []
    for file in os.listdir(folder_path):
        file_path = os.path.join(folder_path, file)
        if file.endswith(".pdf"):
            text = read_pdf(file_path)
        elif file.endswith(".docx"):
            text = read_docx(file_path)
        else:
            continue
        documents.append({"filename": file, "text": text})
    return documents

# === ë¬¸ì¥ ë¶„í•  ===
def split_documents(documents):
    chunks = []
    for doc in documents:
        lines = [line.strip() for line in doc["text"].split("\n") if len(line.strip()) > 20]
        for line in lines:
            chunks.append({"text": line, "source": doc["filename"]})
    return chunks

# === ì¸ë±ìŠ¤ ìƒì„± ===
def build_langchain_vector_index():
    print("ğŸ“‚ ë¬¸ì„œ ë¡œë”© ì¤‘...")
    documents = load_documents(DOCS_DIR)
    if not documents:
        print("âŒ docs í´ë”ì— ìœ íš¨í•œ ë¬¸ì„œê°€ ì—†ìŠµë‹ˆë‹¤.")
        return

    chunks = split_documents(documents)
    if not chunks:
        print("âŒ ë¬¸ì¥ì—ì„œ ìœ ì˜ë¯¸í•œ í…ìŠ¤íŠ¸ë¥¼ ì¶”ì¶œí•˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")
        return

    lc_documents = [
        LCDocument(page_content=chunk["text"], metadata={"source": chunk["source"]})
        for chunk in chunks
    ]

    print(f"âœ… ë¬¸ì„œì—ì„œ ì´ {len(lc_documents)}ê°œì˜ ë¬¸ì¥ ë¶„í•  ë° ì„ë² ë”© ì¤€ë¹„ ì™„ë£Œ")

    embedding_model = HuggingFaceEmbeddings(model_name="sentence-transformers/all-MiniLM-L6-v2")
    vectorstore = FAISS.from_documents(lc_documents, embedding_model)
    vectorstore.save_local(DB_DIR)

    print(f"ğŸ’¾ ì¸ë±ìŠ¤ ì €ì¥ ì™„ë£Œ â†’ {INDEX_FAISS_PATH}")
    print(f"ğŸ’¾ ë©”íƒ€ë°ì´í„° ì €ì¥ ì™„ë£Œ â†’ {INDEX_PKL_PATH}")
    print("ğŸ‰ LangChain í˜¸í™˜ ë²¡í„° DB ìƒì„± ì™„ë£Œ")

# === ê²€ìƒ‰ ì˜ˆì‹œ ===
def search_query(query, top_k=3):
    print(f"\nğŸ” ê²€ìƒ‰ì–´: {query}")
    embedding_model = HuggingFaceEmbeddings(model_name="sentence-transformers/all-MiniLM-L6-v2")
    
    # âœ… pickle íŒŒì¼ ë¡œë”© í—ˆìš©
    vectorstore = FAISS.load_local(DB_DIR, embedding_model, allow_dangerous_deserialization=True)

    results = vectorstore.similarity_search(query, k=top_k)

    print("\nğŸ“Œ ê²€ìƒ‰ ê²°ê³¼:")
    for i, doc in enumerate(results, start=1):
        print(f"[{i}] {doc.page_content}")
        print(f"     â¤· ì¶œì²˜: {doc.metadata['source']}")
        print("-" * 60)


# === ë©”ì¸ ì‹¤í–‰ ===
if __name__ == "__main__":
    build_langchain_vector_index()
    search_query("ê¸°ì¤€ì—´ëŸ‰ì´ ë¬´ì—‡ì¸ê°€ìš”?")
