import streamlit as st
import requests
import json
from typing import List, Dict, Any
import time
import re
import os
import tempfile
import shutil
from pathlib import Path

# build_faiss_with_metadata.pyì—ì„œ í•„ìš”í•œ ëª¨ë“ˆë“¤ import
import pdfplumber
from docx import Document
from langchain_community.vectorstores import FAISS
from langchain_openai import OpenAIEmbeddings
from langchain_core.documents import Document as LCDocument
from langchain_text_splitters import RecursiveCharacterTextSplitter
import gc
from dotenv import load_dotenv

# .env íŒŒì¼ ë¡œë“œ
load_dotenv()

# API ì—”ë“œí¬ì¸íŠ¸ ì„¤ì •
API_URL = "http://localhost:8000/chat"
RELOAD_API_URL = "http://localhost:8000/reload-indexes"

def latex_to_text(text):
    """
    LaTeX ìˆ˜ì‹ì„ ì‚¬ëŒì´ ì½ëŠ” í…ìŠ¤íŠ¸ ìˆ˜ì‹ìœ¼ë¡œ ë³€í™˜
    ì˜ˆ: \frac{a}{b} â†’ (a) / (b)
    """
    # \frac ë³€í™˜ í•¨ìˆ˜
    def frac_repl(match):
        return f"({match.group(1)}) / ({match.group(2)})"

    # LaTeX ë¸”ë¡(\[...\], $$...$$, $...$)ì„ ì°¾ì•„ì„œ ë³€í™˜
    def latex_block_repl(match):
        latex_expr = match.group(1)
        # \leftì™€ \right ì œê±° (ê´„í˜¸ í¬ê¸° ì¡°ì • ëª…ë ¹ì–´) - ë” í¬ê´„ì ìœ¼ë¡œ ì²˜ë¦¬
        latex_expr = re.sub(r'\\left\s*\(', '(', latex_expr)
        latex_expr = re.sub(r'\\right\s*\)', ')', latex_expr)
        latex_expr = re.sub(r'\\left\s*\[', '[', latex_expr)
        latex_expr = re.sub(r'\\right\s*\]', ']', latex_expr)
        latex_expr = re.sub(r'\\left\s*\\{', '{', latex_expr)
        latex_expr = re.sub(r'\\right\s*\\}', '}', latex_expr)
        # \frac ë³€í™˜
        latex_expr = re.sub(r'\\frac\{(.+?)\}\{(.+?)\}', frac_repl, latex_expr)
        # \times ë³€í™˜
        latex_expr = latex_expr.replace(r'\times', 'Ã—')
        # ì¤‘ê´„í˜¸ ì œê±°
        latex_expr = latex_expr.replace('{', '').replace('}', '')
        return latex_expr

    # \[ ... \] ë¸”ë¡ ë³€í™˜
    text = re.sub(r'\\\[(.*?)\\\]', lambda m: latex_block_repl(m), text, flags=re.DOTALL)
    # $$ ... $$ ë¸”ë¡ ë³€í™˜
    text = re.sub(r'\$\$(.*?)\$\$', lambda m: latex_block_repl(m), text, flags=re.DOTALL)
    # $ ... $ ë¸”ë¡ ë³€í™˜
    text = re.sub(r'\$(.*?)\$', lambda m: latex_block_repl(m), text, flags=re.DOTALL)

    # ì¸ë¼ì¸ ë³€í™˜ (í˜¹ì‹œ ë‚¨ì•„ìˆì„ ê²½ìš°)
    # \leftì™€ \right ì œê±° - ë” í¬ê´„ì ìœ¼ë¡œ ì²˜ë¦¬
    text = re.sub(r'\\left\s*\(', '(', text)
    text = re.sub(r'\\right\s*\)', ')', text)
    text = re.sub(r'\\left\s*\[', '[', text)
    text = re.sub(r'\\right\s*\]', ']', text)
    text = re.sub(r'\\left\s*\\{', '{', text)
    text = re.sub(r'\\right\s*\\}', '}', text)
    # \frac ë³€í™˜
    text = re.sub(r'\\frac\{(.+?)\}\{(.+?)\}', frac_repl, text)
    text = text.replace(r'\times', 'Ã—')
    text = text.replace('{', '').replace('}', '')

    return text

# === build_faiss_with_metadata.pyì—ì„œ ê°€ì ¸ì˜¨ í•¨ìˆ˜ë“¤ ===

# ê²½ë¡œ ì„¤ì •
BASE_DIR = Path(__file__).parent.parent / "vectordb"
DOCS_DIR = BASE_DIR / "docs"
DB_DIR = BASE_DIR / "db"

# ì¸ë±ìŠ¤ ë¶„ë¥˜ë³„ ê²½ë¡œ ì„¤ì •
GAS_INDEX_DIR = DB_DIR / "gas_index"
POWER_INDEX_DIR = DB_DIR / "power_index"
OTHER_INDEX_DIR = DB_DIR / "other_index"

# í•„ìš”í•œ í´ë” ìƒì„±
DB_DIR.mkdir(exist_ok=True)
GAS_INDEX_DIR.mkdir(exist_ok=True)
POWER_INDEX_DIR.mkdir(exist_ok=True)
OTHER_INDEX_DIR.mkdir(exist_ok=True)

# OpenAI ì„ë² ë”© ëª¨ë¸ ì´ˆê¸°í™”
def create_embedding_model():
    """ë™ì ìœ¼ë¡œ ë°°ì¹˜ í¬ê¸°ë¥¼ ì¡°ì •í•˜ì—¬ ì„ë² ë”© ëª¨ë¸ì„ ìƒì„±í•©ë‹ˆë‹¤."""
    chunk_size = 1000
    
    try:
        embedding_model = OpenAIEmbeddings(
            model="text-embedding-3-small",
            chunk_size=chunk_size,
            max_retries=3
        )
        return embedding_model
    except Exception as e:
        if "max_tokens_per_request" in str(e):
            chunk_size = 500
            st.info(f"í† í° ì œí•œìœ¼ë¡œ ì¸í•´ ë°°ì¹˜ í¬ê¸°ë¥¼ {chunk_size}ë¡œ ì¡°ì •í•©ë‹ˆë‹¤.")
            return OpenAIEmbeddings(
                model="text-embedding-3-small",
                chunk_size=chunk_size,
                max_retries=3
            )
        else:
            raise e

# í…ìŠ¤íŠ¸ ë¶„í• ê¸° ì´ˆê¸°í™”
text_splitter = RecursiveCharacterTextSplitter(
    chunk_size=400,
    chunk_overlap=80,
    length_function=len,
    separators=["\n\n", "\n", " ", ""]
)

# ë¬¸ì„œ ì½ê¸° í•¨ìˆ˜ë“¤
def read_docx(path):
    """DOCX ë¬¸ì„œë¥¼ ì½ì–´ì„œ í…ìŠ¤íŠ¸ë¥¼ ì¶”ì¶œí•©ë‹ˆë‹¤."""
    doc = Document(path)
    text = "\n".join([para.text for para in doc.paragraphs if para.text.strip()])
    text = latex_to_text(text)
    return text

def read_pdf(path):
    """PDF ë¬¸ì„œë¥¼ ì½ì–´ì„œ ì „ì²´ í…ìŠ¤íŠ¸ë¥¼ ì¶”ì¶œí•©ë‹ˆë‹¤."""
    full_text = ""
    with pdfplumber.open(path) as pdf:
        for page_num, page in enumerate(pdf.pages, start=1):
            try:
                page_text = page.extract_text()
                if page_text:
                    page_text = latex_to_text(page_text)
                    page_text = normalize_text(page_text)
                    full_text += page_text + "\n"
            except Exception as e:
                st.warning(f"í˜ì´ì§€ {page_num} ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {str(e)}")
    return full_text

def read_txt(path):
    """TXT íŒŒì¼ì„ ì½ì–´ì„œ í…ìŠ¤íŠ¸ë¥¼ ì¶”ì¶œí•©ë‹ˆë‹¤."""
    with open(path, 'r', encoding='utf-8') as f:
        text = f.read()
    text = latex_to_text(text)
    return text

# í…ìŠ¤íŠ¸ ì •ê·œí™”
def normalize_text(text):
    """í…ìŠ¤íŠ¸ ì •ê·œí™” (í•œê¸€-ì˜ë¬¸ ê³µë°±, ìˆ˜ì‹ ê¸°í˜¸ ë³´ì¡´)"""
    # í•œê¸€-ì˜ë¬¸ ê³µë°± ì •ë¦¬
    text = re.sub(r'([ê°€-í£])([A-Za-z0-9])', r'\1 \2', text)
    text = re.sub(r'([A-Za-z0-9])([ê°€-í£])', r'\1 \2', text)

    # ìˆ˜ì‹ ê¸°í˜¸ ë³´ì¡´: âˆš Â± â‰ˆ âˆ Ã— Ã· Ï€ Â² Â³ ^ / = % ë“±
    math_symbols = "âˆšÂ±â‰ˆâˆÃ—Ã·Ï€Â²Â³^=/%"

    # ë” ê´€ëŒ€í•œ ì •ê·œí™”: í•œê¸€, ì˜ë¬¸, ìˆ«ì, ê³µë°±, ë¬¸ì¥ë¶€í˜¸, ìˆ˜ì‹ ê¸°í˜¸ë§Œ ìœ ì§€
    # í•œê¸€: ê°€-í£
    # ì˜ë¬¸: A-Za-z
    # ìˆ«ì: 0-9
    # ê³µë°±: \s
    # ë¬¸ì¥ë¶€í˜¸: .,!?;:-()[]{}
    # ìˆ˜ì‹ ê¸°í˜¸: âˆšÂ±â‰ˆâˆÃ—Ã·Ï€Â²Â³^=/%
    # ì¶”ê°€ í—ˆìš© ë¬¸ì: + (í”ŒëŸ¬ìŠ¤ ê¸°í˜¸)
    
    allowed_pattern = r'[ê°€-í£A-Za-z0-9\s.,!?;:\-\(\)\[\]\{\}' + re.escape(math_symbols + '+') + r']'
    text = re.sub(rf'[^{allowed_pattern}]', ' ', text)

    # ì—°ì† ê³µë°± ì •ë¦¬
    text = re.sub(r'\s+', ' ', text)

    return text.strip()

# íŒŒì¼ëª… ì •ê·œí™” í•¨ìˆ˜
def normalize_filename(file_name):
    """íŒŒì¼ëª…ì„ ì •ê·œí™”í•˜ì—¬ ë¶„ë¥˜ì— ì‚¬ìš©í•©ë‹ˆë‹¤."""
    # íŒŒì¼ í™•ì¥ì ì œê±°
    name_without_ext = os.path.splitext(file_name)[0]
    
    # íŠ¹ìˆ˜ë¬¸ì ì œê±° (í•˜ì´í”ˆ, ì–¸ë”ìŠ¤ì½”ì–´, í”ŒëŸ¬ìŠ¤ ë“±ì€ ê³µë°±ìœ¼ë¡œ ë³€í™˜)
    normalized = re.sub(r'[\[\]\(\)\+\-\_]', ' ', name_without_ext)
    
    # ì—°ì† ê³µë°±ì„ ë‹¨ì¼ ê³µë°±ìœ¼ë¡œ ë³€í™˜
    normalized = re.sub(r'\s+', ' ', normalized)
    
    # ì•ë’¤ ê³µë°± ì œê±°
    normalized = normalized.strip()
    
    return normalized

# íŒŒì¼ ë¶„ë¥˜ í•¨ìˆ˜
def classify_file(file_name):
    """íŒŒì¼ëª…ì— ë”°ë¼ ë¶„ë¥˜ë¥¼ ê²°ì •í•©ë‹ˆë‹¤."""
    st.info(f"ğŸ” íŒŒì¼ ë¶„ë¥˜ ì¤‘: '{file_name}'")
    
    # íŒŒì¼ëª… ì •ê·œí™”
    normalized_name = normalize_filename(file_name)
    st.info(f"  ğŸ“ ì •ê·œí™”ëœ íŒŒì¼ëª…: '{normalized_name}'")
    
    if 'ë„ì‹œê°€ìŠ¤' in normalized_name:
        st.info(f"  âœ… 'ë„ì‹œê°€ìŠ¤' í‚¤ì›Œë“œ ë°œê²¬ â†’ Gas ë¶„ë¥˜")
        return 'gas'
    elif 'ì „ë ¥' in normalized_name:
        st.info(f"  âœ… 'ì „ë ¥' í‚¤ì›Œë“œ ë°œê²¬ â†’ Power ë¶„ë¥˜")
        return 'power'
    else:
        st.info(f"  âš ï¸ í‚¤ì›Œë“œ ì—†ìŒ â†’ Other ë¶„ë¥˜")
        return 'other'

def get_index_dir(category):
    """ë¶„ë¥˜ì— ë”°ë¥¸ ì¸ë±ìŠ¤ ë””ë ‰í† ë¦¬ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤."""
    if category == 'gas':
        return GAS_INDEX_DIR
    elif category == 'power':
        return POWER_INDEX_DIR
    else:
        return OTHER_INDEX_DIR

def extract_metadata_from_filename(filename):
    base_name = os.path.splitext(filename)[0]
    parts = re.split(r'[_\-\s]', base_name)

    version = next((p for p in parts if re.match(r'20\d{2}', p)), None)
    region_list = ['ì„œìš¸', 'ë¶€ì‚°', 'ëŒ€êµ¬', 'ê´‘ì£¼', 'ì¸ì²œ', 'ëŒ€ì „', 'ìš¸ì‚°','ê²½ê¸°ë„', 'ê°•ì›ë„', 'ì¶©ì²­ë¶ë„', 'ì¶©ì²­ë‚¨ë„' ,'ì „ë¼ë‚¨ë„','ì „ë¶íŠ¹ë³„ìì¹˜ë„', 'ê²½ìƒë‚¨ë„', 'ê²½ìƒë¶ë„']
    region = next((p for p in parts if p in region_list), None)
    organization_map = {'ë„ì‹œê°€ìŠ¤': 'ë„ì‹œê°€ìŠ¤', 'ì „ë ¥': 'ì „ë ¥'}
    organization = next((p for p in organization_map if p in parts), "ê¸°íƒ€")

    return {
        "version": version,
        "region": region,
        "organization": organization,
        "title": base_name
    }

# ë¬¸ì„œ ì²˜ë¦¬ ë° ì²­í‚¹
def process_document(file_path):
    """ë¬¸ì„œë¥¼ ì½ê³  LangChain í…ìŠ¤íŠ¸ ë¶„í• ê¸°ë¥¼ ì‚¬ìš©í•˜ì—¬ ì²­í‚¹í•©ë‹ˆë‹¤."""
    filename = os.path.basename(file_path)
    
    # ë¬¸ì„œ ì½ê¸°
    if file_path.endswith(".pdf"):
        text = read_pdf(file_path)
    elif file_path.endswith(".docx"):
        text = read_docx(file_path)
    elif file_path.endswith(".txt"):
        text = read_txt(file_path)
    else:
        return []
    
    # í…ìŠ¤íŠ¸ê°€ ë¹„ì–´ìˆìœ¼ë©´ ì²˜ë¦¬ ì¤‘ë‹¨
    if not text:
        st.warning(f"âš ï¸ {filename}ì—ì„œ í…ìŠ¤íŠ¸ë¥¼ ì¶”ì¶œí•˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤. ê±´ë„ˆëœë‹ˆë‹¤.")
        return []
    
    st.info(f"  ğŸ“„ ì›ë³¸ í…ìŠ¤íŠ¸ ê¸¸ì´: {len(text)}ì")
    
    # í…ìŠ¤íŠ¸ ì •ê·œí™”
    original_text = text
    text = normalize_text(text)
    st.info(f"  ğŸ“„ ì •ê·œí™” í›„ í…ìŠ¤íŠ¸ ê¸¸ì´: {len(text)}ì")
    
    # ì •ê·œí™” ê³¼ì •ì—ì„œ í…ìŠ¤íŠ¸ê°€ ë„ˆë¬´ ë§ì´ ì œê±°ë˜ì—ˆëŠ”ì§€ í™•ì¸
    if len(text) < len(original_text) * 0.1:  # 90% ì´ìƒ ì œê±°ëœ ê²½ìš°
        st.warning(f"  âš ï¸ í…ìŠ¤íŠ¸ê°€ ë„ˆë¬´ ë§ì´ ì œê±°ë˜ì—ˆìŠµë‹ˆë‹¤. ì›ë³¸: {len(original_text)}ì â†’ ì •ê·œí™”: {len(text)}ì")
        # ì •ê·œí™”ë¥¼ ê±´ë„ˆë›°ê³  ì›ë³¸ í…ìŠ¤íŠ¸ ì‚¬ìš©
        text = original_text
        st.info(f"  ğŸ”„ ì›ë³¸ í…ìŠ¤íŠ¸ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.")
    
    # LangChain í…ìŠ¤íŠ¸ ë¶„í• ê¸° ì‚¬ìš©
    chunks = text_splitter.split_text(text)
    st.info(f"  ğŸ“„ ì²­í‚¹ í›„ ì²­í¬ ìˆ˜: {len(chunks)}ê°œ")
    
    # íŒŒì¼ëª…ì—ì„œ ì¶”ê°€ ë©”íƒ€ë°ì´í„° ì¶”ì¶œ
    additional_metadata = extract_metadata_from_filename(filename)
    
    # ë©”íƒ€ë°ì´í„°ê°€ í¬í•¨ëœ LangChain ë¬¸ì„œë¡œ ë³€í™˜
    documents = []
    for i, chunk in enumerate(chunks):
        if len(chunk.strip()) > 50:  # ìµœì†Œ ê¸¸ì´ í•„í„° (50ì ì´ìƒ)
            # ê¸°ë³¸ ë©”íƒ€ë°ì´í„°ì™€ ì¶”ê°€ ë©”íƒ€ë°ì´í„°ë¥¼ ê²°í•©
            metadata = {
                "source": filename,
                "chunk_id": i,
                "file_path": file_path,
                "version": additional_metadata["version"],
                "region": additional_metadata["region"],
                "organization": additional_metadata["organization"],
                "title": additional_metadata["title"]
            }
            doc = LCDocument(
                page_content=chunk,
                metadata=metadata
            )
            documents.append(doc)
        else:
            st.info(f"  âš ï¸ ì²­í¬ {i}ê°€ ë„ˆë¬´ ì§§ì•„ ì œì™¸ë¨: {len(chunk.strip())}ì")
    
    st.info(f"[CHUNKS] {filename} â†’ {len(documents)}ê°œ ìƒì„±")
    return documents

# ë²¡í„° ì¸ë±ìŠ¤ êµ¬ì¶•
def build_vector_index_from_uploaded_files(uploaded_files):
    """ì—…ë¡œë“œëœ íŒŒì¼ë“¤ë¡œë¶€í„° ë²¡í„° ì¸ë±ìŠ¤ë¥¼ êµ¬ì¶•í•©ë‹ˆë‹¤."""
    if not uploaded_files:
        st.warning("ì—…ë¡œë“œëœ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤.")
        return False
    
    st.info("ğŸ“‚ ë¬¸ì„œ ì¸ë±ì‹± ì‹œì‘")
    
    # ë¬¸ì„œ ì €ì¥ ë””ë ‰í† ë¦¬ ìƒì„±
    docs_dir = Path("/Users/a07198/IdeaProjects/MIS2/src/vectordb/docs")
    docs_dir.mkdir(parents=True, exist_ok=True)
    st.info(f"ğŸ“ ë¬¸ì„œ ì €ì¥ ë””ë ‰í† ë¦¬: {docs_dir}")
    
    # ì„ë² ë”© ëª¨ë¸ ì´ˆê¸°í™”
    embedding_model = create_embedding_model()
    
    # ë¶„ë¥˜ë³„ë¡œ ë¬¸ì„œ ê·¸ë£¹í™”
    gas_documents = []
    power_documents = []
    other_documents = []
    
    total_files = len(uploaded_files)
    
    # íŒŒì¼ë³„ë¡œ ì²˜ë¦¬ ë° ë¶„ë¥˜
    for i, uploaded_file in enumerate(uploaded_files, 1):
        st.info(f"[{i}/{total_files}] ì²˜ë¦¬ ì¤‘: {uploaded_file.name}")
        
        # ì‹¤ì œ íŒŒì¼ì„ docs ë””ë ‰í† ë¦¬ì— ì €ì¥
        file_path = docs_dir / uploaded_file.name
        with open(file_path, "wb") as f:
            f.write(uploaded_file.getvalue())
        
        st.info(f"  ğŸ’¾ íŒŒì¼ ì €ì¥: {file_path}")
        
        try:
            # íŒŒì¼ ë¶„ë¥˜
            category = classify_file(uploaded_file.name)
            st.info(f"  â†’ ë¶„ë¥˜: {category}")
            
            documents = process_document(str(file_path))
            
            # ë¶„ë¥˜ë³„ë¡œ ë¬¸ì„œ ì¶”ê°€
            if category == 'gas':
                gas_documents.extend(documents)
            elif category == 'power':
                power_documents.extend(documents)
            else:
                other_documents.extend(documents)
            
            # ë©”ëª¨ë¦¬ ìµœì í™”
            del documents
            gc.collect()
            
        except Exception as e:
            st.error(f"  âŒ íŒŒì¼ ì²˜ë¦¬ ì˜¤ë¥˜: {e}")
            # ì˜¤ë¥˜ ë°œìƒ ì‹œ ì €ì¥ëœ íŒŒì¼ ì‚­ì œ
            if file_path.exists():
                file_path.unlink()
            continue
    
    st.info(f"\nğŸ“Š ì „ì²´ ë¬¸ì„œ ìˆ˜: {len(uploaded_files)}ê°œ")
    st.info(f"ğŸ”– Gas ë¬¸ì„œ: {len(gas_documents)}ê°œ")
    st.info(f"ğŸ”– Power ë¬¸ì„œ: {len(power_documents)}ê°œ")
    st.info(f"ğŸ”– Other ë¬¸ì„œ: {len(other_documents)}ê°œ")
    
    # ë¶„ë¥˜ë³„ë¡œ ì¸ë±ìŠ¤ ìƒì„±
    categories = [
        ('gas', gas_documents, 'Gas'),
        ('power', power_documents, 'Power'),
        ('other', other_documents, 'Other')
    ]
    
    success_count = 0
    for category, documents, category_name in categories:
        if len(documents) == 0:
            st.warning(f"âš ï¸ {category_name} ë¬¸ì„œê°€ ì—†ì–´ ì¸ë±ìŠ¤ë¥¼ ìƒì„±í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")
            continue
            
        st.info(f"\nğŸ”§ {category_name} ì¸ë±ìŠ¤ ìƒì„± ì¤‘... (ë¬¸ì„œ ìˆ˜: {len(documents)}ê°œ)")
        
        # ë°°ì¹˜ ë‹¨ìœ„ë¡œ ì„ë² ë”© ì²˜ë¦¬
        try:
            vectorstore = FAISS.from_documents(documents, embedding_model)
        except Exception as e:
            if "max_tokens_per_request" in str(e):
                st.info(f"í† í° ì œí•œìœ¼ë¡œ ì¸í•´ ë°°ì¹˜ í¬ê¸°ë¥¼ 500ìœ¼ë¡œ ì¡°ì •í•©ë‹ˆë‹¤...")
                try:
                    medium_embedding_model = OpenAIEmbeddings(
                        model="text-embedding-3-small",
                        chunk_size=500,
                        max_retries=3
                    )
                    vectorstore = FAISS.from_documents(documents, medium_embedding_model)
                except Exception as e2:
                    if "max_tokens_per_request" in str(e2):
                        st.info(f"í† í° ì œí•œìœ¼ë¡œ ì¸í•´ ë°°ì¹˜ í¬ê¸°ë¥¼ 100ìœ¼ë¡œ ì¡°ì •í•©ë‹ˆë‹¤...")
                        small_embedding_model = OpenAIEmbeddings(
                            model="text-embedding-3-small",
                            chunk_size=100,
                            max_retries=3
                        )
                        vectorstore = FAISS.from_documents(documents, small_embedding_model)
                    else:
                        raise e2
            else:
                raise e

        # ì¸ë±ìŠ¤ ì €ì¥
        index_dir = get_index_dir(category)
        try:
            vectorstore.save_local(str(index_dir))
            st.success(f"ğŸ’¾ {category_name} ì¸ë±ìŠ¤ ì €ì¥ ì™„ë£Œ: {index_dir}")
            success_count += 1
        except Exception as e:
            st.error(f"âŒ {category_name} ì¸ë±ìŠ¤ ì €ì¥ ì˜¤ë¥˜: {e}")
    
    if success_count > 0:
        st.success(f"\nğŸ‰ {success_count}ê°œ ë¶„ë¥˜ë³„ ì¸ë±ìŠ¤ ìƒì„± ì™„ë£Œ")
        return True
    else:
        st.error("âŒ ì¸ë±ìŠ¤ ìƒì„±ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.")
        return False

def reload_backend_indexes():
    """ë°±ì—”ë“œì˜ ì¸ë±ìŠ¤ë¥¼ ë‹¤ì‹œ ë¡œë“œí•©ë‹ˆë‹¤."""
    try:
        st.info("ğŸ”„ ë°±ì—”ë“œ ì¸ë±ìŠ¤ ë¦¬ë¡œë“œ ì¤‘...")
        response = requests.post(RELOAD_API_URL)
        response.raise_for_status()
        
        result = response.json()
        if result.get("success"):
            st.success("âœ… ë°±ì—”ë“œ ì¸ë±ìŠ¤ ë¦¬ë¡œë“œ ì™„ë£Œ!")
            return True
        else:
            st.error(f"âŒ ë°±ì—”ë“œ ì¸ë±ìŠ¤ ë¦¬ë¡œë“œ ì‹¤íŒ¨: {result.get('message', 'ì•Œ ìˆ˜ ì—†ëŠ” ì˜¤ë¥˜')}")
            return False
    except Exception as e:
        st.error(f"âŒ ë°±ì—”ë“œ ì¸ë±ìŠ¤ ë¦¬ë¡œë“œ ì¤‘ ì˜¤ë¥˜: {str(e)}")
        st.warning("âš ï¸ FastAPI ì„œë²„ê°€ ì‹¤í–‰ ì¤‘ì¸ì§€ í™•ì¸í•´ì£¼ì„¸ìš”.")
        return False

def initialize_session_state():
    """ì„¸ì…˜ ìƒíƒœ ì´ˆê¸°í™”"""
    if "messages" not in st.session_state:
        st.session_state.messages = []
    if "streaming" not in st.session_state:
        st.session_state.streaming = False

def display_chat_history():
    """ì±„íŒ… ê¸°ë¡ í‘œì‹œ"""
    for message in st.session_state.messages:
        with st.chat_message(message["role"]):
            st.markdown(message["content"])

def stream_response(response_text: str, loading_placeholder):
    """ìŠ¤íŠ¸ë¦¬ë° ë°©ì‹ìœ¼ë¡œ ì‘ë‹µ í‘œì‹œ"""
    message_placeholder = st.empty()
    full_response = ""
    
    # ë¡œë”© ë©”ì‹œì§€ ì œê±°
    loading_placeholder.empty()
    
    # ë¬¸ìë¥¼ í•˜ë‚˜ì”© í‘œì‹œ
    for char in response_text:
        full_response += char
        message_placeholder.markdown(full_response + "â–Œ")
        time.sleep(0.01)  # íƒ€ì´í•‘ íš¨ê³¼ë¥¼ ìœ„í•œ ì§€ì—°
    
    # ìµœì¢… ì‘ë‹µ í‘œì‹œ
    message_placeholder.markdown(full_response)
    return full_response

def send_message(user_input: str):
    """ë©”ì‹œì§€ ì „ì†¡ ë° ì‘ë‹µ ì²˜ë¦¬"""
    if not user_input:
        return

    # ì‚¬ìš©ì ë©”ì‹œì§€ë¥¼ ë¨¼ì € í™”ë©´ì— í‘œì‹œ
    with st.chat_message("user"):
        st.markdown(user_input)
    
    # ë¡œë”© ë©”ì‹œì§€ í‘œì‹œë¥¼ ìœ„í•œ í”Œë ˆì´ìŠ¤í™€ë”
    loading_placeholder = st.empty()
    with loading_placeholder:
        st.markdown('ğŸ¤– AIê°€ ì‘ë‹µì„ ìƒì„±í•˜ê³  ìˆìŠµë‹ˆë‹¤...')
    
    # ì‚¬ìš©ì ë©”ì‹œì§€ë¥¼ ì„¸ì…˜ì— ì¶”ê°€
    st.session_state.messages.append({"role": "user", "content": user_input})
    
    # API ìš”ì²­ ë°ì´í„° ì¤€ë¹„
    request_data = {
        "messages": [
            {"role": msg["role"], "content": msg["content"]}
            for msg in st.session_state.messages
        ]
    }
    
    try:
        # API í˜¸ì¶œ
        response = requests.post(API_URL, json=request_data)
        response.raise_for_status()
        
        # ì‘ë‹µ ì²˜ë¦¬
        assistant_response = response.json()["response"]
        
        # === ì—¬ê¸°ì—ì„œ ìˆ˜ì‹ ë³€í™˜ ì ìš© ===
        assistant_response = latex_to_text(assistant_response)
        
        # ìŠ¤íŠ¸ë¦¬ë° ë°©ì‹ìœ¼ë¡œ ì‘ë‹µ í‘œì‹œ
        with st.chat_message("assistant"):
            streamed_response = stream_response(assistant_response, loading_placeholder)
            st.session_state.messages.append({"role": "assistant", "content": streamed_response})
        
    except Exception as e:
        loading_placeholder.empty()
        st.error(f"Error: {str(e)}")

def main():
    st.set_page_config(page_title="AI Agent Chat", page_icon="ğŸ¤–", layout="wide")
    if "messages" not in st.session_state:
        st.session_state.messages = []
    if "streaming" not in st.session_state:
        st.session_state.streaming = False

    tab1, tab2 = st.tabs(["ğŸ’¬ ì±„íŒ…", "ğŸ“ ë¬¸ì„œ ì—…ë¡œë“œ"])

    # ì±„íŒ… íƒ­
    with tab1:
        with st.sidebar:
            st.header("About")
            st.markdown("""
            This is a chat interface built with Streamlit.
            It uses OpenAI's GPT model through a FastAPI backend.
            """)
            if st.button("Clear Chat"):
                st.session_state.messages = []
                st.rerun()

        # í•­ìƒ ì±„íŒ… ì…ë ¥ì°½ì´ í•˜ë‹¨ì— ê³ ì •
        display_chat_history()
        user_input = st.chat_input("ë©”ì‹œì§€ë¥¼ ì…ë ¥í•˜ì„¸ìš”", key="chat_input", disabled=False)
        if user_input:
            send_message(user_input)
            st.rerun()

    # ë¬¸ì„œ ì—…ë¡œë“œ íƒ­ (ë³µì›)
    with tab2:
        st.header("ğŸ“ ë¬¸ì„œ ì—…ë¡œë“œ ë° ì¸ë±ìŠ¤ ìƒì„±")
        st.markdown("""
        ### ì§€ì› íŒŒì¼ í˜•ì‹
        - **PDF** (.pdf)
        - **Word ë¬¸ì„œ** (.docx) 
        - **í…ìŠ¤íŠ¸ íŒŒì¼** (.txt)

        ### íŒŒì¼ ë¶„ë¥˜ ê·œì¹™
        - íŒŒì¼ëª…ì— **'ë„ì‹œê°€ìŠ¤'** í¬í•¨ â†’ Gas ë¶„ë¥˜
        - íŒŒì¼ëª…ì— **'ì „ë ¥'** í¬í•¨ â†’ Power ë¶„ë¥˜  
        - ê¸°íƒ€ â†’ Other ë¶„ë¥˜
        """)
        uploaded_files = st.file_uploader(
            "ë¬¸ì„œ íŒŒì¼ì„ ì„ íƒí•˜ì„¸ìš”",
            type=['pdf', 'docx', 'txt'],
            accept_multiple_files=True,
            help="ì—¬ëŸ¬ íŒŒì¼ì„ ë™ì‹œì— ì—…ë¡œë“œí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤."
        )
        if uploaded_files:
            st.info(f"ğŸ“ {len(uploaded_files)}ê°œ íŒŒì¼ì´ ì„ íƒë˜ì—ˆìŠµë‹ˆë‹¤:")
            for file in uploaded_files:
                category = classify_file(file.name)
                st.write(f"- **{file.name}** â†’ **{category}** ë¶„ë¥˜")
                if category == 'other':
                    st.warning(f"  âš ï¸ '{file.name}'ì´ 'other'ë¡œ ë¶„ë¥˜ë˜ì—ˆìŠµë‹ˆë‹¤. íŒŒì¼ëª…ì— 'ë„ì‹œê°€ìŠ¤' ë˜ëŠ” 'ì „ë ¥'ì´ í¬í•¨ë˜ì–´ ìˆëŠ”ì§€ í™•ì¸í•´ì£¼ì„¸ìš”.")
        if st.button("ğŸš€ ì¸ë±ìŠ¤ ìƒì„± ì‹œì‘", type="primary", disabled=not uploaded_files):
            try:
                with st.spinner("ë¬¸ì„œë¥¼ ì²˜ë¦¬í•˜ê³  ì¸ë±ìŠ¤ë¥¼ ìƒì„±í•˜ê³  ìˆìŠµë‹ˆë‹¤..."):
                    success = build_vector_index_from_uploaded_files(uploaded_files)
                    if success:
                        st.balloons()
                        st.success("âœ… ì¸ë±ìŠ¤ ìƒì„±ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤!")
                        
                        # ë°±ì—”ë“œ ì¸ë±ìŠ¤ ë¦¬ë¡œë“œ
                        reload_success = reload_backend_indexes()
                        if reload_success:
                            st.success("ğŸ‰ ëª¨ë“  ì‘ì—…ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤! ì´ì œ ì±„íŒ… íƒ­ì—ì„œ ì§ˆë¬¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
                        else:
                            st.warning("âš ï¸ ì¸ë±ìŠ¤ëŠ” ìƒì„±ë˜ì—ˆì§€ë§Œ ë°±ì—”ë“œ ë¦¬ë¡œë“œì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤. FastAPI ì„œë²„ë¥¼ ì¬ì‹œì‘í•˜ê±°ë‚˜ ìˆ˜ë™ìœ¼ë¡œ ë¦¬ë¡œë“œí•´ì£¼ì„¸ìš”.")
                    else:
                        st.error("âŒ ì¸ë±ìŠ¤ ìƒì„±ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤. ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”.")
            except Exception as e:
                st.error(f"âŒ ì¸ë±ìŠ¤ ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}")
                st.error("API ì„œë²„ê°€ ì‹¤í–‰ë˜ì§€ ì•Šì•˜ì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤. FastAPI ì„œë²„ë¥¼ ë¨¼ì € ì‹¤í–‰í•´ì£¼ì„¸ìš”.")
        st.header("ğŸ“Š ê¸°ì¡´ ì¸ë±ìŠ¤ ì •ë³´")
        col1, col2, col3 = st.columns(3)
        with col1:
            if GAS_INDEX_DIR.exists() and any(GAS_INDEX_DIR.iterdir()):
                st.success("âœ… Gas ì¸ë±ìŠ¤ ì¡´ì¬")
            else:
                st.warning("âš ï¸ Gas ì¸ë±ìŠ¤ ì—†ìŒ")
        with col2:
            if POWER_INDEX_DIR.exists() and any(POWER_INDEX_DIR.iterdir()):
                st.success("âœ… Power ì¸ë±ìŠ¤ ì¡´ì¬")
            else:
                st.warning("âš ï¸ Power ì¸ë±ìŠ¤ ì—†ìŒ")
        with col3:
            if OTHER_INDEX_DIR.exists() and any(OTHER_INDEX_DIR.iterdir()):
                st.success("âœ… Other ì¸ë±ìŠ¤ ì¡´ì¬")
            else:
                st.warning("âš ï¸ Other ì¸ë±ìŠ¤ ì—†ìŒ")
        
        # ìˆ˜ë™ ë¦¬ë¡œë“œ ë²„íŠ¼
        st.header("ğŸ”„ ë°±ì—”ë“œ ì¸ë±ìŠ¤ ê´€ë¦¬")
        if st.button("ğŸ”„ ë°±ì—”ë“œ ì¸ë±ìŠ¤ ë¦¬ë¡œë“œ", type="secondary"):
            reload_backend_indexes()
        
        # ì €ì¥ëœ ë¬¸ì„œ íŒŒì¼ ëª©ë¡ í‘œì‹œ
        st.header("ğŸ“ ì €ì¥ëœ ë¬¸ì„œ íŒŒì¼")
        docs_dir = Path("/Users/a07198/IdeaProjects/MIS2/src/vectordb/docs")
        if docs_dir.exists() and any(docs_dir.iterdir()):
            files = list(docs_dir.glob("*"))
            if files:
                st.info(f"ğŸ“‚ ì´ {len(files)}ê°œ íŒŒì¼ì´ ì €ì¥ë˜ì–´ ìˆìŠµë‹ˆë‹¤:")
                for file in sorted(files):
                    file_size = file.stat().st_size
                    file_size_mb = file_size / (1024 * 1024)
                    category = classify_file(file.name)
                    st.write(f"- **{file.name}** ({category} ë¶„ë¥˜, {file_size_mb:.2f} MB)")
            else:
                st.info("ğŸ“‚ ì €ì¥ëœ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤.")
        else:
            st.info("ğŸ“‚ ë¬¸ì„œ ì €ì¥ ë””ë ‰í† ë¦¬ê°€ ì—†ìŠµë‹ˆë‹¤.")

if __name__ == "__main__":
    main() 