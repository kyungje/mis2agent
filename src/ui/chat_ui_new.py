# chat_ui_docinsight.py (DocInsight AI ìŠ¤íƒ€ì¼ ì ìš© ì „ì²´ ë²„ì „)
import streamlit as st
import requests
import time
import re
import os
from dotenv import load_dotenv

# ë¬¸ì„œ ì—…ë¡œë“œ ê´€ë ¨ ì¶”ê°€ import
import json
import pandas as pd
from typing import List, Dict, Any
import tempfile
import shutil
from pathlib import Path
import unicodedata

# ë¬¸ì„œ ì²˜ë¦¬ ë° ë²¡í„° ì¸ë±ìŠ¤ ìƒì„±ì„ ìœ„í•œ ëª¨ë“ˆë“¤
import pdfplumber
from docx import Document
from langchain_community.vectorstores import FAISS
from langchain_openai import OpenAIEmbeddings
from langchain_core.documents import Document as LCDocument
from langchain_experimental.text_splitter import SemanticChunker
import gc

# í˜ì´ì§€ ì„¤ì •
st.set_page_config(page_title="DocInsight AI", page_icon="ğŸ“„", layout="wide")
load_dotenv()
API_URL = "http://localhost:8000/chat"
RELOAD_API_URL = "http://localhost:8000/reload-indexes"

# === ë¬¸ì„œ ì—…ë¡œë“œë¥¼ ìœ„í•œ í•¨ìˆ˜ë“¤ (chat_ui.pyì—ì„œ ë³µì‚¬) ===

# ê²½ë¡œ ì„¤ì •
BASE_DIR = Path(__file__).parent.parent / "vectordb"
DOCS_DIR = BASE_DIR / "docs"
DB_DIR = BASE_DIR / "db"

# ì¸ë±ìŠ¤ ë¶„ë¥˜ë³„ ê²½ë¡œ ì„¤ì •
GAS_INDEX_DIR = DB_DIR / "gas_index"
POWER_INDEX_DIR = DB_DIR / "power_index"
OTHER_INDEX_DIR = DB_DIR / "other_index"

# í•„ìš”í•œ í´ë” ìƒì„±
DB_DIR.mkdir(exist_ok=True)
GAS_INDEX_DIR.mkdir(exist_ok=True)
POWER_INDEX_DIR.mkdir(exist_ok=True)
OTHER_INDEX_DIR.mkdir(exist_ok=True)

# OpenAI ì„ë² ë”© ëª¨ë¸ ì´ˆê¸°í™”
def create_embedding_model():
    """ë™ì ìœ¼ë¡œ ë°°ì¹˜ í¬ê¸°ë¥¼ ì¡°ì •í•˜ì—¬ ì„ë² ë”© ëª¨ë¸ì„ ìƒì„±í•©ë‹ˆë‹¤."""
    chunk_size = 1000
    
    try:
        embedding_model = OpenAIEmbeddings(
            model="text-embedding-3-small",
            chunk_size=chunk_size,
            max_retries=3
        )
        return embedding_model
    except Exception as e:
        if "max_tokens_per_request" in str(e):
            chunk_size = 500
            st.info(f"í† í° ì œí•œìœ¼ë¡œ ì¸í•´ ë°°ì¹˜ í¬ê¸°ë¥¼ {chunk_size}ë¡œ ì¡°ì •í•©ë‹ˆë‹¤.")
            return OpenAIEmbeddings(
                model="text-embedding-3-small",
                chunk_size=chunk_size,
                max_retries=3
            )
        else:
            raise e

# í…ìŠ¤íŠ¸ ë¶„í• ê¸° ì´ˆê¸°í™” - SemanticChunker ì‚¬ìš©
def create_text_splitter():
    """SemanticChunkerë¥¼ ìƒì„±í•©ë‹ˆë‹¤."""
    embedding_model = create_embedding_model()
    text_splitter = SemanticChunker(
        embeddings=embedding_model,
        breakpoint_threshold_type="percentile",  # ê¸°ë³¸ê°’
        buffer_size=1
    )
    return text_splitter

# ë¬¸ì„œ ì½ê¸° í•¨ìˆ˜ë“¤
def read_docx(path):
    """DOCX ë¬¸ì„œë¥¼ ì½ì–´ì„œ í…ìŠ¤íŠ¸ë¥¼ ì¶”ì¶œí•©ë‹ˆë‹¤."""
    doc = Document(path)
    text = "\n".join([para.text for para in doc.paragraphs if para.text.strip()])
    text = latex_to_text(text)
    return text

def read_pdf(path):
    """PDF ë¬¸ì„œë¥¼ ì½ì–´ì„œ ì „ì²´ í…ìŠ¤íŠ¸ë¥¼ ì¶”ì¶œí•©ë‹ˆë‹¤."""
    full_text = ""
    with pdfplumber.open(path) as pdf:
        for page_num, page in enumerate(pdf.pages, start=1):
            try:
                page_text = page.extract_text()
                if page_text:
                    page_text = latex_to_text(page_text)
                    page_text = normalize_text(page_text)
                    full_text += page_text + "\n"
            except Exception as e:
                st.warning(f"í˜ì´ì§€ {page_num} ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {str(e)}")
    return full_text

def read_txt(path):
    """TXT íŒŒì¼ì„ ì½ì–´ì„œ í…ìŠ¤íŠ¸ë¥¼ ì¶”ì¶œí•©ë‹ˆë‹¤."""
    with open(path, 'r', encoding='utf-8') as f:
        text = f.read()
    text = latex_to_text(text)
    return text

# í…ìŠ¤íŠ¸ ì •ê·œí™”
def normalize_text(text):
    """í…ìŠ¤íŠ¸ ì •ê·œí™” (í•œê¸€-ì˜ë¬¸ ê³µë°±, ìˆ˜ì‹ ê¸°í˜¸ ë³´ì¡´)"""
    # í•œê¸€-ì˜ë¬¸ ê³µë°± ì •ë¦¬
    text = re.sub(r'([ê°€-í£])([A-Za-z0-9])', r'\1 \2', text)
    text = re.sub(r'([A-Za-z0-9])([ê°€-í£])', r'\1 \2', text)

    # ìˆ˜ì‹ ê¸°í˜¸ ë³´ì¡´: âˆš Â± â‰ˆ âˆ Ã— Ã· Ï€ Â² Â³ ^ / = % ë“±
    math_symbols = "âˆšÂ±â‰ˆâˆÃ—Ã·Ï€Â²Â³^=/%"

    # ë” ê´€ëŒ€í•œ ì •ê·œí™”: í•œê¸€, ì˜ë¬¸, ìˆ«ì, ê³µë°±, ë¬¸ì¥ë¶€í˜¸, ìˆ˜ì‹ ê¸°í˜¸ë§Œ ìœ ì§€
    # í•œê¸€: ê°€-í£
    # ì˜ë¬¸: A-Za-z
    # ìˆ«ì: 0-9
    # ê³µë°±: \s
    # ë¬¸ì¥ë¶€í˜¸: .,!?;:-()[]{}
    # ìˆ˜ì‹ ê¸°í˜¸: âˆšÂ±â‰ˆâˆÃ—Ã·Ï€Â²Â³^=/%
    # ì¶”ê°€ í—ˆìš© ë¬¸ì: + (í”ŒëŸ¬ìŠ¤ ê¸°í˜¸)
    
    allowed_pattern = r'[ê°€-í£A-Za-z0-9\s.,!?;:\-\(\)\[\]\{\}' + re.escape(math_symbols + '+') + r']'
    text = re.sub(rf'[^{allowed_pattern}]', ' ', text)

    # ì—°ì† ê³µë°± ì •ë¦¬
    text = re.sub(r'\s+', ' ', text)

    return text.strip()

# íŒŒì¼ëª… ì •ê·œí™” í•¨ìˆ˜
def normalize_filename(file_name):
    """íŒŒì¼ëª…ì„ ì •ê·œí™”í•˜ì—¬ ë¶„ë¥˜ì— ì‚¬ìš©í•©ë‹ˆë‹¤."""
    # íŒŒì¼ í™•ì¥ì ì œê±°
    name_without_ext = os.path.splitext(file_name)[0]
    
    # íŠ¹ìˆ˜ë¬¸ì ì œê±° (í•˜ì´í”ˆ, ì–¸ë”ìŠ¤ì½”ì–´, í”ŒëŸ¬ìŠ¤ ë“±ì€ ê³µë°±ìœ¼ë¡œ ë³€í™˜)
    # ëŒ€ê´„í˜¸ëŠ” ì œê±°í•˜ë˜, í‚¤ì›Œë“œê°€ í¬í•¨ëœ ë¶€ë¶„ì€ ë³´ì¡´
    normalized = re.sub(r'[\[\]\(\)\+\-\_]', ' ', name_without_ext)
    
    # í•œê¸€ì´ ë¶„í•´ë˜ì§€ ì•Šë„ë¡ NFCë¡œ ì •ê·œí™”
    normalized = unicodedata.normalize('NFC', normalized)
    
    # í‚¤ì›Œë“œê°€ í¬í•¨ëœ ë¶€ë¶„ì´ ê³µë°±ìœ¼ë¡œ ë¶„ë¦¬ë˜ì—ˆëŠ”ì§€ í™•ì¸í•˜ê³  ë³µêµ¬
    if ' ì „ë ¥ ' in normalized or normalized.startswith('ì „ë ¥ ') or normalized.endswith(' ì „ë ¥'):
        # ì „ë ¥ í‚¤ì›Œë“œ ì£¼ë³€ì˜ ê³µë°±ì„ ì œê±°í•˜ì—¬ ë³µêµ¬
        normalized = re.sub(r'\s+ì „ë ¥\s+', 'ì „ë ¥', normalized)
        normalized = re.sub(r'^ì „ë ¥\s+', 'ì „ë ¥', normalized)
        normalized = re.sub(r'\s+ì „ë ¥$', 'ì „ë ¥', normalized)
    
    if ' ë„ì‹œê°€ìŠ¤ ' in normalized or normalized.startswith('ë„ì‹œê°€ìŠ¤ ') or normalized.endswith(' ë„ì‹œê°€ìŠ¤'):
        # ë„ì‹œê°€ìŠ¤ í‚¤ì›Œë“œ ì£¼ë³€ì˜ ê³µë°±ì„ ì œê±°í•˜ì—¬ ë³µêµ¬
        normalized = re.sub(r'\s+ë„ì‹œê°€ìŠ¤\s+', 'ë„ì‹œê°€ìŠ¤', normalized)
        normalized = re.sub(r'^ë„ì‹œê°€ìŠ¤\s+', 'ë„ì‹œê°€ìŠ¤', normalized)
        normalized = re.sub(r'\s+ë„ì‹œê°€ìŠ¤$', 'ë„ì‹œê°€ìŠ¤', normalized)
    
    # ì—°ì† ê³µë°±ì„ ë‹¨ì¼ ê³µë°±ìœ¼ë¡œ ë³€í™˜
    normalized = re.sub(r'\s+', ' ', normalized)
    
    # ì•ë’¤ ê³µë°± ì œê±°
    normalized = normalized.strip()
    
    return normalized

# íŒŒì¼ ë¶„ë¥˜ í•¨ìˆ˜
def classify_file(file_name):
    """íŒŒì¼ëª…ì— ë”°ë¼ ë¶„ë¥˜ë¥¼ ê²°ì •í•©ë‹ˆë‹¤."""
    # íŒŒì¼ëª… ì •ê·œí™”
    normalized_name = normalize_filename(file_name)
    
    # í‚¤ì›Œë“œ ê²€ìƒ‰ ê²°ê³¼ (ì—¬ëŸ¬ ë°©ë²•ìœ¼ë¡œ ê²€ìƒ‰)
    gas_found = 'ë„ì‹œê°€ìŠ¤' in normalized_name
    power_found = 'ì „ë ¥' in normalized_name
    
    # ëŒ€ì•ˆ ê²€ìƒ‰ ë°©ë²• (ì¸ì½”ë”© ë¬¸ì œ í•´ê²°ì„ ìœ„í•´)
    gas_found_alt = normalized_name.find('ë„ì‹œê°€ìŠ¤') != -1
    power_found_alt = normalized_name.find('ì „ë ¥') != -1
    
    # ìµœì¢… ê²°ê³¼ ê²°ì • (ë‘˜ ì¤‘ í•˜ë‚˜ë¼ë„ Trueë©´ True)
    gas_found = gas_found or gas_found_alt
    power_found = power_found or power_found_alt
    
    # ë¶„ë¥˜ ê²°ì •
    if gas_found:
        return 'gas'
    elif power_found:
        return 'power'
    else:
        # ì›ë³¸ íŒŒì¼ëª…ìœ¼ë¡œ ì¬ì‹œë„
        original_gas_found = 'ë„ì‹œê°€ìŠ¤' in file_name
        original_power_found = 'ì „ë ¥' in file_name
        
        if original_gas_found:
            return 'gas'
        elif original_power_found:
            return 'power'
        else:
            # ë§ˆì§€ë§‰ ì•ˆì „ì¥ì¹˜: íŒŒì¼ëª…ì˜ ëª¨ë“  ë¶€ë¶„ì„ ê°œë³„ì ìœ¼ë¡œ í™•ì¸
            file_parts = re.split(r'[_\-\s\[\]\(\)]', file_name)
            
            for part in file_parts:
                if 'ë„ì‹œê°€ìŠ¤' in part:
                    return 'gas'
                elif 'ì „ë ¥' in part:
                    return 'power'
            
            return 'other'

def get_index_dir(category):
    """ë¶„ë¥˜ì— ë”°ë¥¸ ì¸ë±ìŠ¤ ë””ë ‰í† ë¦¬ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤."""
    if category == 'gas':
        return GAS_INDEX_DIR
    elif category == 'power':
        return POWER_INDEX_DIR
    else:
        return OTHER_INDEX_DIR

def extract_metadata_from_filename(filename):
    base_name = os.path.splitext(filename)[0]
    parts = re.split(r'[_\-\s\+\(\)]', base_name)  # +ì™€ ()ë„ êµ¬ë¶„ìë¡œ ì¶”ê°€

    # ë²„ì „ ì¶”ì¶œ ê°œì„ : 2024, 24.01.01, 2024.7.1 ë“± ë‹¤ì–‘í•œ í˜•ì‹ ì§€ì›
    version = None
    for part in parts:
        # 4ìë¦¬ ì—°ë„ í¬í•¨ ë‚ ì§œ (2024.7.1) - ìš°ì„ ìˆœìœ„ 1
        if re.match(r'20\d{2}\.\d{1,2}\.\d{1,2}', part):
            version = part  # ì „ì²´ ë‚ ì§œ ë³´ì¡´
            break
        # 2ìë¦¬ ì—°ë„ í¬í•¨ ë‚ ì§œ (24.01.01) - ìš°ì„ ìˆœìœ„ 2
        elif re.match(r'\d{2}\.\d{1,2}\.\d{1,2}', part):
            year = part.split('.')[0]
            if int(year) >= 20:  # 20ë…„ ì´í›„
                version = f"20{part}"  # 2024.01.01 í˜•íƒœë¡œ ë³€í™˜
            break
        # 4ìë¦¬ ì—°ë„ë§Œ (2024) - ìš°ì„ ìˆœìœ„ 3
        elif re.match(r'20\d{2}$', part):
            version = part
            break
    
    region_list = ['ì„œìš¸', 'ë¶€ì‚°', 'ëŒ€êµ¬', 'ê´‘ì£¼', 'ì¸ì²œ', 'ëŒ€ì „', 'ìš¸ì‚°','ê²½ê¸°ë„', 'ê°•ì›ë„', 'ì¶©ì²­ë¶ë„', 'ì¶©ì²­ë‚¨ë„' ,'ì „ë¼ë‚¨ë„','ì „ë¶íŠ¹ë³„ìì¹˜ë„', 'ê²½ìƒë‚¨ë„', 'ê²½ìƒë¶ë„']
    region = next((p for p in parts if p in region_list), None)
    organization_map = {'ë„ì‹œê°€ìŠ¤': 'ë„ì‹œê°€ìŠ¤', 'ì „ë ¥': 'ì „ë ¥'}
    organization = next((p for p in organization_map if p in parts), "ê¸°íƒ€")

    return {
        "version": version,
        "region": region,
        "organization": organization,
        "title": base_name
    }

# ë¬¸ì„œ ì²˜ë¦¬ ë° ì²­í‚¹
def process_document(file_path):
    """ë¬¸ì„œë¥¼ ì½ê³  SemanticChunkerë¥¼ ì‚¬ìš©í•˜ì—¬ ì²­í‚¹í•©ë‹ˆë‹¤."""
    filename = os.path.basename(file_path)
    
    # ë¬¸ì„œ ì½ê¸°
    if file_path.endswith(".pdf"):
        text = read_pdf(file_path)
    elif file_path.endswith(".docx"):
        text = read_docx(file_path)
    elif file_path.endswith(".txt"):
        text = read_txt(file_path)
    else:
        return []
    
    # í…ìŠ¤íŠ¸ê°€ ë¹„ì–´ìˆìœ¼ë©´ ì²˜ë¦¬ ì¤‘ë‹¨
    if not text:
        st.warning(f"âš ï¸ {filename}ì—ì„œ í…ìŠ¤íŠ¸ë¥¼ ì¶”ì¶œí•˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤. ê±´ë„ˆëœë‹ˆë‹¤.")
        return []
    
    # í…ìŠ¤íŠ¸ ì •ê·œí™”
    original_text = text
    text = normalize_text(text)
    
    # ì •ê·œí™” ê³¼ì •ì—ì„œ í…ìŠ¤íŠ¸ê°€ ë„ˆë¬´ ë§ì´ ì œê±°ë˜ì—ˆëŠ”ì§€ í™•ì¸
    if len(text) < len(original_text) * 0.1:  # 90% ì´ìƒ ì œê±°ëœ ê²½ìš°
        st.warning(f"  âš ï¸ í…ìŠ¤íŠ¸ê°€ ë„ˆë¬´ ë§ì´ ì œê±°ë˜ì—ˆìŠµë‹ˆë‹¤. ì›ë³¸: {len(original_text)}ì â†’ ì •ê·œí™”: {len(text)}ì")
        # ì •ê·œí™”ë¥¼ ê±´ë„ˆë›°ê³  ì›ë³¸ í…ìŠ¤íŠ¸ ì‚¬ìš©
        text = original_text
    
    # SemanticChunker ì‚¬ìš©
    text_splitter = create_text_splitter()
    chunks = text_splitter.split_text(text)
    
    # íŒŒì¼ëª…ì—ì„œ ì¶”ê°€ ë©”íƒ€ë°ì´í„° ì¶”ì¶œ
    additional_metadata = extract_metadata_from_filename(filename)
    
    # ë©”íƒ€ë°ì´í„°ê°€ í¬í•¨ëœ LangChain ë¬¸ì„œë¡œ ë³€í™˜
    documents = []
    for i, chunk in enumerate(chunks):
        if len(chunk.strip()) > 50:  # ìµœì†Œ ê¸¸ì´ í•„í„° (50ì ì´ìƒ)
            # ê¸°ë³¸ ë©”íƒ€ë°ì´í„°ì™€ ì¶”ê°€ ë©”íƒ€ë°ì´í„°ë¥¼ ê²°í•©
            metadata = {
                "source": filename,
                "chunk_id": i,
                "file_path": file_path,
                "version": additional_metadata["version"],
                "region": additional_metadata["region"],
                "organization": additional_metadata["organization"],
                "title": additional_metadata["title"]
            }
            doc = LCDocument(
                page_content=chunk,
                metadata=metadata
            )
            documents.append(doc)
    
    return documents

# ë²¡í„° ì¸ë±ìŠ¤ êµ¬ì¶•
def build_vector_index_from_uploaded_files(uploaded_files):
    """ì—…ë¡œë“œëœ íŒŒì¼ë“¤ë¡œë¶€í„° ë²¡í„° ì¸ë±ìŠ¤ë¥¼ êµ¬ì¶•í•©ë‹ˆë‹¤."""
    if not uploaded_files:
        st.warning("ì—…ë¡œë“œëœ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤.")
        return False
    
    # ë‹¨ê³„ë³„ ë©”ì‹œì§€ë¥¼ ìœ„í•œ í”Œë ˆì´ìŠ¤í™€ë”ë“¤
    main_status_placeholder = st.empty()
    file_status_placeholder = st.empty()
    index_status_placeholder = st.empty()
    
    main_status_placeholder.info("ğŸ“‚ ë¬¸ì„œ ì¸ë±ì‹± ì‹œì‘")
    
    # ë¬¸ì„œ ì €ì¥ ë””ë ‰í† ë¦¬ ìƒì„±
    docs_dir = Path(__file__).parent.parent.parent / "vectordb" / "docs"
    docs_dir.mkdir(parents=True, exist_ok=True)
    
    # ì„ë² ë”© ëª¨ë¸ ì´ˆê¸°í™”
    embedding_model = create_embedding_model()
    
    # ë¶„ë¥˜ë³„ë¡œ ë¬¸ì„œ ê·¸ë£¹í™”
    gas_documents = []
    power_documents = []
    other_documents = []
    
    total_files = len(uploaded_files)
    
    # íŒŒì¼ë³„ë¡œ ì²˜ë¦¬ ë° ë¶„ë¥˜
    for i, uploaded_file in enumerate(uploaded_files, 1):
        file_status_placeholder.info(f"[{i}/{total_files}] ì²˜ë¦¬ ì¤‘: {uploaded_file.name}")
        
        # ì‹¤ì œ íŒŒì¼ì„ docs ë””ë ‰í† ë¦¬ì— ì €ì¥
        file_path = docs_dir / uploaded_file.name
        
        # ì´ë¯¸ ë™ì¼í•œ íŒŒì¼ëª…ì´ ì¡´ì¬í•˜ëŠ”ì§€ í™•ì¸
        if file_path.exists():
            file_status_placeholder.empty()
            main_status_placeholder.empty()
            st.warning(f"âš ï¸ '{uploaded_file.name}' íŒŒì¼ì´ ì´ë¯¸ ì¡´ì¬í•©ë‹ˆë‹¤. ì¸ë±ìŠ¤ ìƒì„±ì„ ê±´ë„ˆëœë‹ˆë‹¤.")
            return False
        
        with open(file_path, "wb") as f:
            f.write(uploaded_file.getvalue())
        
        try:
            # íŒŒì¼ ë¶„ë¥˜
            category = classify_file(uploaded_file.name)
            
            documents = process_document(str(file_path))
            
            # ë¶„ë¥˜ë³„ë¡œ ë¬¸ì„œ ì¶”ê°€
            if category == 'gas':
                gas_documents.extend(documents)
            elif category == 'power':
                power_documents.extend(documents)
            else:
                other_documents.extend(documents)
            
            # ë©”ëª¨ë¦¬ ìµœì í™”
            del documents
            gc.collect()
            
        except Exception as e:
            file_status_placeholder.empty()
            main_status_placeholder.empty()
            st.error(f"âŒ íŒŒì¼ ì²˜ë¦¬ ì˜¤ë¥˜: {e}")
            # ì˜¤ë¥˜ ë°œìƒ ì‹œ ì €ì¥ëœ íŒŒì¼ ì‚­ì œ
            if file_path.exists():
                file_path.unlink()
            continue
    
    # íŒŒì¼ ì²˜ë¦¬ ì™„ë£Œ ë©”ì‹œì§€ ì‚­ì œ
    file_status_placeholder.empty()
    
    # ë¶„ë¥˜ë³„ë¡œ ì¸ë±ìŠ¤ ìƒì„±
    categories = [
        ('gas', gas_documents, 'Gas'),
        ('power', power_documents, 'Power'),
        ('other', other_documents, 'Other')
    ]
    
    success_count = 0
    for category, documents, category_name in categories:
        if len(documents) == 0:
            continue
            
        index_status_placeholder.info(f"ğŸ”§ {category_name} ì¸ë±ìŠ¤ ìƒì„± ì¤‘... (ë¬¸ì„œ ìˆ˜: {len(documents)}ê°œ)")
        
        # ë°°ì¹˜ ë‹¨ìœ„ë¡œ ì„ë² ë”© ì²˜ë¦¬
        try:
            vectorstore = FAISS.from_documents(documents, embedding_model)
        except Exception as e:
            if "max_tokens_per_request" in str(e):
                try:
                    medium_embedding_model = OpenAIEmbeddings(
                        model="text-embedding-3-small",
                        chunk_size=500,
                        max_retries=3
                    )
                    vectorstore = FAISS.from_documents(documents, medium_embedding_model)
                except Exception as e2:
                    if "max_tokens_per_request" in str(e2):
                        small_embedding_model = OpenAIEmbeddings(
                            model="text-embedding-3-small",
                            chunk_size=100,
                            max_retries=3
                        )
                        vectorstore = FAISS.from_documents(documents, small_embedding_model)
                    else:
                        raise e2
            else:
                raise e

        # ì¸ë±ìŠ¤ ì €ì¥
        index_dir = get_index_dir(category)
        try:
            vectorstore.save_local(str(index_dir))
            index_status_placeholder.success(f"ğŸ’¾ {category_name} ì¸ë±ìŠ¤ ì €ì¥ ì™„ë£Œ: {index_dir}")
            success_count += 1
        except Exception as e:
            index_status_placeholder.empty()
            main_status_placeholder.empty()
            st.error(f"âŒ {category_name} ì¸ë±ìŠ¤ ì €ì¥ ì˜¤ë¥˜: {e}")
            return False
    
    # ëª¨ë“  ë‹¨ê³„ ì™„ë£Œ í›„ í”Œë ˆì´ìŠ¤í™€ë”ë“¤ ì •ë¦¬
    main_status_placeholder.empty()
    index_status_placeholder.empty()
    
    if success_count > 0:
        st.success(f"ğŸ‰ {success_count}ê°œ ë¶„ë¥˜ë³„ ì¸ë±ìŠ¤ ìƒì„± ì™„ë£Œ")
        return True
    else:
        st.error("âŒ ì¸ë±ìŠ¤ ìƒì„±ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.")
        return False

def reload_backend_indexes():
    """ë°±ì—”ë“œì˜ ì¸ë±ìŠ¤ë¥¼ ë‹¤ì‹œ ë¡œë“œí•©ë‹ˆë‹¤."""
    # ë¡œë”© ë©”ì‹œì§€ë¥¼ ìœ„í•œ í”Œë ˆì´ìŠ¤í™€ë” ìƒì„±
    loading_placeholder = st.empty()
    
    try:
        loading_placeholder.info("ğŸ”„ ë°±ì—”ë“œ ì¸ë±ìŠ¤ ë¦¬ë¡œë“œ ì¤‘...")
        response = requests.post(RELOAD_API_URL)
        response.raise_for_status()
        
        result = response.json()
        if result.get("success"):
            loading_placeholder.empty()  # ë¡œë”© ë©”ì‹œì§€ ì‚­ì œ
            st.success("âœ… ë°±ì—”ë“œ ì¸ë±ìŠ¤ ë¦¬ë¡œë“œ ì™„ë£Œ!")
            return True
        else:
            loading_placeholder.empty()  # ë¡œë”© ë©”ì‹œì§€ ì‚­ì œ
            st.error(f"âŒ ë°±ì—”ë“œ ì¸ë±ìŠ¤ ë¦¬ë¡œë“œ ì‹¤íŒ¨: {result.get('message', 'ì•Œ ìˆ˜ ì—†ëŠ” ì˜¤ë¥˜')}")
            return False
    except Exception as e:
        loading_placeholder.empty()  # ë¡œë”© ë©”ì‹œì§€ ì‚­ì œ
        st.error(f"âŒ ë°±ì—”ë“œ ì¸ë±ìŠ¤ ë¦¬ë¡œë“œ ì¤‘ ì˜¤ë¥˜: {str(e)}")
        st.warning("âš ï¸ FastAPI ì„œë²„ê°€ ì‹¤í–‰ ì¤‘ì¸ì§€ í™•ì¸í•´ì£¼ì„¸ìš”.")
        return False

def latex_to_text(text):
    """
    LaTeX ìˆ˜ì‹ì„ ì‚¬ëŒì´ ì½ëŠ” í…ìŠ¤íŠ¸ ìˆ˜ì‹ìœ¼ë¡œ ë³€í™˜
    ì˜ˆ: \frac{a}{b} â†’ (a) / (b)
    """
    # \frac ë³€í™˜ í•¨ìˆ˜
    def frac_repl(match):
        return f"({match.group(1)}) / ({match.group(2)})"

    # LaTeX ë¸”ë¡(\[...\], $$...$$, $...$)ì„ ì°¾ì•„ì„œ ë³€í™˜
    def latex_block_repl(match):
        latex_expr = match.group(1)
        # \leftì™€ \right ì œê±° (ê´„í˜¸ í¬ê¸° ì¡°ì • ëª…ë ¹ì–´) - ë” í¬ê´„ì ìœ¼ë¡œ ì²˜ë¦¬
        latex_expr = re.sub(r'\\left\s*\(', '(', latex_expr)
        latex_expr = re.sub(r'\\right\s*\)', ')', latex_expr)
        latex_expr = re.sub(r'\\left\s*\[', '[', latex_expr)
        latex_expr = re.sub(r'\\right\s*\]', ']', latex_expr)
        latex_expr = re.sub(r'\\left\s*\\{', '{', latex_expr)
        latex_expr = re.sub(r'\\right\s*\\}', '}', latex_expr)
        # \frac ë³€í™˜
        latex_expr = re.sub(r'\\frac\{(.+?)\}\{(.+?)\}', frac_repl, latex_expr)
        # \times ë³€í™˜
        latex_expr = latex_expr.replace(r'\times', 'Ã—')
        # ì¤‘ê´„í˜¸ ì œê±°
        latex_expr = latex_expr.replace('{', '').replace('}', '')
        return latex_expr

    # \[ ... \] ë¸”ë¡ ë³€í™˜
    text = re.sub(r'\\\[(.*?)\\\]', lambda m: latex_block_repl(m), text, flags=re.DOTALL)
    # $$ ... $$ ë¸”ë¡ ë³€í™˜
    text = re.sub(r'\$\$(.*?)\$\$', lambda m: latex_block_repl(m), text, flags=re.DOTALL)
    # $ ... $ ë¸”ë¡ ë³€í™˜
    text = re.sub(r'\$(.*?)\$', lambda m: latex_block_repl(m), text, flags=re.DOTALL)

    # ì¸ë¼ì¸ ë³€í™˜ (í˜¹ì‹œ ë‚¨ì•„ìˆì„ ê²½ìš°)
    # \leftì™€ \right ì œê±° - ë” í¬ê´„ì ìœ¼ë¡œ ì²˜ë¦¬
    text = re.sub(r'\\left\s*\(', '(', text)
    text = re.sub(r'\\right\s*\)', ')', text)
    text = re.sub(r'\\left\s*\[', '[', text)
    text = re.sub(r'\\right\s*\]', ']', text)
    text = re.sub(r'\\left\s*\\{', '{', text)
    text = re.sub(r'\\right\s*\\}', '}', text)
    # \frac ë³€í™˜
    text = re.sub(r'\\frac\{(.+?)\}\{(.+?)\}', frac_repl, text)
    text = text.replace(r'\times', 'Ã—')
    text = text.replace('{', '').replace('}', '')

    return text

def initialize_session_state():
    if "messages" not in st.session_state:
        st.session_state.messages = []
    if "streaming" not in st.session_state:
        st.session_state.streaming = False
    if "current_page" not in st.session_state:
        st.session_state.current_page = "chat"
    if "api_processing" not in st.session_state:
        st.session_state.api_processing = False

def display_chat_history():
    for message in st.session_state.messages:
        with st.chat_message(message["role"]):
            st.markdown(message["content"])

def stream_response(response_text: str, loading_placeholder):
    message_placeholder = st.empty()
    full_response = ""
    loading_placeholder.empty()
    for char in response_text:
        full_response += char
        message_placeholder.markdown(full_response + "â–Œ")
        time.sleep(0.01)
    message_placeholder.markdown(full_response)
    return full_response

def send_message(user_input: str):
    if not user_input:
        return
    
    # API ì²˜ë¦¬ ìƒíƒœ ì‹œì‘ (ì¦‰ì‹œ ì„¤ì •)
    st.session_state.api_processing = True
    
    # ì• ë‹ˆë©”ì´ì…˜ ìƒíƒœ ì´ˆê¸°í™”
    if "animation_step" not in st.session_state:
        st.session_state.animation_step = 0
    
    # ì‚¬ìš©ì ë©”ì‹œì§€ í‘œì‹œ
    with st.chat_message("user"):
        st.markdown(user_input)
    
    # ë¡œë”© ë©”ì‹œì§€ë¥¼ ìœ„í•œ í”Œë ˆì´ìŠ¤í™€ë” 
    loading_placeholder = st.empty()
    
    # ë©”ì‹œì§€ë¥¼ ì„¸ì…˜ì— ì¶”ê°€
    st.session_state.messages.append({"role": "user", "content": user_input})

    # API ìš”ì²­ ë°ì´í„° ì¤€ë¹„
    request_data = {
        "messages": [
            {"role": msg["role"], "content": msg["content"]}
            for msg in st.session_state.messages
        ]
    }
    
    try:
        # ì—°ì†ëœ ì• ë‹ˆë©”ì´ì…˜ê³¼ API í˜¸ì¶œì„ ë™ì‹œì— ì²˜ë¦¬
        dot_patterns = ["", ".", "..", "...", "...."]
        
        # API í˜¸ì¶œì„ ë³„ë„ ìŠ¤ë ˆë“œì—ì„œ ì‹¤í–‰
        import concurrent.futures
        import threading
        
        result_container = {"response": None, "error": None, "completed": False}
        
        def api_call():
            try:
                response = requests.post(API_URL, json=request_data)
                response.raise_for_status()
                result_container["response"] = response.json()["response"]
            except Exception as e:
                result_container["error"] = str(e)
            finally:
                result_container["completed"] = True
        
        # API í˜¸ì¶œ ìŠ¤ë ˆë“œ ì‹œì‘
        api_thread = threading.Thread(target=api_call, daemon=True)
        api_thread.start()
        
        # ì• ë‹ˆë©”ì´ì…˜ ì‹¤í–‰ (API ì™„ë£Œê¹Œì§€)
        cycle = 0
        while not result_container["completed"]:
            dots = dot_patterns[cycle % len(dot_patterns)]
            loading_placeholder.markdown(
                f"<div style='font-size: 1rem; color: #6b7280;'>ğŸ” AIê°€ ì‘ë‹µì„ ìƒì„±í•˜ê³  ìˆìŠµë‹ˆë‹¤{dots}</div>",
                unsafe_allow_html=True
            )
            time.sleep(0.4)
            cycle += 1
        
        # API í˜¸ì¶œ ì™„ë£Œ ëŒ€ê¸°
        api_thread.join()
        
        if result_container["error"]:
            raise Exception(result_container["error"])
        
        # ì‘ë‹µ ì²˜ë¦¬
        assistant_response = result_container["response"]
        
        # ìˆ˜ì‹ ë³€í™˜ ì ìš©
        assistant_response = latex_to_text(assistant_response)
        
        # API ì²˜ë¦¬ ì™„ë£Œ ìƒíƒœë¡œ ë³€ê²½
        st.session_state.api_processing = False
        
        # ìŠ¤íŠ¸ë¦¬ë° ë°©ì‹ìœ¼ë¡œ ì‘ë‹µ í‘œì‹œ
        with st.chat_message("assistant"):
            streamed_response = stream_response(assistant_response, loading_placeholder)
            st.session_state.messages.append({"role": "assistant", "content": streamed_response})
        
    except Exception as e:
        # ì˜¤ë¥˜ ë°œìƒ ì‹œì—ë„ API ì²˜ë¦¬ ì™„ë£Œ ìƒíƒœë¡œ ë³€ê²½
        st.session_state.api_processing = False
        loading_placeholder.empty()
        st.error(f"Error: {str(e)}")
    
    # ìµœì¢…ì ìœ¼ë¡œ API ì²˜ë¦¬ ìƒíƒœ í™•ì‹¤íˆ ì¢…ë£Œ
    st.session_state.api_processing = False

def show_upload_page():
    """ë¬¸ì„œ ì—…ë¡œë“œ í˜ì´ì§€ë¥¼ í‘œì‹œí•©ë‹ˆë‹¤."""
    # ë‚´ë¶€ ë¡œê³  ì˜ì—­ (ì±„íŒ… í™”ë©´ê³¼ ë™ì¼í•œ ìŠ¤íƒ€ì¼)
    st.markdown("""
        <div style="background-color:#ffffff; padding: 1rem 2rem; border-radius: 6px; margin-bottom: 1rem; display: flex; align-items: center; border: 1px solid #e5e7eb; box-shadow: 0 2px 4px rgba(0,0,0,0.04);">
            <img src="https://img.icons8.com/ios-filled/50/2d9bf0/document--v1.png" width="24px" style="margin-right: 10px;" />
            <span style="font-size: 1.3rem; font-weight: bold; color: #111827;">ë¬¸ì„œ ì—…ë¡œë“œ </span>
        </div>
        <div style="color: #666666; font-size: 0.95rem; margin-bottom: 1.5rem;">
            ë¬¸ì„œë¥¼ ì—…ë¡œë“œ í•´ AIë¥¼ í•™ìŠµì‹œì¼œë³´ì„¸ìš”.
        </div>
    """, unsafe_allow_html=True)

    st.markdown("""
        <div style="background-color: rgba(30, 41, 59, 0.9); padding: 1rem 1.5rem; border-radius: 10px; border: 1px solid #1f2937; color: #f3f4f6; font-size: 0.95rem; line-height: 1.7;">
        <strong style="font-size: 1.1rem; color: #ffffff;">ğŸ“Œ íŒŒì¼ ë¶„ë¥˜ ê·œì¹™</strong>
        <ul style="list-style-type: 'ğŸ“‚ '; padding-left: 1.2em; margin: 0;">
            <li><b>ë„ì‹œê°€ìŠ¤</b> í‚¤ì›Œë“œê°€ íŒŒì¼ëª…ì— í¬í•¨ â†’ <span style="color: #38bdf8;"><b>Gas</b></span> ë¶„ë¥˜</li>
            <li><b>ì „ë ¥</b> í‚¤ì›Œë“œê°€ íŒŒì¼ëª…ì— í¬í•¨ â†’ <span style="color: #38bdf8;"><b>Power</b></span> ë¶„ë¥˜</li>
            <li>ê·¸ ì™¸ í‚¤ì›Œë“œì¼ ê²½ìš° â†’ <span style="color: #facc15;"><b>Other</b></span> ë¶„ë¥˜</li>
        </ul>
        </div>
    """, unsafe_allow_html=True)
    
    st.write("")  # ê³µë°± ì¶”ê°€
    
    uploaded_files = st.file_uploader(
        "ë¬¸ì„œ íŒŒì¼ì„ ì„ íƒí•˜ì„¸ìš”",
        type=['pdf', 'docx', 'txt'],
        accept_multiple_files=True,
        help="ì—¬ëŸ¬ íŒŒì¼ì„ ë™ì‹œì— ì—…ë¡œë“œí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.",
        label_visibility="collapsed"
    )
    # st.caption("PDF, DOCX, TXT íŒŒì¼ì„ ì—…ë¡œë“œ ê°€ëŠ¥ í•©ë‹ˆë‹¤. (ìµœëŒ€ 200MB)")

    st.write("")  # ê³µë°± ì¶”ê°€

    if uploaded_files:
        file_data = []

        for file in uploaded_files:
            file_name = file.name
            file_size_kb = len(file.getvalue()) / 1024
            category = classify_file(file_name)

            warning_msg = ""
            if category == "other":
                warning_msg = "âš ï¸ ë¶„ë¥˜ ë¶ˆí™•ì‹¤"

            file_data.append({
                "íŒŒì¼ëª…": file_name,
                "í¬ê¸°": f"{file_size_kb:.1f} KB" if file_size_kb < 1024 else f"{file_size_kb/1024:.2f} MB",
                "ë¶„ë¥˜": warning_msg if warning_msg else category
            })

        df = pd.DataFrame(file_data)
        st.markdown("""
            <div style="background-color:#ffffff; padding: 1rem 2rem; border-radius: 6px; margin-bottom: 1rem; display: flex; align-items: center; border: 1px solid #e5e7eb; box-shadow: 0 2px 4px rgba(0,0,0,0.04);">
                <img src="https://img.icons8.com/ios-filled/50/2d9bf0/database--v1.png" width="24px" style="margin-right: 10px;" />
                <span style="font-size: 1.3rem; font-weight: bold; color: #111827;">ğŸ“„ ì—…ë¡œë“œëœ ë¬¸ì„œ</span>
            </div>
        """, unsafe_allow_html=True)
        st.table(df)
 
    if st.button("â–¶ï¸ AI ë¬¸ì„œ í•™ìŠµ ì‹œì‘", type="primary", disabled=not uploaded_files):
        try:
            with st.spinner("ë¬¸ì„œë¥¼ ì²˜ë¦¬í•˜ê³  ì¸ë±ìŠ¤ë¥¼ ìƒì„±í•˜ê³  ìˆìŠµë‹ˆë‹¤..."):
                success = build_vector_index_from_uploaded_files(uploaded_files)
                if success:
                    st.success("âœ… ì¸ë±ìŠ¤ ìƒì„±ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤!")
                    
                    # ë°±ì—”ë“œ ì¸ë±ìŠ¤ ë¦¬ë¡œë“œ
                    reload_success = reload_backend_indexes()
                    if reload_success:
                        st.success("ğŸ‰ ëª¨ë“  ì‘ì—…ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤! ì´ì œ ì±„íŒ… íƒ­ì—ì„œ ì§ˆë¬¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
                    else:
                        st.warning("âš ï¸ ì¸ë±ìŠ¤ëŠ” ìƒì„±ë˜ì—ˆì§€ë§Œ ë°±ì—”ë“œ ë¦¬ë¡œë“œì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤. FastAPI ì„œë²„ë¥¼ ì¬ì‹œì‘í•˜ê±°ë‚˜ ìˆ˜ë™ìœ¼ë¡œ í•™ìŠµ ë¬¸ì„œ ì •ë³´ ë¦¬ë¡œë“œ] ë²„íŠ¼ì„ ëˆŒëŸ¬ ì£¼ì„¸ìš”.")
                else:
                    st.error("âŒ ì¸ë±ìŠ¤ ìƒì„±ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤. ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”.")
        except Exception as e:
            st.error(f"âŒ ì¸ë±ìŠ¤ ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}")
            st.error("API ì„œë²„ê°€ ì‹¤í–‰ë˜ì§€ ì•Šì•˜ì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤. FastAPI ì„œë²„ë¥¼ ë¨¼ì € ì‹¤í–‰í•´ì£¼ì„¸ìš”.")
    
    st.write("")  # ê³µë°± ì¶”ê°€

    # ê¸°ì¡´ í•™ìŠµ ì •ë³´ í—¤ë”
    st.markdown("""
        <div style="background-color:#ffffff; padding: 1rem 2rem; border-radius: 6px; margin-bottom: 1rem; display: flex; align-items: center; border: 1px solid #e5e7eb; box-shadow: 0 2px 4px rgba(0,0,0,0.04);">
            <img src="https://img.icons8.com/ios-filled/50/2d9bf0/database--v1.png" width="24px" style="margin-right: 10px;" />
            <span style="font-size: 1.3rem; font-weight: bold; color: #111827;">í•™ìŠµ ì™„ë£Œëœ ë¬¸ì„œ ì •ë³´</span>
        </div>
    """, unsafe_allow_html=True)
    
    col1, col2, col3 = st.columns(3)
    
    # Gas ì¸ë±ìŠ¤ ì¹´ë“œ
    with col1:
        if GAS_INDEX_DIR.exists() and any(GAS_INDEX_DIR.iterdir()):
            # ì €ì¥ëœ ë¬¸ì„œ ìˆ˜ ê³„ì‚°
            docs_dir = Path(__file__).parent.parent.parent / "vectordb" / "docs"
            gas_docs = [f for f in docs_dir.iterdir() if f.is_file() and classify_file(f.name) == 'gas'] if docs_dir.exists() else []
            doc_count = len(gas_docs)
            
            # ì‹¤ì œ ë§ˆì§€ë§‰ ì—…ë°ì´íŠ¸ ë‚ ì§œ ê³„ì‚°
            import datetime
            index_files = list(GAS_INDEX_DIR.iterdir())
            if index_files:
                latest_time = max(f.stat().st_mtime for f in index_files)
                last_update = datetime.datetime.fromtimestamp(latest_time).strftime("%Y-%m-%d")
            else:
                last_update = "ì•Œ ìˆ˜ ì—†ìŒ"
            
            st.markdown(f"""
                <div style="background-color: #f8f9fa; padding: 1.5rem; border-radius: 10px; border: 1px solid #e9ecef; text-align: center;">
                    <div style="font-weight: bold; font-size: 1.1rem; color: #333; margin-bottom: 0.5rem;">âœ… Gas ë¬¸ì„œ í•™ìŠµ ì™„ë£Œ</div>
                    <div style="color: #666; font-size: 0.9rem; margin-bottom: 0.3rem;">í•™ìŠµ ì™„ë£Œ ë¬¸ì„œ {doc_count}ê±´</div>
                    <div style="color: #999; font-size: 0.8rem;">ë§ˆì§€ë§‰ ì—…ë°ì´íŠ¸: {last_update}</div>
                </div>
            """, unsafe_allow_html=True)
        else:
            st.markdown(f"""
                <div style="background-color: #f8f9fa; padding: 1.5rem; border-radius: 10px; border: 1px solid #e9ecef; text-align: center;">
                    <div style="font-weight: bold; font-size: 1.1rem; color: #333; margin-bottom: 0.5rem;">âš ï¸ Gas ë¬¸ì„œ í•™ìŠµ ë¯¸ì™„ë£Œ</div>
                    <div style="color: #666; font-size: 0.9rem; margin-bottom: 0.3rem;">í•™ìŠµ ì™„ë£Œ ë¬¸ì„œ 0ê±´</div>
                    <div style="color: #999; font-size: 0.8rem;"></div>
                </div>
            """, unsafe_allow_html=True)
    
    # Power ì¸ë±ìŠ¤ ì¹´ë“œ
    with col2:
        if POWER_INDEX_DIR.exists() and any(POWER_INDEX_DIR.iterdir()):
            # ì €ì¥ëœ ë¬¸ì„œ ìˆ˜ ê³„ì‚°
            docs_dir = Path(__file__).parent.parent.parent / "vectordb" / "docs"
            power_docs = [f for f in docs_dir.iterdir() if f.is_file() and classify_file(f.name) == 'power'] if docs_dir.exists() else []
            doc_count = len(power_docs)
            
            # ì‹¤ì œ ë§ˆì§€ë§‰ ì—…ë°ì´íŠ¸ ë‚ ì§œ ê³„ì‚°
            import datetime
            index_files = list(POWER_INDEX_DIR.iterdir())
            if index_files:
                latest_time = max(f.stat().st_mtime for f in index_files)
                last_update = datetime.datetime.fromtimestamp(latest_time).strftime("%Y-%m-%d")
            else:
                last_update = "ì•Œ ìˆ˜ ì—†ìŒ"
            
            st.markdown(f"""
                <div style="background-color: #f8f9fa; padding: 1.5rem; border-radius: 10px; border: 1px solid #e9ecef; text-align: center;">
                    <div style="font-weight: bold; font-size: 1.1rem; color: #333; margin-bottom: 0.5rem;">âœ… Power ë¬¸ì„œ í•™ìŠµ ì™„ë£Œ</div>
                    <div style="color: #666; font-size: 0.9rem; margin-bottom: 0.3rem;">í•™ìŠµ ì™„ë£Œ ë¬¸ì„œ {doc_count}ê±´</div>
                    <div style="color: #999; font-size: 0.8rem;">ë§ˆì§€ë§‰ ì—…ë°ì´íŠ¸: {last_update}</div>
                </div>
            """, unsafe_allow_html=True)
        else:
            st.markdown(f"""
                <div style="background-color: #f8f9fa; padding: 1.5rem; border-radius: 10px; border: 1px solid #e9ecef; text-align: center;">
                    <div style="font-weight: bold; font-size: 1.1rem; color: #333; margin-bottom: 0.5rem;">âš ï¸ Power ë¬¸ì„œ í•™ìŠµ ë¯¸ì™„ë£Œ</div>
                    <div style="color: #666; font-size: 0.9rem; margin-bottom: 0.3rem;">í•™ìŠµ ì™„ë£Œ ë¬¸ì„œ 0ê±´</div>
                    <div style="color: #999; font-size: 0.8rem;"></div>
                </div>
            """, unsafe_allow_html=True)
    
    # Other ì¸ë±ìŠ¤ ì¹´ë“œ
    with col3:
        if OTHER_INDEX_DIR.exists() and any(OTHER_INDEX_DIR.iterdir()):
            # ì €ì¥ëœ ë¬¸ì„œ ìˆ˜ ê³„ì‚°
            docs_dir = Path(__file__).parent.parent.parent / "vectordb" / "docs"
            other_docs = [f for f in docs_dir.iterdir() if f.is_file() and classify_file(f.name) == 'other'] if docs_dir.exists() else []
            doc_count = len(other_docs)
            
            # ì‹¤ì œ ë§ˆì§€ë§‰ ì—…ë°ì´íŠ¸ ë‚ ì§œ ê³„ì‚°
            import datetime
            index_files = list(OTHER_INDEX_DIR.iterdir())
            if index_files:
                latest_time = max(f.stat().st_mtime for f in index_files)
                last_update = datetime.datetime.fromtimestamp(latest_time).strftime("%Y-%m-%d")
            else:
                last_update = "ì•Œ ìˆ˜ ì—†ìŒ"
            
            st.markdown(f"""
                <div style="background-color: #f8f9fa; padding: 1.5rem; border-radius: 10px; border: 1px solid #e9ecef; text-align: center;">
                    <div style="font-weight: bold; font-size: 1.1rem; color: #333; margin-bottom: 0.5rem;">âœ… Other ë¬¸ì„œ í•™ìŠµ ì™„ë£Œ</div>
                    <div style="color: #666; font-size: 0.9rem; margin-bottom: 0.3rem;">í•™ìŠµ ì™„ë£Œ ë¬¸ì„œ {doc_count}ê±´</div>
                    <div style="color: #999; font-size: 0.8rem;">ë§ˆì§€ë§‰ ì—…ë°ì´íŠ¸: {last_update}</div>
                </div>
            """, unsafe_allow_html=True)
        else:
            st.markdown(f"""
                <div style="background-color: #f8f9fa; padding: 1.5rem; border-radius: 10px; border: 1px solid #e9ecef; text-align: center;">
                    <div style="font-weight: bold; font-size: 1.1rem; color: #333; margin-bottom: 0.5rem;">âš ï¸ Other ë¬¸ì„œ í•™ìŠµ ë¯¸ì™„ë£Œ</div>
                    <div style="color: #666; font-size: 0.9rem; margin-bottom: 0.3rem;">í•™ìŠµ ì™„ë£Œ ë¬¸ì„œ 0ê±´</div>
                    <div style="color: #999; font-size: 0.8rem;"></div>
                </div>
            """, unsafe_allow_html=True)
    
    
    st.write("")  # ê³µë°± ì¶”ê°€
    
    # ì €ì¥ëœ ë¬¸ì„œ íŒŒì¼ ëª©ë¡ í‘œì‹œ
    docs_dir = Path(__file__).parent.parent.parent / "vectordb" / "docs"
    if docs_dir.exists() and any(docs_dir.iterdir()):
        # ìˆ¨ê¹€ íŒŒì¼ ì œì™¸í•˜ê³  íŒŒì¼ ëª©ë¡ ê°€ì ¸ì˜¤ê¸°
        files = [f for f in docs_dir.iterdir() if f.is_file() and not f.name.startswith('.')]
        if files:
            # st.info(f"ğŸ“‚ ì´ {len(files)}ê°œ íŒŒì¼ì´ ì €ì¥ë˜ì–´ ìˆìŠµë‹ˆë‹¤:")
            
            # íŒŒì¼ ë°ì´í„°ë¥¼ í‘œ í˜•íƒœë¡œ ì¤€ë¹„
            file_data = []
            for file in sorted(files):
                file_size = file.stat().st_size
                file_size_kb = file_size / 1024
                category = classify_file(file.name)
                
                file_data.append({
                    "íŒŒì¼ëª…": file.name,
                    "ì‚¬ì´ì¦ˆ": f"{file_size_kb:.1f} KB" if file_size_kb < 1024 else f"{file_size_kb/1024:.2f} MB",
                    "ë¶„ë¥˜": category
                })
            
            df = pd.DataFrame(file_data)
            st.table(df)
        else:
            st.info("ğŸ“‚ ì €ì¥ëœ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤.")
    else:
        st.info("ğŸ“‚ ë¬¸ì„œ ì €ì¥ ë””ë ‰í† ë¦¬ê°€ ì—†ìŠµë‹ˆë‹¤.")

    # ìˆ˜ë™ ë¦¬ë¡œë“œ ë²„íŠ¼
    if st.button("ğŸ”„ í•™ìŠµ ë¬¸ì„œ ì •ë³´ ë¦¬ë¡œë“œ", type="secondary"):
        reload_backend_indexes()

def show_chat_page():
    """ì±„íŒ… í˜ì´ì§€ë¥¼ í‘œì‹œí•©ë‹ˆë‹¤."""
    # ë‚´ë¶€ ë¡œê³  ì˜ì—­
    st.markdown("""
        <div style="background-color:#ffffff; padding: 1rem 2rem; border-radius: 6px; margin-bottom: 1rem; display: flex; align-items: center; border: 1px solid #e5e7eb; box-shadow: 0 2px 4px rgba(0,0,0,0.04);">
            <img src="https://img.icons8.com/ios-filled/50/2d9bf0/document--v1.png" width="24px" style="margin-right: 10px;" />
            <span style="font-size: 1.3rem; font-weight: bold; color: #111827;">DocInsight AI</span>
        </div>
        <div style="color: #666666; font-size: 0.95rem; margin-bottom: 1.5rem;">
           ë‹¤ì–‘í•œ ë¬¸ì„œë¥¼ í•™ìŠµí•œ AIê°€ ì§ˆë¬¸ì— ë‹µë³€í•©ë‹ˆë‹¤.
        </div>
    """, unsafe_allow_html=True)

    display_chat_history()

    user_input = st.chat_input("ì„œìš¸ì‹œ ë„ì‹œê°€ìŠ¤ ìš”ê¸ˆ ì‚°ì • ë°©ì‹ì€?")
    if user_input:
        send_message(user_input)
        st.rerun()

def main():
    initialize_session_state()

    with st.sidebar:
        st.markdown("""
            <style>
            .sidebar-title {
                font-size: 1.3rem;
                font-weight: bold;
                margin-bottom: 1rem;
                color: white !important;
            }
            </style>
            <div class="sidebar-title">ë¬¸ì„œê´€ë¦¬</div>
        """, unsafe_allow_html=True)
        
        # API ì²˜ë¦¬ ì¤‘ì¸ì§€ í™•ì¸
        is_processing = st.session_state.get("api_processing", False)
        
        # ë¬¸ì„œì—…ë¡œë“œ ë²„íŠ¼
        upload_clicked = st.button(
            "ğŸ“‚ ë¬¸ì„œ ì—…ë¡œë“œ", 
            use_container_width=True,
            disabled=is_processing,
            key="upload_button"
        )
        
        if upload_clicked and not is_processing:
            st.session_state.current_page = "upload"
            st.rerun()
        
        # ë’¤ë¡œê°€ê¸° ë²„íŠ¼ì€ ë¬¸ì„œ ì—…ë¡œë“œ í™”ë©´ì—ì„œë§Œ í‘œì‹œ
        if st.session_state.current_page == "upload":
            # ë¬¸ì„œ ì—…ë¡œë“œ ë²„íŠ¼ ë°”ë¡œ ì•„ë˜ì— ë’¤ë¡œê°€ê¸° ë²„íŠ¼ ë°°ì¹˜
            back_clicked = st.button(
                "â† ë’¤ë¡œê°€ê¸°", 
                use_container_width=True,
                disabled=is_processing,
                key="back_button"
            )
            
            if back_clicked and not is_processing:
                st.session_state.current_page = "chat"
                st.rerun()

    # ìƒë‹¨ ì „ì²´ í—¤ë” ì˜ì—­ ì–´ë‘¡ê²Œ ì²˜ë¦¬
    st.markdown("""
        <style>
        header[data-testid="stHeader"] {
            background-color: #111827;
        }

        section[data-testid="stSidebar"] {
            width: 187px !important;
            background-color: #111827 !important;
            color: white !important;
            overflow-y: hidden !important;  /* ì‚¬ì´ë“œë°” ì„¸ë¡œ ìŠ¤í¬ë¡¤ ë¹„í™œì„±í™” */
            overflow-x: hidden !important;  /* ì‚¬ì´ë“œë°” ê°€ë¡œ ìŠ¤í¬ë¡¤ ë¹„í™œì„±í™” */
            height: 100vh !important;       /* ì‚¬ì´ë“œë°” ë†’ì´ë¥¼ í™”ë©´ ë†’ì´ë¡œ ê³ ì • */
            max-height: 100vh !important;   /* ìµœëŒ€ ë†’ì´ ì œí•œ */
        }

        /* ì‚¬ì´ë“œë°” ë‚´ë¶€ ì»¨í…Œì´ë„ˆë„ ìŠ¤í¬ë¡¤ ë°©ì§€ */
        section[data-testid="stSidebar"] > div {
            overflow-y: hidden !important;
            overflow-x: hidden !important;
            height: 100vh !important;
            max-height: 100vh !important;
        }

        /* ì‚¬ì´ë“œë°” ë‚´ë¶€ ëª¨ë“  ìš”ì†Œë“¤ì˜ ìŠ¤í¬ë¡¤ ë°©ì§€ */
        section[data-testid="stSidebar"] * {
            overflow-y: visible !important;
            overflow-x: visible !important;
        }

        /* ì‚¬ì´ë“œë°” ë²„íŠ¼ì´ disabled ìƒíƒœì—ì„œë„ ìƒ‰ìƒ ìœ ì§€ */
        section[data-testid="stSidebar"] .stButton > button {
            background-color: #333333 !important;
            color: white !important;
            border-radius: 8px !important;
            padding: 0.5rem 1rem !important;
        }
        section[data-testid="stSidebar"] .stButton > button:hover {
            background-color: #555555 !important;
        }
        section[data-testid="stSidebar"] .stButton > button:disabled {
            background-color: #333333 !important;
            color: white !important;
            opacity: 0.6 !important;
            cursor: not-allowed !important;
        }

        /* ì‚¬ì´ë“œë°” í† ê¸€ ë²„íŠ¼ ê°•ì¡° */
        button[data-testid="collapsedControl"] {
            border: 2px solid #9ca3af !important;
            border-radius: 50% !important;
            background-color: #f3f4f6 !important;
            width: 36px !important;
            height: 36px !important;
            display: flex;
            align-items: center;
            justify-content: center;
            position: fixed;
            top: 1.25rem;
            left: 0.75rem;
            z-index: 10000;
        }
        button[data-testid="collapsedControl"] svg {
            stroke: #374151 !important;
            width: 20px !important;
            height: 20px !important;
        }
        </style>
    """, unsafe_allow_html=True)

    # í˜„ì¬ í˜ì´ì§€ì— ë”°ë¼ ë‹¤ë¥¸ ë‚´ìš© í‘œì‹œ
    if st.session_state.current_page == "upload":
        show_upload_page()
    else:
        show_chat_page()

    st.markdown("""
        <style>
        html, body, [class*="css"]  {
            font-family: 'Pretendard', sans-serif !important;
        }
        /* ë©”ì¸ ì½˜í…ì¸  ì˜ì—­ì˜ ë²„íŠ¼ë§Œ ìŠ¤íƒ€ì¼ ì ìš© (ì‚¬ì´ë“œë°” ì œì™¸) */
        .main .stButton button {
            background-color: #333333 !important;
            color: white !important;
            border-radius: 8px !important;
            padding: 0.5rem 1rem !important;
        }
        .main .stButton button:hover {
            background-color: #555555 !important;
        }
        </style>
    """, unsafe_allow_html=True)

if __name__ == "__main__":
    main()
