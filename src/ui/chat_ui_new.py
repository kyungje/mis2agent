# chat_ui_docinsight.py (DocInsight AI ìŠ¤íƒ€ì¼ ì ìš© ì „ì²´ ë²„ì „)
import streamlit as st
import requests
import time
import re
import os
from dotenv import load_dotenv

# ë¬¸ì„œ ì—…ë¡œë“œ ê´€ë ¨ ì¶”ê°€ import
import json
import pandas as pd
from typing import List, Dict, Any
import tempfile
import shutil
from pathlib import Path
import unicodedata

# í†µí•© ë¡œê·¸ ê´€ë¦¬ í´ë˜ìŠ¤
class UnifiedLogger:
    """í•œ ì˜ì—­ì—ì„œ ë¡œê·¸ë¥¼ ê´€ë¦¬í•˜ëŠ” í´ë˜ìŠ¤"""
    def __init__(self):
        self.main_container = None
        self.detail_container = None
        self.is_active = False
    
    def start(self, title="ì§„í–‰ ìƒí™©"):
        """ë¡œê¹… ì‹œì‘"""
        if not self.is_active:
            self.main_container = st.empty()
            self.detail_container = st.empty()
            self.is_active = True
        self.update_main(title, "ì‹œì‘", "ğŸš€")
    
    def update_main(self, message, status="ì§„í–‰ì¤‘", icon="ğŸ“"):
        """ë©”ì¸ ìƒíƒœ ì—…ë°ì´íŠ¸"""
        if self.main_container:
            status_color = {
                "ì‹œì‘": "#3b82f6",
                "ì§„í–‰ì¤‘": "#f59e0b", 
                "ì™„ë£Œ": "#10b981",
                "ì‹¤íŒ¨": "#ef4444"
            }.get(status, "#6b7280")
            
            self.main_container.markdown(f"""
                <div style="background-color: rgba(59, 130, 246, 0.1); padding: 1rem; border-radius: 8px; border-left: 4px solid {status_color}; margin-bottom: 1rem;">
                    <div style="display: flex; align-items: center; font-weight: bold; color: {status_color};">
                        <span style="margin-right: 8px; font-size: 1.2rem;">{icon}</span>
                        <span>{message}</span>
                        <span style="margin-left: auto; font-size: 0.9rem;">({status})</span>
                    </div>
                </div>
            """, unsafe_allow_html=True)
    
    def update_detail(self, message, type="info"):
        """ì„¸ë¶€ ì§„í–‰ìƒí™© ì—…ë°ì´íŠ¸"""
        if self.detail_container:
            colors = {
                "info": "#3b82f6",
                "success": "#10b981", 
                "warning": "#f59e0b",
                "error": "#ef4444"
            }
            icons = {
                "info": "â„¹ï¸",
                "success": "âœ…",
                "warning": "âš ï¸", 
                "error": "âŒ"
            }
            
            color = colors.get(type, "#6b7280")
            icon = icons.get(type, "ğŸ“")
            
            self.detail_container.markdown(f"""
                <div style="background-color: rgba(107, 114, 128, 0.05); padding: 0.75rem; border-radius: 6px; border-left: 3px solid {color}; margin-bottom: 0.5rem;">
                    <div style="display: flex; align-items: center; color: {color}; font-size: 0.9rem;">
                        <span style="margin-right: 6px;">{icon}</span>
                        <span>{message}</span>
                    </div>
                </div>
            """, unsafe_allow_html=True)
    
    def success(self, message):
        """ì„±ê³µ ë©”ì‹œì§€"""
        self.update_main(message, "ì™„ë£Œ", "ğŸ‰")
        if self.detail_container:
            self.detail_container.empty()
    
    def error(self, message):
        """ì—ëŸ¬ ë©”ì‹œì§€"""
        self.update_main(message, "ì‹¤íŒ¨", "âŒ")
        if self.detail_container:
            self.detail_container.empty()
    
    def finish(self, success_message=None, error_message=None):
        """ë¡œê¹… ì¢…ë£Œ"""
        if success_message:
            self.success(success_message)
        elif error_message:
            self.error(error_message)
        
        if self.detail_container:
            self.detail_container.empty()
        self.is_active = False
    
    def clear(self):
        """ëª¨ë“  ë©”ì‹œì§€ ì •ë¦¬"""
        if self.main_container:
            self.main_container.empty()
        if self.detail_container:
            self.detail_container.empty()
        self.is_active = False

# ë¬¸ì„œ ì²˜ë¦¬ ë° ë²¡í„° ì¸ë±ìŠ¤ ìƒì„±ì„ ìœ„í•œ ëª¨ë“ˆë“¤
import pdfplumber
from docx import Document
from langchain_community.vectorstores import FAISS
from langchain_openai import OpenAIEmbeddings
from langchain_core.documents import Document as LCDocument
from langchain_experimental.text_splitter import SemanticChunker
import tiktoken  # ì‹¤ì œ í† í° ì¹´ìš´íŠ¸ë¥¼ ìœ„í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬
import gc

# === UI ì „ìš© ì„¤ì • (í†µí•© ê´€ë¦¬) ===
# OpenAI ëª¨ë¸ ì„¤ì •
OPENAI_EMBEDDING_MODEL = "text-embedding-3-small"  # ì„ë² ë”© ëª¨ë¸ëª…
OPENAI_MAX_RETRIES = 3  # API ì¬ì‹œë„ íšŸìˆ˜

# ì„ë² ë”© ë°°ì¹˜ í¬ê¸° (API í˜¸ì¶œ ìµœì í™”) - ì•ˆì •ì„±ê³¼ íš¨ìœ¨ì„±ì˜ ê· í˜•
EMBEDDING_BATCH_SIZE_INITIAL = 500     # 1ì°¨ ì‹œë„ (ì ì • ì²˜ë¦¬)
EMBEDDING_BATCH_SIZE_RETRY = 200       # 2ì°¨ ì‹œë„ (ì•ˆì „)
EMBEDDING_BATCH_SIZE_FINAL = 100       # 3ì°¨ ì‹œë„ (ë§¤ìš° ì•ˆì „)

# í˜ì´ì§€ ì„¤ì •
st.set_page_config(page_title="DocInsight AI", page_icon="ğŸ“„", layout="wide")
load_dotenv()
API_URL = "http://localhost:8000/chat"
RELOAD_API_URL = "http://localhost:8000/reload-indexes"
DOCUMENT_API_URL = "http://localhost:8000/get-document"

# === ë¬¸ì„œ ì—…ë¡œë“œë¥¼ ìœ„í•œ í•¨ìˆ˜ë“¤ (chat_ui.pyì—ì„œ ë³µì‚¬) ===

# ê²½ë¡œ ì„¤ì •
BASE_DIR = Path(__file__).parent.parent / "vectordb"
DOCS_DIR = BASE_DIR / "docs"
DB_DIR = BASE_DIR / "db"

# ì¸ë±ìŠ¤ ë¶„ë¥˜ë³„ ê²½ë¡œ ì„¤ì •
GAS_INDEX_DIR = DB_DIR / "gas_index"
POWER_INDEX_DIR = DB_DIR / "power_index"
OTHER_INDEX_DIR = DB_DIR / "other_index"

# í•„ìš”í•œ í´ë” ìƒì„±
DB_DIR.mkdir(exist_ok=True)
GAS_INDEX_DIR.mkdir(exist_ok=True)
POWER_INDEX_DIR.mkdir(exist_ok=True)
OTHER_INDEX_DIR.mkdir(exist_ok=True)

# í† í° ë¶„í•  ì²˜ë¦¬ë¥¼ ìœ„í•œ í•¨ìˆ˜ë“¤
def get_token_encoding():
    """OpenAI ì„ë² ë”© ëª¨ë¸ì— ë§ëŠ” í† í° ì¸ì½”ë”©ì„ ë°˜í™˜í•©ë‹ˆë‹¤."""
    try:
        # text-embedding-3-small ëª¨ë¸ì— ë§ëŠ” ì¸ì½”ë”© (cl100k_base)
        return tiktoken.get_encoding("cl100k_base")
    except Exception as e:
        # ë¡œê·¸ ì¶œë ¥ ì œê±° - ì´ë¯¸ ìƒìœ„ í•¨ìˆ˜ì—ì„œ ì²˜ë¦¬ë¨
        return None

def count_actual_tokens(text: str) -> int:
    """ì‹¤ì œ í† í° ìˆ˜ë¥¼ ì •í™•íˆ ê³„ì‚°í•©ë‹ˆë‹¤."""
    encoding = get_token_encoding()
    if encoding:
        try:
            return len(encoding.encode(text))
        except Exception as e:
            # ë¡œê·¸ ì¶œë ¥ ì œê±° - ì´ë¯¸ ìƒìœ„ í•¨ìˆ˜ì—ì„œ ì²˜ë¦¬ë¨
            # í´ë°±: ì¶”ì • ë°©ì‹
            return int(len(text) * 0.4)
    else:
        # í´ë°±: ì¶”ì • ë°©ì‹
        return int(len(text) * 0.4)

def calculate_total_tokens(documents: list) -> int:
    """ë¬¸ì„œ ë¦¬ìŠ¤íŠ¸ì˜ ì‹¤ì œ ì´ í† í° ìˆ˜ë¥¼ ê³„ì‚°í•©ë‹ˆë‹¤."""
    total_text = "".join([doc.page_content for doc in documents])
    return count_actual_tokens(total_text)

def create_vectorstore_with_token_limit(documents, embedding_model, max_tokens=280000, logger=None):  # 28ë§Œ í† í°ìœ¼ë¡œ ì¦ê°€ (OpenAI ì œí•œì˜ 93%)
    """ì‹¤ì œ í† í° ìˆ˜ ê¸°ì¤€ìœ¼ë¡œ ë²¡í„°ìŠ¤í† ì–´ë¥¼ ìƒì„±í•©ë‹ˆë‹¤."""
    
    # ì‹¤ì œ í† í° ê³„ì‚° ì „ì— ë¡œê·¸
    total_tokens = calculate_total_tokens(documents)
    
    # 250,000 í† í° ì œí•œì— ë§ì¶° ë¶„í•  ì²˜ë¦¬ ì—¬ë¶€ ê²°ì •
    if total_tokens > max_tokens:
        if logger:
            logger.update_detail(f"í† í° ì œí•œ({max_tokens:,}) ì´ˆê³¼. ë°°ì¹˜ ì²˜ë¦¬ ì‹œì‘ (ì´ {total_tokens:,} í† í°)", "info")
        
        # ì•ˆì „í•œ ë°°ì¹˜ í¬ê¸° (ì‹¤ì œ í† í° ê¸°ì¤€) - í† í° í™œìš©ë¥  ê·¹ëŒ€í™”
        safe_max_tokens = 240000  # 24ë§Œ í† í°ìœ¼ë¡œ ì¦ê°€ (OpenAI ì œí•œì˜ 80%)
        
        vectorstore = None
        current_batch = []
        current_tokens = 0
        batch_num = 1
                
        for i, doc in enumerate(documents):
            doc_tokens = count_actual_tokens(doc.page_content)
            
            # ë‹¨ì¼ ë¬¸ì„œê°€ ë°°ì¹˜ í¬ê¸°ë¥¼ í¬ê²Œ ì´ˆê³¼í•˜ëŠ” ê²½ìš°ì—ë§Œ ê²½ê³  ë° ê±´ë„ˆë›°ê¸° (50% ì—¬ìœ ë¥¼ ë‘ì–´ ì¤‘ìš”í•œ ë¬¸ì„œ ì†ì‹¤ ë°©ì§€)
            if doc_tokens > safe_max_tokens * 1.5:  # í˜„ì¬ëŠ” 30ë§Œ í† í°(200,000 * 1.5)
                if logger:
                    logger.update_detail(f"ë¬¸ì„œ {i+1}ì´ ë„ˆë¬´ í½ë‹ˆë‹¤ ({doc_tokens:,} í† í°). ê±´ë„ˆëœ€", "warning")
                continue
            
            # ì§„í–‰ìƒí™© í‘œì‹œ (25ê°œë§ˆë‹¤) - ë¹ˆë„ ì¤„ì„
            if (i + 1) % 25 == 0:
                if logger:
                    logger.update_detail(f"ë²¡í„° ì„ë² ë”© ìƒì„±: {i+1}/{len(documents)} (ë°°ì¹˜ {batch_num})", "info")
            
            # í˜„ì¬ ë°°ì¹˜ì— ì¶”ê°€í•´ë„ ì•ˆì „í•œì§€ í™•ì¸
            if current_tokens + doc_tokens <= safe_max_tokens:
                current_batch.append(doc)
                current_tokens += doc_tokens
            else:
                # í˜„ì¬ ë°°ì¹˜ ì²˜ë¦¬
                if current_batch:
                    try:
                        if logger:
                            logger.update_detail(f"ë²¡í„°ìŠ¤í† ì–´ ë°°ì¹˜ {batch_num} ìƒì„± ì¤‘... ({len(current_batch)}ê°œ ë¬¸ì„œ)", "info")
                        
                        if vectorstore is None:
                            vectorstore = FAISS.from_documents(current_batch, embedding_model)
                        else:
                            batch_vectorstore = FAISS.from_documents(current_batch, embedding_model)
                            vectorstore.merge_from(batch_vectorstore)
                        
                        if logger:
                            logger.update_detail(f"ë²¡í„°ìŠ¤í† ì–´ ë°°ì¹˜ {batch_num} ì™„ë£Œ", "success")
                        batch_num += 1
                        
                    except Exception as e:
                        if logger:
                            logger.update_detail(f"ë°°ì¹˜ {batch_num} ì²˜ë¦¬ ì‹¤íŒ¨: {e}", "error")
                        
                        # ë” ì‘ì€ ë°°ì¹˜ë¡œ ì¬ì‹œë„
                        if len(current_batch) > 1:
                            if logger:
                                logger.update_detail(f"ë°°ì¹˜ë¥¼ ì ˆë°˜ìœ¼ë¡œ ë‚˜ëˆ„ì–´ ì¬ì‹œë„", "info")
                            
                            # ë°°ì¹˜ë¥¼ ì ˆë°˜ìœ¼ë¡œ ë‚˜ëˆ„ì–´ ì¬ì‹œë„
                            mid = len(current_batch) // 2
                            for sub_batch in [current_batch[:mid], current_batch[mid:]]:
                                if sub_batch:
                                    try:
                                        if vectorstore is None:
                                            vectorstore = FAISS.from_documents(sub_batch, embedding_model)
                                        else:
                                            batch_vectorstore = FAISS.from_documents(sub_batch, embedding_model)
                                            vectorstore.merge_from(batch_vectorstore)
                                    except Exception as e2:
                                        if logger:
                                            logger.update_detail(f"ì†Œë°°ì¹˜ ì‹¤íŒ¨: {e2}", "error")
                                        continue
                        else:
                            # ë‹¨ì¼ ë¬¸ì„œë„ ì‹¤íŒ¨í•˜ëŠ” ê²½ìš° ê±´ë„ˆë›°ê¸°
                            if logger:
                                logger.update_detail(f"ë‹¨ì¼ ë¬¸ì„œ ì²˜ë¦¬ ì‹¤íŒ¨, ê±´ë„ˆëœ€: {e}", "warning")
                
                # ìƒˆ ë°°ì¹˜ ì‹œì‘
                current_batch = [doc]
                current_tokens = doc_tokens
        
        # ë§ˆì§€ë§‰ ë°°ì¹˜ ì²˜ë¦¬
        if current_batch:
            try:
                if logger:
                    logger.update_detail(f"ë§ˆì§€ë§‰ ë²¡í„°ìŠ¤í† ì–´ ë°°ì¹˜ {batch_num} ìƒì„± ì¤‘... ({len(current_batch)}ê°œ ë¬¸ì„œ)", "info")
                
                if vectorstore is None:
                    vectorstore = FAISS.from_documents(current_batch, embedding_model)
                else:
                    batch_vectorstore = FAISS.from_documents(current_batch, embedding_model)
                    vectorstore.merge_from(batch_vectorstore)
                
                if logger:
                    logger.update_detail(f"ë§ˆì§€ë§‰ ë²¡í„°ìŠ¤í† ì–´ ë°°ì¹˜ {batch_num} ì™„ë£Œ", "success")
                
            except Exception as e:
                if logger:
                    logger.update_detail(f"ë§ˆì§€ë§‰ ë°°ì¹˜ ì²˜ë¦¬ ì‹¤íŒ¨: {e}", "error")
                
                # ë§ˆì§€ë§‰ ë°°ì¹˜ë„ ë¶„í•  ì‹œë„
                if len(current_batch) > 1:
                    mid = len(current_batch) // 2
                    for sub_batch in [current_batch[:mid], current_batch[mid:]]:
                        if sub_batch:
                            try:
                                if vectorstore is None:
                                    vectorstore = FAISS.from_documents(sub_batch, embedding_model)
                                else:
                                    batch_vectorstore = FAISS.from_documents(sub_batch, embedding_model)
                                    vectorstore.merge_from(batch_vectorstore)
                                if logger:
                                    logger.update_detail(f"ì†Œë°°ì¹˜ ì™„ë£Œ: {len(sub_batch)}ê°œ ë¬¸ì„œ", "success")
                            except:
                                continue
        
        if vectorstore is None:
            raise Exception("ëª¨ë“  ë¬¸ì„œ ì²˜ë¦¬ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.")
        
        return vectorstore
    
    else:
        # í† í° ì œí•œ ë‚´ì— ìˆìœ¼ë©´ ì§ì ‘ ì²˜ë¦¬
        try:
            if logger:
                logger.update_detail(f"í† í° ì œí•œ ë‚´ ì§ì ‘ ì²˜ë¦¬ ({total_tokens:,} í† í°)", "info")
            return FAISS.from_documents(documents, embedding_model)
        except Exception as e:
            if logger:
                logger.update_detail(f"ì§ì ‘ ì²˜ë¦¬ ì‹¤íŒ¨: {e}", "error")
            if "max_tokens_per_request" in str(e):
                # ê°•ì œ ë¶„í•  ì²˜ë¦¬ë¡œ ì¬ê·€ í˜¸ì¶œ - ì ë‹¹í•œ í† í° ì œí•œ ì ìš©
                return create_vectorstore_with_token_limit(documents, embedding_model, max_tokens=150000, logger=logger)
            else:
                raise e

# OpenAI ì„ë² ë”© ëª¨ë¸ ì´ˆê¸°í™”
def create_embedding_model():
    """OpenAI ì„ë² ë”© ëª¨ë¸ì„ ìƒì„±í•©ë‹ˆë‹¤.
    
    í† í° ì œí•œ ì˜¤ë¥˜ ì‹œ ë°°ì¹˜ í¬ê¸°ë¥¼ ë‹¨ê³„ì ìœ¼ë¡œ ì¤„ì—¬ê°€ë©° ì¬ì‹œë„í•©ë‹ˆë‹¤.
    - 1ì°¨: 1000ê°œ ë°°ì¹˜ (ë¹ ë¥¸ ì²˜ë¦¬)
    - 2ì°¨: 500ê°œ ë°°ì¹˜ (ì•ˆì „)
    """
    
    try:
        embedding_model = OpenAIEmbeddings(
            model=OPENAI_EMBEDDING_MODEL,
            chunk_size=EMBEDDING_BATCH_SIZE_INITIAL,
            max_retries=OPENAI_MAX_RETRIES
        )
        return embedding_model
    except Exception as e:
        if "max_tokens_per_request" in str(e):
            # ë¡œê·¸ ì¶œë ¥ ì œê±° - ì´ë¯¸ ìƒìœ„ í•¨ìˆ˜ì—ì„œ ì²˜ë¦¬ë¨
            return OpenAIEmbeddings(
                model=OPENAI_EMBEDDING_MODEL,
                chunk_size=EMBEDDING_BATCH_SIZE_RETRY,
                max_retries=OPENAI_MAX_RETRIES
            )
        else:
            raise e

# í…ìŠ¤íŠ¸ ë¶„í• ê¸° ì´ˆê¸°í™” - í•˜ì´ë¸Œë¦¬ë“œ ë°©ì‹ (ì‚¬ì „ ë¶„í•  + SemanticChunker)
def create_text_splitter():
    """í•˜ì´ë¸Œë¦¬ë“œ í…ìŠ¤íŠ¸ ë¶„í• ê¸°ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.
    
    1ë‹¨ê³„: RecursiveCharacterTextSplitterë¡œ í° ì²­í¬ë“¤ë¡œ ì‚¬ì „ ë¶„í• 
    2ë‹¨ê³„: ê° ì²­í¬ì— SemanticChunker ì ìš©í•˜ì—¬ ì˜ë¯¸ì  ë¶„í• 
    """
    from langchain.text_splitter import RecursiveCharacterTextSplitter
    from langchain_experimental.text_splitter import SemanticChunker
    
    # 1ë‹¨ê³„: ì‚¬ì „ ë¶„í• ê¸° (í° ì²­í¬ë¡œ ë‚˜ëˆ„ê¸°) - ë²•ë ¹ ë¬¸ì„œ ìµœì í™”
    pre_splitter = RecursiveCharacterTextSplitter(
        chunk_size=35000,  # 35,000ìë¡œ ì¦ê°€ (ë” í° ë¬¸ë§¥ì—ì„œ ì˜ë¯¸ ë¶„ì„)
        chunk_overlap=100, # 100ìë¡œ ìµœì†Œí™” (ì¡°ë¬¸ ì—°ê²°ë¶€ë§Œ ë³´ì¡´)
        length_function=len,
        separators=[
            "\n\nì œ", "\n\nì¡°", "\n\ní•­", "\n\ní˜¸",  # ë²•ë ¹ êµ¬ì¡° ìš°ì„  (ì œ/ì¡°/í•­/í˜¸)
            "\n\n\n\n", "\n\n\n", "\n\n", "\n",      # ì¼ë°˜ êµ¬ì¡°
            ".", ")", " ", ""                          # ìµœí›„ ìˆ˜ë‹¨
        ]
    )
    
    return pre_splitter  # ìš°ì„  ì‚¬ì „ ë¶„í• ê¸°ë§Œ ë°˜í™˜

def apply_semantic_chunking(text_chunk: str, embedding_model) -> list:
    """ê°œë³„ í…ìŠ¤íŠ¸ ì²­í¬ì— SemanticChunkerë¥¼ ì ìš©í•©ë‹ˆë‹¤."""
    try:
        # í…ìŠ¤íŠ¸ê°€ ë„ˆë¬´ ì§§ìœ¼ë©´ SemanticChunker ê±´ë„ˆë›°ê¸° (ë” í° ì‚¬ì „ ì²­í¬ì— ë§ê²Œ ì¡°ì •)
        if len(text_chunk) < 2000:
            return [text_chunk]
                
        # SemanticChunker ìƒì„± ë° ì ìš©
        semantic_splitter = SemanticChunker(
            embeddings=embedding_model,
            breakpoint_threshold_type="percentile",  # ë‹¤ë¥¸ ì˜µì…˜: "standard_deviation", "interquartile"
            breakpoint_threshold_amount=90,  # ë”ìš± í° ì˜ë¯¸ ë‹¨ìœ„ë¡œ ë¶„í•  (90%)
            buffer_size=2,  # ë” í° ì²­í¬ì—ì„œëŠ” ë²„í¼ í¬ê¸° ì¦ê°€ë¡œ ì •êµí•œ ë¶„ì„
            sentence_split_regex=r'(?<=[.!?])\s+',  # ë¬¸ì¥ ë¶„í•  ì •ê·œì‹ ëª…ì‹œ
            add_start_index=True  # ì›ë³¸ í…ìŠ¤íŠ¸ì—ì„œì˜ ì‹œì‘ ìœ„ì¹˜ ì¶”ê°€
        )
        
        # ì˜ë¯¸ì  ë¶„í•  ìˆ˜í–‰
        semantic_chunks = semantic_splitter.split_text(text_chunk)
        
        # ë¹ˆ ì²­í¬ ì œê±° ë° ìµœì†Œ ê¸¸ì´ í•„í„°ë§ (ë”ìš± ê´€ëŒ€í•œ ê¸°ì¤€ìœ¼ë¡œ ì¡°ì •)
        filtered_chunks = [chunk for chunk in semantic_chunks if len(chunk.strip()) >= 100]
        
        return filtered_chunks if filtered_chunks else [text_chunk]
        
    except Exception as e:
        # SemanticChunker ì‹¤íŒ¨ ì‹œ ê³ ì • í¬ê¸° ë¶„í• ë¡œ í´ë°±
        # ë¡œê·¸ ì¶œë ¥ ì œê±° - ì •ìƒì ì¸ ì²˜ë¦¬ ê³¼ì •ì˜ ì¼ë¶€
        fallback_splitter = RecursiveCharacterTextSplitter(
            chunk_size=8000,   # ì ì ˆí•œ ì¤‘ê°„ í¬ê¸° (25,000ì˜ 1/3 ì •ë„)
            chunk_overlap=100, # 1ë‹¨ê³„ì™€ ë™ì¼í•œ ê²¹ì¹¨
            length_function=len,
            separators=[
                "\n\nì œ", "\n\nì¡°", "\n\ní•­", "\n\ní˜¸",  # ë²•ë ¹ êµ¬ì¡° ìœ ì§€
                "\n\n\n", "\n\n", "\n", ".", " ", ""
            ]
        )
        return fallback_splitter.split_text(text_chunk)

def remove_duplicate_chunks(chunks: list) -> list:
    """ì¤‘ë³µë˜ê±°ë‚˜ ë§¤ìš° ìœ ì‚¬í•œ ì²­í¬ë¥¼ ì œê±°í•©ë‹ˆë‹¤. (ë²•ë ¹ ë¬¸ì„œ ìµœì í™”)"""
    if not chunks:
        return chunks
        
    # ë²•ë ¹ ë¬¸ì„œì—ì„œëŠ” ë” ì—„ê²©í•œ ì¤‘ë³µ ê¸°ì¤€ ì ìš© (65% ì´ìƒ ìœ ì‚¬í•˜ë©´ ì¤‘ë³µ)
    similarity_threshold = 0.65
    unique_chunks = []
    
    for i, chunk in enumerate(chunks):
        is_duplicate = False
        chunk_text = chunk.strip()
        
        # ë„ˆë¬´ ì§§ì€ ì²­í¬ëŠ” ì œì™¸
        if len(chunk_text) < 100:
            continue
            
        chunk_words = set(chunk_text.split())
        
        # ì´ë¯¸ ì¶”ê°€ëœ ì²­í¬ë“¤ê³¼ ë¹„êµ
        for j, existing_chunk in enumerate(unique_chunks):
            existing_text = existing_chunk.strip()
            existing_words = set(existing_text.split())
            
            # ë‘ ì²­í¬ ëª¨ë‘ ì¶©ë¶„í•œ ê¸¸ì´ì¸ ê²½ìš°ì—ë§Œ ë¹„êµ
            if len(chunk_words) > 10 and len(existing_words) > 10:
                # Jaccard ìœ ì‚¬ë„ ê³„ì‚° (ë” ì •í™•)
                intersection = len(chunk_words.intersection(existing_words))
                union = len(chunk_words.union(existing_words))
                jaccard_similarity = intersection / union if union > 0 else 0
                
                if jaccard_similarity >= similarity_threshold:
                    is_duplicate = True
                    break
        
        if not is_duplicate:
            unique_chunks.append(chunk)
    
    removed_count = len(chunks) - len(unique_chunks)
    
    return unique_chunks

# ë¬¸ì„œ ì½ê¸° í•¨ìˆ˜ë“¤
def read_docx(path):
    """DOCX ë¬¸ì„œë¥¼ ì½ì–´ì„œ í…ìŠ¤íŠ¸ë¥¼ ì¶”ì¶œí•©ë‹ˆë‹¤."""
    doc = Document(path)
    text = "\n".join([para.text for para in doc.paragraphs if para.text.strip()])
    text = latex_to_text(text)
    return text

def read_pdf(path):
    """PDF ë¬¸ì„œë¥¼ ì½ì–´ì„œ ì „ì²´ í…ìŠ¤íŠ¸ë¥¼ ì¶”ì¶œí•©ë‹ˆë‹¤."""
    full_text = ""
    with pdfplumber.open(path) as pdf:
        for page_num, page in enumerate(pdf.pages, start=1):
            try:
                page_text = page.extract_text()
                if page_text:
                    page_text = latex_to_text(page_text)
                    page_text = normalize_text(page_text)
                    full_text += page_text + "\n"
            except Exception as e:
                # ë¡œê·¸ ì¶œë ¥ ì œê±° - ì •ìƒì ì¸ ì—ëŸ¬ ì²˜ë¦¬
                continue
    return full_text

def read_txt(path):
    """TXT íŒŒì¼ì„ ì½ì–´ì„œ í…ìŠ¤íŠ¸ë¥¼ ì¶”ì¶œí•©ë‹ˆë‹¤."""
    with open(path, 'r', encoding='utf-8') as f:
        text = f.read()
    text = latex_to_text(text)
    return text

# í…ìŠ¤íŠ¸ ì •ê·œí™”
def normalize_text(text):
    """í…ìŠ¤íŠ¸ ì •ê·œí™” (í•œê¸€-ì˜ë¬¸ ê³µë°±, ìˆ˜ì‹ ê¸°í˜¸ ë³´ì¡´)"""
    # í•œê¸€-ì˜ë¬¸ ê³µë°± ì •ë¦¬
    text = re.sub(r'([ê°€-í£])([A-Za-z0-9])', r'\1 \2', text)
    text = re.sub(r'([A-Za-z0-9])([ê°€-í£])', r'\1 \2', text)

    # ìˆ˜ì‹ ê¸°í˜¸ ë³´ì¡´: âˆš Â± â‰ˆ âˆ Ã— Ã· Ï€ Â² Â³ ^ / = % ë“±
    math_symbols = "âˆšÂ±â‰ˆâˆÃ—Ã·Ï€Â²Â³^=/%"

    # ë” ê´€ëŒ€í•œ ì •ê·œí™”: í•œê¸€, ì˜ë¬¸, ìˆ«ì, ê³µë°±, ë¬¸ì¥ë¶€í˜¸, ìˆ˜ì‹ ê¸°í˜¸ë§Œ ìœ ì§€
    # í•œê¸€: ê°€-í£
    # ì˜ë¬¸: A-Za-z
    # ìˆ«ì: 0-9
    # ê³µë°±: \s
    # ë¬¸ì¥ë¶€í˜¸: .,!?;:-()[]{}
    # ìˆ˜ì‹ ê¸°í˜¸: âˆšÂ±â‰ˆâˆÃ—Ã·Ï€Â²Â³^=/%
    # ì¶”ê°€ í—ˆìš© ë¬¸ì: + (í”ŒëŸ¬ìŠ¤ ê¸°í˜¸)
    
    allowed_pattern = r'[ê°€-í£A-Za-z0-9\s.,!?;:\-\(\)\[\]\{\}' + re.escape(math_symbols + '+') + r']'
    text = re.sub(rf'[^{allowed_pattern}]', ' ', text)

    # ì—°ì† ê³µë°± ì •ë¦¬
    text = re.sub(r'\s+', ' ', text)

    return text.strip()

# íŒŒì¼ëª… ì •ê·œí™” í•¨ìˆ˜
def normalize_filename(file_name):
    """íŒŒì¼ëª…ì„ ì •ê·œí™”í•˜ì—¬ ë¶„ë¥˜ì— ì‚¬ìš©í•©ë‹ˆë‹¤."""
    # íŒŒì¼ í™•ì¥ì ì œê±°
    name_without_ext = os.path.splitext(file_name)[0]
    
    # íŠ¹ìˆ˜ë¬¸ì ì œê±° (í•˜ì´í”ˆ, ì–¸ë”ìŠ¤ì½”ì–´, í”ŒëŸ¬ìŠ¤ ë“±ì€ ê³µë°±ìœ¼ë¡œ ë³€í™˜)
    # ëŒ€ê´„í˜¸ëŠ” ì œê±°í•˜ë˜, í‚¤ì›Œë“œê°€ í¬í•¨ëœ ë¶€ë¶„ì€ ë³´ì¡´
    normalized = re.sub(r'[\[\]\(\)\+\-\_]', ' ', name_without_ext)
    
    # í•œê¸€ì´ ë¶„í•´ë˜ì§€ ì•Šë„ë¡ NFCë¡œ ì •ê·œí™”
    normalized = unicodedata.normalize('NFC', normalized)
    
    # í‚¤ì›Œë“œê°€ í¬í•¨ëœ ë¶€ë¶„ì´ ê³µë°±ìœ¼ë¡œ ë¶„ë¦¬ë˜ì—ˆëŠ”ì§€ í™•ì¸í•˜ê³  ë³µêµ¬
    if ' ì „ë ¥ ' in normalized or normalized.startswith('ì „ë ¥ ') or normalized.endswith(' ì „ë ¥'):
        # ì „ë ¥ í‚¤ì›Œë“œ ì£¼ë³€ì˜ ê³µë°±ì„ ì œê±°í•˜ì—¬ ë³µêµ¬
        normalized = re.sub(r'\s+ì „ë ¥\s+', 'ì „ë ¥', normalized)
        normalized = re.sub(r'^ì „ë ¥\s+', 'ì „ë ¥', normalized)
        normalized = re.sub(r'\s+ì „ë ¥$', 'ì „ë ¥', normalized)
    
    if ' ë„ì‹œê°€ìŠ¤ ' in normalized or normalized.startswith('ë„ì‹œê°€ìŠ¤ ') or normalized.endswith(' ë„ì‹œê°€ìŠ¤'):
        # ë„ì‹œê°€ìŠ¤ í‚¤ì›Œë“œ ì£¼ë³€ì˜ ê³µë°±ì„ ì œê±°í•˜ì—¬ ë³µêµ¬
        normalized = re.sub(r'\s+ë„ì‹œê°€ìŠ¤\s+', 'ë„ì‹œê°€ìŠ¤', normalized)
        normalized = re.sub(r'^ë„ì‹œê°€ìŠ¤\s+', 'ë„ì‹œê°€ìŠ¤', normalized)
        normalized = re.sub(r'\s+ë„ì‹œê°€ìŠ¤$', 'ë„ì‹œê°€ìŠ¤', normalized)
    
    # ì—°ì† ê³µë°±ì„ ë‹¨ì¼ ê³µë°±ìœ¼ë¡œ ë³€í™˜
    normalized = re.sub(r'\s+', ' ', normalized)
    
    # ì•ë’¤ ê³µë°± ì œê±°
    normalized = normalized.strip()
    
    return normalized

# íŒŒì¼ ë¶„ë¥˜ í•¨ìˆ˜
def classify_file(file_name):
    """íŒŒì¼ëª…ì— ë”°ë¼ ë¶„ë¥˜ë¥¼ ê²°ì •í•©ë‹ˆë‹¤."""
    # íŒŒì¼ëª… ì •ê·œí™”
    normalized_name = normalize_filename(file_name)
    
    # í‚¤ì›Œë“œ ê²€ìƒ‰ ê²°ê³¼ (ì—¬ëŸ¬ ë°©ë²•ìœ¼ë¡œ ê²€ìƒ‰)
    gas_found = 'ë„ì‹œê°€ìŠ¤' in normalized_name
    power_found = 'ì „ë ¥' in normalized_name
    
    # ëŒ€ì•ˆ ê²€ìƒ‰ ë°©ë²• (ì¸ì½”ë”© ë¬¸ì œ í•´ê²°ì„ ìœ„í•´)
    gas_found_alt = normalized_name.find('ë„ì‹œê°€ìŠ¤') != -1
    power_found_alt = normalized_name.find('ì „ë ¥') != -1
    
    # ìµœì¢… ê²°ê³¼ ê²°ì • (ë‘˜ ì¤‘ í•˜ë‚˜ë¼ë„ Trueë©´ True)
    gas_found = gas_found or gas_found_alt
    power_found = power_found or power_found_alt
    
    # ë¶„ë¥˜ ê²°ì •
    if gas_found:
        return 'gas'
    elif power_found:
        return 'power'
    else:
        # ì›ë³¸ íŒŒì¼ëª…ìœ¼ë¡œ ì¬ì‹œë„
        original_gas_found = 'ë„ì‹œê°€ìŠ¤' in file_name
        original_power_found = 'ì „ë ¥' in file_name
        
        if original_gas_found:
            return 'gas'
        elif original_power_found:
            return 'power'
        else:
            # ë§ˆì§€ë§‰ ì•ˆì „ì¥ì¹˜: íŒŒì¼ëª…ì˜ ëª¨ë“  ë¶€ë¶„ì„ ê°œë³„ì ìœ¼ë¡œ í™•ì¸
            file_parts = re.split(r'[_\-\s\[\]\(\)]', file_name)
            
            for part in file_parts:
                if 'ë„ì‹œê°€ìŠ¤' in part:
                    return 'gas'
                elif 'ì „ë ¥' in part:
                    return 'power'
            
            return 'other'

def get_index_dir(category):
    """ë¶„ë¥˜ì— ë”°ë¥¸ ì¸ë±ìŠ¤ ë””ë ‰í† ë¦¬ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤."""
    if category == 'gas':
        return GAS_INDEX_DIR
    elif category == 'power':
        return POWER_INDEX_DIR
    else:
        return OTHER_INDEX_DIR

def extract_metadata_from_filename(filename):
    base_name = os.path.splitext(filename)[0]
    parts = re.split(r'[_\-\s\+\(\)]', base_name)  # +ì™€ ()ë„ êµ¬ë¶„ìë¡œ ì¶”ê°€

    # ë²„ì „ ì¶”ì¶œ ê°œì„ : 2024, 24.01.01, 2024.7.1 ë“± ë‹¤ì–‘í•œ í˜•ì‹ ì§€ì›
    version = None
    for part in parts:
        # 4ìë¦¬ ì—°ë„ í¬í•¨ ë‚ ì§œ (2024.7.1) - ìš°ì„ ìˆœìœ„ 1
        if re.match(r'20\d{2}\.\d{1,2}\.\d{1,2}', part):
            version = part  # ì „ì²´ ë‚ ì§œ ë³´ì¡´
            break
        # 2ìë¦¬ ì—°ë„ í¬í•¨ ë‚ ì§œ (24.01.01) - ìš°ì„ ìˆœìœ„ 2
        elif re.match(r'\d{2}\.\d{1,2}\.\d{1,2}', part):
            year = part.split('.')[0]
            if int(year) >= 20:  # 20ë…„ ì´í›„
                version = f"20{part}"  # 2024.01.01 í˜•íƒœë¡œ ë³€í™˜
            break
        # 4ìë¦¬ ì—°ë„ë§Œ (2024) - ìš°ì„ ìˆœìœ„ 3
        elif re.match(r'20\d{2}$', part):
            version = part
            break
    
    region_list = ['ì„œìš¸', 'ë¶€ì‚°', 'ëŒ€êµ¬', 'ê´‘ì£¼', 'ì¸ì²œ', 'ëŒ€ì „', 'ìš¸ì‚°','ê²½ê¸°ë„', 'ê°•ì›ë„', 'ì¶©ì²­ë¶ë„', 'ì¶©ì²­ë‚¨ë„' ,'ì „ë¼ë‚¨ë„','ì „ë¶íŠ¹ë³„ìì¹˜ë„', 'ê²½ìƒë‚¨ë„', 'ê²½ìƒë¶ë„']
    region = next((p for p in parts if p in region_list), None)
    organization_map = {'ë„ì‹œê°€ìŠ¤': 'ë„ì‹œê°€ìŠ¤', 'ì „ë ¥': 'ì „ë ¥'}
    organization = next((p for p in organization_map if p in parts), "ê¸°íƒ€")

    return {
        "version": version,
        "region": region,
        "organization": organization,
        "title": base_name
    }

# ë¬¸ì„œ ì²˜ë¦¬ ë° ì²­í‚¹
def process_document(file_path, logger=None):
    """ë¬¸ì„œë¥¼ ì½ê³  SemanticChunkerë¥¼ ì‚¬ìš©í•˜ì—¬ ì˜ë¯¸ì  ì²­í‚¹ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤.
    
    SemanticChunkerëŠ” ë¬¸ì„œì˜ ì˜ë¯¸ë¥¼ ê³ ë ¤í•˜ì—¬ ìë™ìœ¼ë¡œ ì ì ˆí•œ í¬ê¸°ì˜ ì²­í¬ë¡œ ë¶„í• í•©ë‹ˆë‹¤.
    """
    filename = os.path.basename(file_path)
    
    if logger:
        logger.update_detail(f"ğŸ“– í…ìŠ¤íŠ¸ ì¶”ì¶œ ì¤‘: {filename}")
    
    # ë¬¸ì„œ ì½ê¸°
    if file_path.endswith(".pdf"):
        text = read_pdf(file_path)
    elif file_path.endswith(".docx"):
        text = read_docx(file_path)
    elif file_path.endswith(".txt"):
        text = read_txt(file_path)
    else:
        # ë¡œê·¸ ì¶œë ¥ ì œê±° - ì •ìƒì ì¸ ì—ëŸ¬ ì²˜ë¦¬
        return []
    
    
    # í…ìŠ¤íŠ¸ê°€ ë¹„ì–´ìˆìœ¼ë©´ ì²˜ë¦¬ ì¤‘ë‹¨
    if not text:
        if logger:
            logger.update_detail(f"âŒ {filename}: í…ìŠ¤íŠ¸ ì¶”ì¶œ ì‹¤íŒ¨", "error")
        return []
    
    if logger:
        logger.update_detail(f"âœ… í…ìŠ¤íŠ¸ ì¶”ì¶œ ì™„ë£Œ: {len(text):,}ì")
    
    # í…ìŠ¤íŠ¸ ì •ê·œí™”
    original_text = text
    text = normalize_text(text)
    
    # ì •ê·œí™” ê³¼ì •ì—ì„œ í…ìŠ¤íŠ¸ê°€ ë„ˆë¬´ ë§ì´ ì œê±°ë˜ì—ˆëŠ”ì§€ í™•ì¸
    if len(text) < len(original_text) * 0.1:  # 90% ì´ìƒ ì œê±°ëœ ê²½ìš°
        # ì •ê·œí™”ë¥¼ ê±´ë„ˆë›°ê³  ì›ë³¸ í…ìŠ¤íŠ¸ ì‚¬ìš©
        text = original_text
        
    # ë¬¸ì„œ í¬ê¸° ê¸°ì¤€ ì„¤ì •
    LARGE_DOCUMENT_THRESHOLD = 60000  # 6ë§Œì ì´ìƒì€ ëŒ€ìš©ëŸ‰ ë¬¸ì„œë¡œ íŒë‹¨
    
    if len(text) <= LARGE_DOCUMENT_THRESHOLD:
        # ì‘ì€ ë¬¸ì„œ: SemanticChunker ì§ì ‘ ì‚¬ìš©
        if logger:
            logger.update_detail(f"ğŸ§  ì˜ë¯¸ì  ì²­í‚¹ ì¤‘...")
        
        embedding_model = create_embedding_model()
        
        try:
            chunks = apply_semantic_chunking(text, embedding_model)
        except Exception as e:
            # ì‹¤íŒ¨ ì‹œ ê°„ë‹¨í•œ ê³ ì • í¬ê¸° ë¶„í• 
            simple_splitter = RecursiveCharacterTextSplitter(
                chunk_size=3000,
                chunk_overlap=100,
                length_function=len,
                separators=[
                    "\n\nì œ", "\n\nì¡°", "\n\ní•­", "\n\ní˜¸",  # ë²•ë ¹ êµ¬ì¡° ìœ ì§€
                    "\n\n\n", "\n\n", "\n", ".", " ", ""
                ]
            )
            chunks = simple_splitter.split_text(text)
            
    else:
        # ëŒ€ìš©ëŸ‰ ë¬¸ì„œ: í•˜ì´ë¸Œë¦¬ë“œ ë°©ì‹ (ì‚¬ì „ ë¶„í•  + SemanticChunker)
        if logger:
            logger.update_detail(f"ğŸ§  ëŒ€ìš©ëŸ‰ ë¬¸ì„œ ì²­í‚¹ ì¤‘...")
        
        text_splitter = create_text_splitter()
        pre_chunks = text_splitter.split_text(text)
        
        # 2ë‹¨ê³„: ê° ì‚¬ì „ ì²­í¬ì— SemanticChunker ì ìš©
        all_chunks = []
        embedding_model = create_embedding_model()  # SemanticChunkerìš© ì„ë² ë”© ëª¨ë¸
        
        for i, pre_chunk in enumerate(pre_chunks):
            if logger and (i + 1) % 5 == 0:  # 5ê°œë§ˆë‹¤ ì§„í–‰ìƒí™© í‘œì‹œ
                logger.update_detail(f"ğŸ”„ ì˜ë¯¸ì  ì²­í‚¹ ì§„í–‰: {i+1}/{len(pre_chunks)}")
            
            try:
                semantic_chunks = apply_semantic_chunking(pre_chunk, embedding_model)
                all_chunks.extend(semantic_chunks)
            except Exception as e:
                # ì‹¤íŒ¨ ì‹œ ì›ë³¸ ì‚¬ì „ ì²­í¬ë¥¼ ë” ì‘ê²Œ ë‚˜ëˆ„ì–´ ì¶”ê°€
                fallback_splitter = RecursiveCharacterTextSplitter(
                    chunk_size=8000,   # apply_semantic_chunkingê³¼ ë™ì¼í•œ ì„¤ì •
                    chunk_overlap=100, # 1ë‹¨ê³„ì™€ ë™ì¼í•œ ê²¹ì¹¨
                    length_function=len,
                    separators=[
                        "\n\nì œ", "\n\nì¡°", "\n\ní•­", "\n\ní˜¸",  # ë²•ë ¹ êµ¬ì¡° ìœ ì§€
                        "\n\n\n", "\n\n", "\n", ".", " ", ""
                    ]
                )
                fallback_chunks = fallback_splitter.split_text(pre_chunk)
                all_chunks.extend(fallback_chunks)
        
        chunks = all_chunks
    
    # 3ë‹¨ê³„: ì¤‘ë³µ ì²­í¬ ì œê±° (ë²•ë ¹ ë¬¸ì„œ ì •í™•ì„± í™•ë³´)
    chunks = remove_duplicate_chunks(chunks)
    
    if logger:
        logger.update_detail(f"âœ… ì²­í‚¹ ì™„ë£Œ: {len(chunks)}ê°œ ë¬¸ì„œ ìƒì„±")
    
    # íŒŒì¼ëª…ì—ì„œ ì¶”ê°€ ë©”íƒ€ë°ì´í„° ì¶”ì¶œ
    additional_metadata = extract_metadata_from_filename(filename)
    
    # ë©”íƒ€ë°ì´í„°ê°€ í¬í•¨ëœ LangChain ë¬¸ì„œë¡œ ë³€í™˜
    documents = []
    for i, chunk in enumerate(chunks):
        if len(chunk.strip()) > 100:  # ìµœì†Œ ê¸¸ì´ í•„í„° (100ì ì´ìƒ)
            # ê¸°ë³¸ ë©”íƒ€ë°ì´í„°ì™€ ì¶”ê°€ ë©”íƒ€ë°ì´í„°ë¥¼ ê²°í•©
            metadata = {
                "source": filename,
                "chunk_id": i,
                "file_path": file_path,
                "version": additional_metadata["version"],
                "region": additional_metadata["region"],
                "organization": additional_metadata["organization"],
                "title": additional_metadata["title"]
            }
            doc = LCDocument(
                page_content=chunk,
                metadata=metadata
            )
            documents.append(doc)
    
    return documents

# ë²¡í„° ì¸ë±ìŠ¤ êµ¬ì¶•
def build_vector_index_from_uploaded_files(uploaded_files):
    """ì—…ë¡œë“œëœ íŒŒì¼ë“¤ë¡œë¶€í„° ë²¡í„° ì¸ë±ìŠ¤ë¥¼ êµ¬ì¶•í•©ë‹ˆë‹¤."""
   
    if not uploaded_files:
        st.warning("ì—…ë¡œë“œëœ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤.")
        return False
    
    # í†µí•© ë¡œê±° ìƒì„±
    logger = UnifiedLogger()
    logger.start("AI ë¬¸ì„œ í•™ìŠµ ì‹œì‘")
    
    try:
        # ë¬¸ì„œ ì €ì¥ ë””ë ‰í† ë¦¬ ìƒì„±
        docs_dir = Path(__file__).parent.parent / "vectordb" / "docs"
        docs_dir.mkdir(parents=True, exist_ok=True)
        
        # ì„ë² ë”© ëª¨ë¸ ì´ˆê¸°í™”
        logger.update_main("ì„ë² ë”© ëª¨ë¸ ì´ˆê¸°í™”", "ì§„í–‰ì¤‘", "ğŸ”§")
        embedding_model = create_embedding_model()
        
        # ë¶„ë¥˜ë³„ë¡œ ë¬¸ì„œ ê·¸ë£¹í™”
        gas_documents = []
        power_documents = []
        other_documents = []
        
        total_files = len(uploaded_files)
        logger.update_main(f"íŒŒì¼ ì²˜ë¦¬ ({total_files}ê°œ)", "ì§„í–‰ì¤‘", "ğŸ“")
        
        # íŒŒì¼ë³„ë¡œ ì²˜ë¦¬ ë° ë¶„ë¥˜
        for i, uploaded_file in enumerate(uploaded_files, 1):
            logger.update_detail(f"[{i}/{total_files}] ì²˜ë¦¬ ì¤‘: {uploaded_file.name}")
            
            # ì‹¤ì œ íŒŒì¼ì„ docs ë””ë ‰í† ë¦¬ì— ì €ì¥
            file_path = docs_dir / uploaded_file.name
            
            # ì´ë¯¸ ë™ì¼í•œ íŒŒì¼ëª…ì´ ì¡´ì¬í•˜ëŠ”ì§€ í™•ì¸
            if file_path.exists():
                logger.error(f"'{uploaded_file.name}' íŒŒì¼ì´ ì´ë¯¸ ì¡´ì¬í•©ë‹ˆë‹¤.")
                return False
            
            with open(file_path, "wb") as f:
                f.write(uploaded_file.getvalue())
            
            try:
                # íŒŒì¼ ë¶„ë¥˜
                category = classify_file(uploaded_file.name)
                
                documents = process_document(str(file_path), logger)
                
                # ë¶„ë¥˜ë³„ë¡œ ë¬¸ì„œ ì¶”ê°€
                if category == 'gas':
                    gas_documents.extend(documents)
                elif category == 'power':
                    power_documents.extend(documents)
                else:
                    other_documents.extend(documents)
                
                logger.update_detail(f"âœ… {uploaded_file.name} ì²˜ë¦¬ ì™„ë£Œ ({category} ë¶„ë¥˜, {len(documents)}ê°œ ë¬¸ì„œ)", "success")
                
                # ë©”ëª¨ë¦¬ ìµœì í™”
                del documents
                gc.collect()
                
            except Exception as e:
                logger.error(f"íŒŒì¼ ì²˜ë¦¬ ì˜¤ë¥˜: {e}")
                # ì˜¤ë¥˜ ë°œìƒ ì‹œ ì €ì¥ëœ íŒŒì¼ ì‚­ì œ
                if file_path.exists():
                    file_path.unlink()
                return False
        
        # ë¶„ë¥˜ë³„ë¡œ ì¸ë±ìŠ¤ ìƒì„±
        logger.update_main("ë²¡í„° ì¸ë±ìŠ¤ ìƒì„±", "ì§„í–‰ì¤‘", "ğŸ”§")
        
        categories = [
            ('gas', gas_documents, 'Gas'),
            ('power', power_documents, 'Power'),
            ('other', other_documents, 'Other')
        ]
        
        success_count = 0
        for category, documents, category_name in categories:
            if len(documents) == 0:
                logger.update_detail(f"{category_name} ë¶„ë¥˜: ë¬¸ì„œ ì—†ìŒ, ê±´ë„ˆëœ€")
                continue
                
            logger.update_detail(f"{category_name} ì¸ë±ìŠ¤ ìƒì„± ì¤‘... (ë¬¸ì„œ ìˆ˜: {len(documents)}ê°œ)")
            
            # ë°°ì¹˜ ë‹¨ìœ„ë¡œ ì„ë² ë”© ì²˜ë¦¬
            try:
                vectorstore = create_vectorstore_with_token_limit(documents, embedding_model, logger=logger)
            except Exception as e:
                if "max_tokens_per_request" in str(e):
                    try:
                        logger.update_detail(f"{category_name}: í† í° ì œí•œìœ¼ë¡œ ë°°ì¹˜ í¬ê¸° ì¡°ì • (ì¬ì‹œë„ 1/2)", "warning")
                        medium_embedding_model = OpenAIEmbeddings(
                            model=OPENAI_EMBEDDING_MODEL,
                            chunk_size=EMBEDDING_BATCH_SIZE_RETRY,
                            max_retries=OPENAI_MAX_RETRIES
                        )
                        vectorstore = create_vectorstore_with_token_limit(documents, medium_embedding_model, logger=logger)
                    except Exception as e2:
                        if "max_tokens_per_request" in str(e2):
                            logger.update_detail(f"{category_name}: í† í° ì œí•œìœ¼ë¡œ ë°°ì¹˜ í¬ê¸° ì¬ì¡°ì • (ì¬ì‹œë„ 2/2)", "warning")
                            small_embedding_model = OpenAIEmbeddings(
                                model=OPENAI_EMBEDDING_MODEL,
                                chunk_size=EMBEDDING_BATCH_SIZE_FINAL,
                                max_retries=OPENAI_MAX_RETRIES
                            )
                            vectorstore = create_vectorstore_with_token_limit(documents, small_embedding_model, logger=logger)
                        else:
                            raise e2
                else:
                    raise e

            # ì¸ë±ìŠ¤ ì €ì¥
            index_dir = get_index_dir(category)
            try:
                vectorstore.save_local(str(index_dir))
                logger.update_detail(f"{category_name} ì¸ë±ìŠ¤ ì €ì¥ ì™„ë£Œ", "success")
                success_count += 1
            except Exception as e:
                logger.error(f"{category_name} ì¸ë±ìŠ¤ ì €ì¥ ì˜¤ë¥˜: {e}")
                return False
        
        if success_count > 0:
            logger.success(f"ğŸ‰ {success_count}ê°œ ë¶„ë¥˜ë³„ ì¸ë±ìŠ¤ ìƒì„± ì™„ë£Œ")
            return True
        else:
            logger.error("âŒ ì¸ë±ìŠ¤ ìƒì„±ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.")
            return False
            
    except Exception as e:
        logger.error(f"ì˜ˆìƒì¹˜ ëª»í•œ ì˜¤ë¥˜: {e}")
        return False

def reload_backend_indexes():
    """ë°±ì—”ë“œì˜ ì¸ë±ìŠ¤ë¥¼ ë‹¤ì‹œ ë¡œë“œí•©ë‹ˆë‹¤."""
    # í†µí•© ë¡œê±° ìƒì„±
    logger = UnifiedLogger()
    logger.start("ë°±ì—”ë“œ ì¸ë±ìŠ¤ ë¦¬ë¡œë“œ")
    
    try:
        logger.update_detail("API ì„œë²„ì— ë¦¬ë¡œë“œ ìš”ì²­ ì¤‘...")
        response = requests.post(RELOAD_API_URL)
        response.raise_for_status()
        
        result = response.json()
        if result.get("success"):
            logger.success("âœ… ë°±ì—”ë“œ ì¸ë±ìŠ¤ ë¦¬ë¡œë“œ ì™„ë£Œ!")
            return True
        else:
            logger.error(f"ë°±ì—”ë“œ ì¸ë±ìŠ¤ ë¦¬ë¡œë“œ ì‹¤íŒ¨: {result.get('message', 'ì•Œ ìˆ˜ ì—†ëŠ” ì˜¤ë¥˜')}")
            return False
    except Exception as e:
        logger.error(f"ë°±ì—”ë“œ ì¸ë±ìŠ¤ ë¦¬ë¡œë“œ ì¤‘ ì˜¤ë¥˜: {str(e)}")
        st.warning("âš ï¸ FastAPI ì„œë²„ê°€ ì‹¤í–‰ ì¤‘ì¸ì§€ í™•ì¸í•´ì£¼ì„¸ìš”.")
        return False

def latex_to_text(text):
    """
    LaTeX ìˆ˜ì‹ì„ ì‚¬ëŒì´ ì½ëŠ” í…ìŠ¤íŠ¸ ìˆ˜ì‹ìœ¼ë¡œ ë³€í™˜
    ì˜ˆ: \frac{a}{b} â†’ (a) / (b)
    """
    # \frac ë³€í™˜ í•¨ìˆ˜
    def frac_repl(match):
        return f"({match.group(1)}) / ({match.group(2)})"

    # LaTeX ë¸”ë¡(\[...\], $$...$$, $...$)ì„ ì°¾ì•„ì„œ ë³€í™˜
    def latex_block_repl(match):
        latex_expr = match.group(1)
        # \leftì™€ \right ì œê±° (ê´„í˜¸ í¬ê¸° ì¡°ì • ëª…ë ¹ì–´) - ë” í¬ê´„ì ìœ¼ë¡œ ì²˜ë¦¬
        latex_expr = re.sub(r'\\left\s*\(', '(', latex_expr)
        latex_expr = re.sub(r'\\right\s*\)', ')', latex_expr)
        latex_expr = re.sub(r'\\left\s*\[', '[', latex_expr)
        latex_expr = re.sub(r'\\right\s*\]', ']', latex_expr)
        latex_expr = re.sub(r'\\left\s*\\{', '{', latex_expr)
        latex_expr = re.sub(r'\\right\s*\\}', '}', latex_expr)
        # \frac ë³€í™˜
        latex_expr = re.sub(r'\\frac\{(.+?)\}\{(.+?)\}', frac_repl, latex_expr)
        # \times ë³€í™˜
        latex_expr = latex_expr.replace(r'\times', 'Ã—')
        # ì¤‘ê´„í˜¸ ì œê±°
        latex_expr = latex_expr.replace('{', '').replace('}', '')
        return latex_expr

    # \[ ... \] ë¸”ë¡ ë³€í™˜
    text = re.sub(r'\\\[(.*?)\\\]', lambda m: latex_block_repl(m), text, flags=re.DOTALL)
    # $$ ... $$ ë¸”ë¡ ë³€í™˜
    text = re.sub(r'\$\$(.*?)\$\$', lambda m: latex_block_repl(m), text, flags=re.DOTALL)
    # $ ... $ ë¸”ë¡ ë³€í™˜
    text = re.sub(r'\$(.*?)\$', lambda m: latex_block_repl(m), text, flags=re.DOTALL)

    # ì¸ë¼ì¸ ë³€í™˜ (í˜¹ì‹œ ë‚¨ì•„ìˆì„ ê²½ìš°)
    # \leftì™€ \right ì œê±° - ë” í¬ê´„ì ìœ¼ë¡œ ì²˜ë¦¬
    text = re.sub(r'\\left\s*\(', '(', text)
    text = re.sub(r'\\right\s*\)', ')', text)
    text = re.sub(r'\\left\s*\[', '[', text)
    text = re.sub(r'\\right\s*\]', ']', text)
    text = re.sub(r'\\left\s*\\{', '{', text)
    text = re.sub(r'\\right\s*\\}', '}', text)
    # \frac ë³€í™˜
    text = re.sub(r'\\frac\{(.+?)\}\{(.+?)\}', frac_repl, text)
    text = text.replace(r'\times', 'Ã—')
    text = text.replace('{', '').replace('}', '')

    return text

def initialize_session_state():
    if "messages" not in st.session_state:
        st.session_state.messages = []
    if "streaming" not in st.session_state:
        st.session_state.streaming = False
    if "current_page" not in st.session_state:
        st.session_state.current_page = "chat"
    if "api_processing" not in st.session_state:
        st.session_state.api_processing = False




def highlight_relevant_content(content: str, search_terms: list) -> str:
    """ë¬¸ì„œ ë‚´ìš©ì—ì„œ ê´€ë ¨ ë¶€ë¶„ì„ í•˜ì´ë¼ì´íŠ¸í•©ë‹ˆë‹¤."""
    if not search_terms:
        return content
    
    highlighted_content = content
    
    # ê° ê²€ìƒ‰ì–´ì— ëŒ€í•´ í•˜ì´ë¼ì´íŠ¸ ì ìš©
    for term in search_terms:
        if term and len(term.strip()) > 1:
            # HTML í•˜ì´ë¼ì´íŠ¸ ì ìš©
            highlighted_content = re.sub(
                f'({re.escape(term)})',
                r'<mark style="background-color: #ffeb3b; padding: 1px 2px;">\1</mark>',
                highlighted_content,
                flags=re.IGNORECASE
            )
    
    return highlighted_content

def extract_keywords_from_recent_chat() -> list:
    """ìµœê·¼ ì±„íŒ…ì—ì„œ í‚¤ì›Œë“œë¥¼ ì¶”ì¶œí•©ë‹ˆë‹¤."""
    if not st.session_state.messages:
        return []
    
    # ìµœê·¼ ì‚¬ìš©ì ì§ˆë¬¸ì—ì„œ í‚¤ì›Œë“œ ì¶”ì¶œ
    recent_user_messages = [
        msg["content"] for msg in st.session_state.messages[-5:] 
        if msg["role"] == "user"
    ]
    
    keywords = []
    for message in recent_user_messages:
        # ê°„ë‹¨í•œ í‚¤ì›Œë“œ ì¶”ì¶œ (í•œê¸€ 2ê¸€ì ì´ìƒ)
        words = re.findall(r'[ê°€-í£]{2,}', message)
        keywords.extend(words)
    
    # ì¤‘ë³µ ì œê±° ë° ê¸¸ì´ ì •ë ¬ (ê¸´ í‚¤ì›Œë“œ ìš°ì„ )
    unique_keywords = list(set(keywords))
    unique_keywords.sort(key=len, reverse=True)
    
    return unique_keywords[:10]  # ìƒìœ„ 10ê°œë§Œ ì‚¬ìš©



def extract_document_links(content: str) -> list:
    """ë©”ì‹œì§€ ë‚´ìš©ì—ì„œ ë¬¸ì„œ ë§í¬ë¥¼ ì¶”ì¶œí•©ë‹ˆë‹¤."""
    pattern = r'\[DOCUMENT:([^\]]+)\]'
    matches = re.findall(pattern, content)
    return matches

def find_actual_file_path(docs_dir: Path, filename: str) -> Path:
    """íŒŒì¼ëª…ì„ ì •í™•íˆ ì°¾ê¸° ìœ„í•´ ë‹¤ì–‘í•œ íŒ¨í„´ìœ¼ë¡œ ê²€ìƒ‰í•©ë‹ˆë‹¤."""
    print(f"DEBUG: ì°¾ëŠ” íŒŒì¼ëª…: {filename}")
    print(f"DEBUG: ê²€ìƒ‰ ë””ë ‰í† ë¦¬: {docs_dir}")
    
    # ë””ë ‰í† ë¦¬ì˜ ëª¨ë“  íŒŒì¼ ë‚˜ì—´
    try:
        all_files = list(docs_dir.iterdir())
        print(f"DEBUG: ë””ë ‰í† ë¦¬ì˜ ëª¨ë“  íŒŒì¼: {[f.name for f in all_files if f.is_file()]}")
    except Exception as e:
        print(f"DEBUG: ë””ë ‰í† ë¦¬ ì½ê¸° ì˜¤ë¥˜: {e}")
    
    # 1. ì •í™•í•œ íŒŒì¼ëª…ìœ¼ë¡œ ì°¾ê¸°
    file_path = docs_dir / filename
    print(f"DEBUG: ì •í™•í•œ íŒŒì¼ëª…ìœ¼ë¡œ ê²€ìƒ‰: {file_path}")
    if file_path.exists():
        print(f"DEBUG: íŒŒì¼ ë°œê²¬! {file_path}")
        return file_path
    
    # 1-1. íŒŒì¼ëª…ì— ëŒ€ê´„í˜¸ê°€ ìˆëŠ” ê²½ìš°, ê·¸ëŒ€ë¡œ ê²€ìƒ‰
    if '[' in filename and ']' in filename:
        # ì´ë¯¸ ëŒ€ê´„í˜¸ê°€ ìˆëŠ” íŒŒì¼ëª…ì´ë¯€ë¡œ ê·¸ëŒ€ë¡œ ì‚¬ìš©
        file_path = docs_dir / filename
        print(f"DEBUG: ëŒ€ê´„í˜¸ í¬í•¨ íŒŒì¼ëª…ìœ¼ë¡œ ê²€ìƒ‰: {file_path}")
        if file_path.exists():
            print(f"DEBUG: ëŒ€ê´„í˜¸ í¬í•¨ íŒŒì¼ëª…ìœ¼ë¡œ íŒŒì¼ ë°œê²¬! {file_path}")
            return file_path
    
    # 2. ëŒ€ê´„í˜¸ê°€ ë¹ ì§„ ê²½ìš°ë¥¼ ê³ ë ¤í•´ì„œ ì°¾ê¸°
    if not filename.startswith('['):
        print(f"DEBUG: ëŒ€ê´„í˜¸ê°€ ì—†ëŠ” íŒŒì¼ëª…, í‚¤ì›Œë“œ ê²€ìƒ‰ ì‹œì‘")
        keywords = ['ë„ì‹œê°€ìŠ¤', 'ì „ë ¥', 'ê¸°íƒ€']
        for keyword in keywords:
            if keyword in filename:
                bracketed_filename = f"[{keyword}]{filename}"
                file_path = docs_dir / bracketed_filename
                print(f"DEBUG: í‚¤ì›Œë“œ {keyword}ë¡œ ê²€ìƒ‰: {file_path}")
                if file_path.exists():
                    print(f"DEBUG: í‚¤ì›Œë“œ ê²€ìƒ‰ìœ¼ë¡œ íŒŒì¼ ë°œê²¬! {file_path}")
                    return file_path
    
    # 3. ëŒ€ê´„í˜¸ê°€ ìˆëŠ” ê²½ìš°, ëŒ€ê´„í˜¸ë¥¼ ì œê±°í•œ í˜•íƒœë¡œë„ ì°¾ê¸°
    if filename.startswith('[') and ']' in filename:
        clean_filename = filename.split(']', 1)[1] if ']' in filename else filename
        file_path = docs_dir / clean_filename
        if file_path.exists():
            return file_path
    
    # 4. ì§€ì—­ëª… ìš°ì„  ë§¤ì¹­ ê²€ìƒ‰
    try:
        print(f"DEBUG: ì§€ì—­ëª… ìš°ì„  ë§¤ì¹­ ê²€ìƒ‰ ì‹œì‘")
        
        # ìš”ì²­ íŒŒì¼ëª…ì—ì„œ ì§€ì—­ëª… ì¶”ì¶œ
        def extract_region_from_filename(fname):
            """íŒŒì¼ëª…ì—ì„œ ì§€ì—­ëª…ì„ ì¶”ì¶œí•©ë‹ˆë‹¤."""
            regions = ['ì„œìš¸íŠ¹ë³„ì‹œ', 'ê°•ì›ë„', 'ê²½ê¸°ë„', 'ê²½ìƒë¶ë„', 'ì „ë¼ë‚¨ë„', 'ì¶©ì²­ë¶ë„', 'ë¶€ì‚°', 'ì„œìš¸', 'ë¶€ì‚°ê´‘ì—­ì‹œ', 'ëŒ€êµ¬ê´‘ì—­ì‹œ', 'ì¸ì²œê´‘ì—­ì‹œ', 'ê´‘ì£¼ê´‘ì—­ì‹œ', 'ëŒ€ì „ê´‘ì—­ì‹œ', 'ìš¸ì‚°ê´‘ì—­ì‹œ', 'ì„¸ì¢…íŠ¹ë³„ìì¹˜ì‹œ', 'ê²½ê¸°ë„', 'ê°•ì›ë„', 'ì¶©ì²­ë¶ë„', 'ì¶©ì²­ë‚¨ë„', 'ì „ë¼ë¶ë„', 'ì „ë¼ë‚¨ë„', 'ê²½ìƒë¶ë„', 'ê²½ìƒë‚¨ë„', 'ì œì£¼íŠ¹ë³„ìì¹˜ë„']
            for region in regions:
                if region in fname:
                    return region
            return None
        
        requested_region = extract_region_from_filename(filename)
        print(f"DEBUG: ìš”ì²­ íŒŒì¼ëª…ì—ì„œ ì¶”ì¶œí•œ ì§€ì—­: {requested_region}")
        
        # ì§€ì—­ëª…ì´ ìˆëŠ” ê²½ìš°, ì •í™•í•œ ì§€ì—­ íŒŒì¼ë§Œ ì°¾ê¸°
        if requested_region:
            print(f"DEBUG: ì§€ì—­ëª… '{requested_region}'ìœ¼ë¡œ ì •í™•í•œ ì§€ì—­ íŒŒì¼ ê²€ìƒ‰")
            for file in docs_dir.iterdir():
                if file.is_file():
                    file_region = extract_region_from_filename(file.name)
                    if file_region and requested_region in file_region:
                        # ì¶”ê°€ë¡œ íŒŒì¼ ë‚´ìš©ì´ ìœ ì‚¬í•œì§€ í™•ì¸
                        clean_request = filename.replace('[', '').replace(']', '').replace('+', ' ').lower()
                        clean_file = file.name.replace('[', '').replace(']', '').replace('+', ' ').lower()
                        
                        # íŒŒì¼ëª…ì˜ ì£¼ìš” í‚¤ì›Œë“œë“¤ì´ ì¼ì¹˜í•˜ëŠ”ì§€ í™•ì¸
                        request_keywords = set(clean_request.split())
                        file_keywords = set(clean_file.split())
                        
                        # 60% ì´ìƒ í‚¤ì›Œë“œê°€ ì¼ì¹˜í•˜ë©´ í•´ë‹¹ íŒŒì¼ë¡œ ì„ íƒ
                        if len(request_keywords.intersection(file_keywords)) / len(request_keywords) >= 0.6:
                            print(f"DEBUG: ì§€ì—­ëª… ë§¤ì¹­ìœ¼ë¡œ íŒŒì¼ ë°œê²¬! {file}")
                            return file
        
        # ì§€ì—­ëª…ì´ ì—†ê±°ë‚˜ ì§€ì—­ëª… ë§¤ì¹­ì— ì‹¤íŒ¨í•œ ê²½ìš°, ë¶€ë¶„ ë§¤ì¹­ìœ¼ë¡œ ê²€ìƒ‰
        print(f"DEBUG: ë¶€ë¶„ ë§¤ì¹­ ê²€ìƒ‰ ì‹œì‘")
        for file in docs_dir.iterdir():
            if file.is_file():
                print(f"DEBUG: ê²€ì‚¬ ì¤‘ì¸ íŒŒì¼: {file.name}")
                
                # íŒŒì¼ëª… ì •ê·œí™” (ëŒ€ê´„í˜¸, ê³µë°±, íŠ¹ìˆ˜ë¬¸ì ì²˜ë¦¬)
                clean_request = filename.replace('[', '').replace(']', '').replace('+', ' ').lower().strip()
                clean_file = file.name.replace('[', '').replace(']', '').replace('+', ' ').lower().strip()
                
                print(f"DEBUG: ì •ê·œí™”ëœ ìš”ì²­: '{clean_request}'")
                print(f"DEBUG: ì •ê·œí™”ëœ íŒŒì¼: '{clean_file}'")
                
                # 1. ì •í™•í•œ ë¶€ë¶„ ë¬¸ìì—´ ë§¤ì¹­
                if clean_request in clean_file or clean_file in clean_request:
                    print(f"DEBUG: ë¶€ë¶„ ë¬¸ìì—´ ë§¤ì¹­ìœ¼ë¡œ íŒŒì¼ ë°œê²¬! {file}")
                    return file
                
                # 2. í‚¤ì›Œë“œ ê¸°ë°˜ ë§¤ì¹­
                request_keywords = set(clean_request.split())
                file_keywords = set(clean_file.split())
                
                # ê³µí†µ í‚¤ì›Œë“œê°€ ìˆëŠ”ì§€ í™•ì¸
                common_keywords = request_keywords.intersection(file_keywords)
                if common_keywords:
                    print(f"DEBUG: ê³µí†µ í‚¤ì›Œë“œ ë°œê²¬: {common_keywords}")
                    # 50% ì´ìƒ í‚¤ì›Œë“œê°€ ì¼ì¹˜í•˜ë©´ í•´ë‹¹ íŒŒì¼ë¡œ ì„ íƒ
                    if len(common_keywords) / len(request_keywords) >= 0.5:
                        print(f"DEBUG: í‚¤ì›Œë“œ ë§¤ì¹­ìœ¼ë¡œ íŒŒì¼ ë°œê²¬! {file}")
                        return file
                
                # 3. íŠ¹ì • í‚¤ì›Œë“œê°€ í¬í•¨ëœ ê²½ìš° (ì „ë ¥, ë„ì‹œê°€ìŠ¤ ë“±)
                important_keywords = ['ì „ë ¥', 'ë„ì‹œê°€ìŠ¤', 'ê¸°íƒ€', 'ë¹„ìš©í‰ê°€', 'ìš´ì˜ê·œì •']
                for keyword in important_keywords:
                    if keyword in clean_request and keyword in clean_file:
                        print(f"DEBUG: ì¤‘ìš” í‚¤ì›Œë“œ '{keyword}' ë§¤ì¹­ìœ¼ë¡œ íŒŒì¼ ë°œê²¬! {file}")
                        return file
                        
    except Exception as e:
        print(f"DEBUG: ê²€ìƒ‰ ì¤‘ ì˜¤ë¥˜: {e}")
        pass
    
    return None



def display_chat_history():
    for idx, message in enumerate(st.session_state.messages):
        with st.chat_message(message["role"]):
            if message["role"] == "assistant":
                # [DOCUMENT:[filename]] íŒ¨í„´ì„ ë‹¤ìš´ë¡œë“œ ê°€ëŠ¥í•œ íŒŒì¼ëª…ìœ¼ë¡œ ë³€í™˜
                content = message["content"]
                
                def create_download_link(match):
                    filename = match.group(1)
                    # íŒŒì¼ ê²½ë¡œ ìƒì„± (ì—…ë¡œë“œ ê²½ë¡œì™€ ì¼ì¹˜í•˜ë„ë¡ ìˆ˜ì •)
                    docs_dir = Path(__file__).parent.parent / "vectordb" / "docs"
                    file_path = find_actual_file_path(docs_dir, filename)
                    
                    if file_path and file_path.exists():
                        # íŒŒì¼ì„ ì½ì–´ì„œ ë‹¤ìš´ë¡œë“œ ê°€ëŠ¥í•œ ë§í¬ ìƒì„±
                        try:
                            with open(file_path, "rb") as f:
                                file_data = f.read()
                            
                            # base64ë¡œ ì¸ì½”ë”©í•˜ì—¬ ë‹¤ìš´ë¡œë“œ ë§í¬ ìƒì„±
                            import base64
                            b64_data = base64.b64encode(file_data).decode()
                            file_ext = file_path.suffix.lower()
                            
                            # MIME íƒ€ì… ì„¤ì •
                            mime_types = {
                                '.pdf': 'application/pdf',
                                '.docx': 'application/vnd.openxmlformats-officedocument.wordprocessingml.document',
                                '.txt': 'text/plain'
                            }
                            mime_type = mime_types.get(file_ext, 'application/octet-stream')
                            
                            # ë‹¤ìš´ë¡œë“œ ë§í¬ HTML ìƒì„±
                            href = f'<a href="data:{mime_type};base64,{b64_data}" download="{file_path.name}" style="color: #2d9bf0; text-decoration: underline; font-weight: bold;">ğŸ“‹ {filename}</a>'
                            return href
                        except Exception as e:
                            return f'ğŸ“¥ ë‹¤ìš´ë¡œë“œ (ì˜¤ë¥˜)'
                    else:
                        return f'ğŸ“¥ ë‹¤ìš´ë¡œë“œ (íŒŒì¼ ì—†ìŒ)'
                
                pattern = r'\[DOCUMENT:(.*)\]'
                content = re.sub(pattern, create_download_link, content)
                
                # ë©”ì‹œì§€ ë‚´ìš© í‘œì‹œ (HTML í—ˆìš©)
                st.markdown(content, unsafe_allow_html=True)
            else:
                st.markdown(message["content"])

def stream_response(response_text: str, loading_placeholder):
    message_placeholder = st.empty()
    full_response = ""
    loading_placeholder.empty()
    for char in response_text:
        full_response += char
        message_placeholder.markdown(full_response + "â–Œ")
        time.sleep(0.01)
    message_placeholder.markdown(full_response)
    return full_response

def send_message(user_input: str):
    if not user_input:
        return
    
    # API ì²˜ë¦¬ ìƒíƒœ ì‹œì‘ (ì¦‰ì‹œ ì„¤ì •)
    st.session_state.api_processing = True
    
    # ì• ë‹ˆë©”ì´ì…˜ ìƒíƒœ ì´ˆê¸°í™”
    if "animation_step" not in st.session_state:
        st.session_state.animation_step = 0
    
    # ì‚¬ìš©ì ë©”ì‹œì§€ í‘œì‹œ
    with st.chat_message("user"):
        st.markdown(user_input)
    
    # ë¡œë”© ë©”ì‹œì§€ë¥¼ ìœ„í•œ í”Œë ˆì´ìŠ¤í™€ë” 
    loading_placeholder = st.empty()
    
    # ë©”ì‹œì§€ë¥¼ ì„¸ì…˜ì— ì¶”ê°€
    st.session_state.messages.append({"role": "user", "content": user_input})

    # API ìš”ì²­ ë°ì´í„° ì¤€ë¹„
    request_data = {
        "messages": [
            {"role": msg["role"], "content": msg["content"]}
            for msg in st.session_state.messages
        ]
    }
    
    try:
        # ì—°ì†ëœ ì• ë‹ˆë©”ì´ì…˜ê³¼ API í˜¸ì¶œì„ ë™ì‹œì— ì²˜ë¦¬
        dot_patterns = ["", ".", "..", "...", "...."]
        
        # API í˜¸ì¶œì„ ë³„ë„ ìŠ¤ë ˆë“œì—ì„œ ì‹¤í–‰
        import concurrent.futures
        import threading
        
        result_container = {"response": None, "error": None, "completed": False}
        
        def api_call():
            try:
                response = requests.post(API_URL, json=request_data)
                response.raise_for_status()
                result_container["response"] = response.json()["response"]
            except Exception as e:
                result_container["error"] = str(e)
            finally:
                result_container["completed"] = True
        
        # API í˜¸ì¶œ ìŠ¤ë ˆë“œ ì‹œì‘
        api_thread = threading.Thread(target=api_call, daemon=True)
        api_thread.start()
        
        # ì• ë‹ˆë©”ì´ì…˜ ì‹¤í–‰ (API ì™„ë£Œê¹Œì§€)
        cycle = 0
        while not result_container["completed"]:
            dots = dot_patterns[cycle % len(dot_patterns)]
            loading_placeholder.markdown(
                f"<div style='font-size: 1rem; color: #6b7280;'>ğŸ” AIê°€ ì‘ë‹µì„ ìƒì„±í•˜ê³  ìˆìŠµë‹ˆë‹¤{dots}</div>",
                unsafe_allow_html=True
            )
            time.sleep(0.4)
            cycle += 1
        
        # API í˜¸ì¶œ ì™„ë£Œ ëŒ€ê¸°
        api_thread.join()
        
        if result_container["error"]:
            raise Exception(result_container["error"])
        
        # ì‘ë‹µ ì²˜ë¦¬
        assistant_response = result_container["response"]
        
        # ìˆ˜ì‹ ë³€í™˜ ì ìš©
        assistant_response = latex_to_text(assistant_response)
        
        # API ì²˜ë¦¬ ì™„ë£Œ ìƒíƒœë¡œ ë³€ê²½
        st.session_state.api_processing = False
        
        # ìŠ¤íŠ¸ë¦¬ë° ë°©ì‹ìœ¼ë¡œ ì‘ë‹µ í‘œì‹œ
        with st.chat_message("assistant"):
            streamed_response = stream_response(assistant_response, loading_placeholder)
            st.session_state.messages.append({"role": "assistant", "content": streamed_response})
        
    except Exception as e:
        # ì˜¤ë¥˜ ë°œìƒ ì‹œì—ë„ API ì²˜ë¦¬ ì™„ë£Œ ìƒíƒœë¡œ ë³€ê²½
        st.session_state.api_processing = False
        loading_placeholder.empty()
        st.error(f"Error: {str(e)}")
    
    # ìµœì¢…ì ìœ¼ë¡œ API ì²˜ë¦¬ ìƒíƒœ í™•ì‹¤íˆ ì¢…ë£Œ
    st.session_state.api_processing = False

def show_upload_page():
    """ë¬¸ì„œ ì—…ë¡œë“œ í˜ì´ì§€ë¥¼ í‘œì‹œí•©ë‹ˆë‹¤."""
    # ë‚´ë¶€ ë¡œê³  ì˜ì—­ (ì±„íŒ… í™”ë©´ê³¼ ë™ì¼í•œ ìŠ¤íƒ€ì¼)
    st.markdown("""
        <div style="background-color:#ffffff; padding: 1rem 2rem; border-radius: 6px; margin-bottom: 1rem; display: flex; align-items: center; border: 1px solid #e5e7eb; box-shadow: 0 2px 4px rgba(0,0,0,0.04);">
            <img src="https://img.icons8.com/ios-filled/50/2d9bf0/document--v1.png" width="24px" style="margin-right: 10px;" />
            <span style="font-size: 1.3rem; font-weight: bold; color: #111827;">ë¬¸ì„œ ì—…ë¡œë“œ </span>
        </div>
        <div style="color: #666666; font-size: 0.95rem; margin-bottom: 1.5rem;">
            ë¬¸ì„œë¥¼ ì—…ë¡œë“œ í•´ AIë¥¼ í•™ìŠµì‹œì¼œë³´ì„¸ìš”.
        </div>
    """, unsafe_allow_html=True)

    st.markdown("""
        <div style="background-color: rgba(30, 41, 59, 0.9); padding: 1rem 1.5rem; border-radius: 10px; border: 1px solid #1f2937; color: #f3f4f6; font-size: 0.95rem; line-height: 1.7;">
        <strong style="font-size: 1.1rem; color: #ffffff;">ğŸ“Œ íŒŒì¼ ë¶„ë¥˜ ê·œì¹™</strong>
        <ul style="list-style-type: 'ğŸ“‚ '; padding-left: 1.2em; margin: 0;">
            <li><b>ë„ì‹œê°€ìŠ¤</b> í‚¤ì›Œë“œê°€ íŒŒì¼ëª…ì— í¬í•¨ â†’ <span style="color: #38bdf8;"><b>Gas</b></span> ë¶„ë¥˜</li>
            <li><b>ì „ë ¥</b> í‚¤ì›Œë“œê°€ íŒŒì¼ëª…ì— í¬í•¨ â†’ <span style="color: #38bdf8;"><b>Power</b></span> ë¶„ë¥˜</li>
            <li>ê·¸ ì™¸ í‚¤ì›Œë“œì¼ ê²½ìš° â†’ <span style="color: #facc15;"><b>Other</b></span> ë¶„ë¥˜</li>
        </ul>
        </div>
    """, unsafe_allow_html=True)
    
    st.write("")  # ê³µë°± ì¶”ê°€
    
    uploaded_files = st.file_uploader(
        "ë¬¸ì„œ íŒŒì¼ì„ ì„ íƒí•˜ì„¸ìš”",
        type=['pdf', 'docx', 'txt'],
        accept_multiple_files=True,
        help="ì—¬ëŸ¬ íŒŒì¼ì„ ë™ì‹œì— ì—…ë¡œë“œí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.",
        label_visibility="collapsed"
    )
    # st.caption("PDF, DOCX, TXT íŒŒì¼ì„ ì—…ë¡œë“œ ê°€ëŠ¥ í•©ë‹ˆë‹¤. (ìµœëŒ€ 200MB)")

    st.write("")  # ê³µë°± ì¶”ê°€

    if uploaded_files:
        file_data = []

        for file in uploaded_files:
            file_name = file.name
            file_size_kb = len(file.getvalue()) / 1024
            category = classify_file(file_name)

            warning_msg = ""
            if category == "other":
                warning_msg = "âš ï¸ ë¶„ë¥˜ ë¶ˆí™•ì‹¤"

            file_data.append({
                "íŒŒì¼ëª…": file_name,
                "í¬ê¸°": f"{file_size_kb:.1f} KB" if file_size_kb < 1024 else f"{file_size_kb/1024:.2f} MB",
                "ë¶„ë¥˜": warning_msg if warning_msg else category
            })

        df = pd.DataFrame(file_data)
        st.markdown("""
            <div style="background-color:#ffffff; padding: 1rem 2rem; border-radius: 6px; margin-bottom: 1rem; display: flex; align-items: center; border: 1px solid #e5e7eb; box-shadow: 0 2px 4px rgba(0,0,0,0.04);">
                <img src="https://img.icons8.com/ios-filled/50/2d9bf0/database--v1.png" width="24px" style="margin-right: 10px;" />
                <span style="font-size: 1.3rem; font-weight: bold; color: #111827;">ğŸ“„ ì—…ë¡œë“œëœ ë¬¸ì„œ</span>
            </div>
        """, unsafe_allow_html=True)
        st.table(df)
 
    if st.button("â–¶ï¸ AI ë¬¸ì„œ í•™ìŠµ ì‹œì‘", type="primary", disabled=not uploaded_files):
        try:
            success = build_vector_index_from_uploaded_files(uploaded_files)
            if success:
                # ë°±ì—”ë“œ ì¸ë±ìŠ¤ ë¦¬ë¡œë“œ
                reload_success = reload_backend_indexes()
                if reload_success:
                    st.success("ğŸ‰ ëª¨ë“  ì‘ì—…ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤! ì´ì œ ì±„íŒ… íƒ­ì—ì„œ ì§ˆë¬¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
                else:
                    st.warning("âš ï¸ ì¸ë±ìŠ¤ëŠ” ìƒì„±ë˜ì—ˆì§€ë§Œ ë°±ì—”ë“œ ë¦¬ë¡œë“œì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤. FastAPI ì„œë²„ë¥¼ ì¬ì‹œì‘í•˜ê±°ë‚˜ [í•™ìŠµ ë¬¸ì„œ ì •ë³´ ë¦¬ë¡œë“œ] ë²„íŠ¼ì„ ëˆŒëŸ¬ ì£¼ì„¸ìš”.")
        except Exception as e:
            st.error(f"âŒ ì¸ë±ìŠ¤ ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}")
            st.warning("API ì„œë²„ê°€ ì‹¤í–‰ë˜ì§€ ì•Šì•˜ì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤. FastAPI ì„œë²„ë¥¼ ë¨¼ì € ì‹¤í–‰í•´ì£¼ì„¸ìš”.")
    
    st.write("")  # ê³µë°± ì¶”ê°€

    # ê¸°ì¡´ í•™ìŠµ ì •ë³´ í—¤ë”
    st.markdown("""
        <div style="background-color:#ffffff; padding: 1rem 2rem; border-radius: 6px; margin-bottom: 1rem; display: flex; align-items: center; border: 1px solid #e5e7eb; box-shadow: 0 2px 4px rgba(0,0,0,0.04);">
            <img src="https://img.icons8.com/ios-filled/50/2d9bf0/database--v1.png" width="24px" style="margin-right: 10px;" />
            <span style="font-size: 1.3rem; font-weight: bold; color: #111827;">í•™ìŠµ ì™„ë£Œëœ ë¬¸ì„œ ì •ë³´</span>
        </div>
    """, unsafe_allow_html=True)
    
    col1, col2, col3 = st.columns(3)
    
    # Gas ì¸ë±ìŠ¤ ì¹´ë“œ
    with col1:
        if GAS_INDEX_DIR.exists() and any(GAS_INDEX_DIR.iterdir()):
            # ì €ì¥ëœ ë¬¸ì„œ ìˆ˜ ê³„ì‚°
            docs_dir = Path(__file__).parent.parent / "vectordb" / "docs"
            gas_docs = [f for f in docs_dir.iterdir() if f.is_file() and classify_file(f.name) == 'gas'] if docs_dir.exists() else []
            doc_count = len(gas_docs)
            
            # ì‹¤ì œ ë§ˆì§€ë§‰ ì—…ë°ì´íŠ¸ ë‚ ì§œ ê³„ì‚°
            import datetime
            index_files = list(GAS_INDEX_DIR.iterdir())
            if index_files:
                latest_time = max(f.stat().st_mtime for f in index_files)
                last_update = datetime.datetime.fromtimestamp(latest_time).strftime("%Y-%m-%d")
            else:
                last_update = "ì•Œ ìˆ˜ ì—†ìŒ"
            
            st.markdown(f"""
                <div style="background-color: #f8f9fa; padding: 1.5rem; border-radius: 10px; border: 1px solid #e9ecef; text-align: center;">
                    <div style="font-weight: bold; font-size: 1.1rem; color: #333; margin-bottom: 0.5rem;">âœ… Gas ë¬¸ì„œ í•™ìŠµ ì™„ë£Œ</div>
                    <div style="color: #666; font-size: 0.9rem; margin-bottom: 0.3rem;">í•™ìŠµ ì™„ë£Œ ë¬¸ì„œ {doc_count}ê±´</div>
                    <div style="color: #999; font-size: 0.8rem;">ë§ˆì§€ë§‰ ì—…ë°ì´íŠ¸: {last_update}</div>
                </div>
            """, unsafe_allow_html=True)
        else:
            st.markdown(f"""
                <div style="background-color: #f8f9fa; padding: 1.5rem; border-radius: 10px; border: 1px solid #e9ecef; text-align: center;">
                    <div style="font-weight: bold; font-size: 1.1rem; color: #333; margin-bottom: 0.5rem;">âš ï¸ Gas ë¬¸ì„œ í•™ìŠµ ë¯¸ì™„ë£Œ</div>
                    <div style="color: #666; font-size: 0.9rem; margin-bottom: 0.3rem;">í•™ìŠµ ì™„ë£Œ ë¬¸ì„œ 0ê±´</div>
                    <div style="color: #999; font-size: 0.8rem;"></div>
                </div>
            """, unsafe_allow_html=True)
    
    # Power ì¸ë±ìŠ¤ ì¹´ë“œ
    with col2:
        if POWER_INDEX_DIR.exists() and any(POWER_INDEX_DIR.iterdir()):
            # ì €ì¥ëœ ë¬¸ì„œ ìˆ˜ ê³„ì‚°
            docs_dir = Path(__file__).parent.parent / "vectordb" / "docs"
            power_docs = [f for f in docs_dir.iterdir() if f.is_file() and classify_file(f.name) == 'power'] if docs_dir.exists() else []
            doc_count = len(power_docs)
            
            # ì‹¤ì œ ë§ˆì§€ë§‰ ì—…ë°ì´íŠ¸ ë‚ ì§œ ê³„ì‚°
            import datetime
            index_files = list(POWER_INDEX_DIR.iterdir())
            if index_files:
                latest_time = max(f.stat().st_mtime for f in index_files)
                last_update = datetime.datetime.fromtimestamp(latest_time).strftime("%Y-%m-%d")
            else:
                last_update = "ì•Œ ìˆ˜ ì—†ìŒ"
            
            st.markdown(f"""
                <div style="background-color: #f8f9fa; padding: 1.5rem; border-radius: 10px; border: 1px solid #e9ecef; text-align: center;">
                    <div style="font-weight: bold; font-size: 1.1rem; color: #333; margin-bottom: 0.5rem;">âœ… Power ë¬¸ì„œ í•™ìŠµ ì™„ë£Œ</div>
                    <div style="color: #666; font-size: 0.9rem; margin-bottom: 0.3rem;">í•™ìŠµ ì™„ë£Œ ë¬¸ì„œ {doc_count}ê±´</div>
                    <div style="color: #999; font-size: 0.8rem;">ë§ˆì§€ë§‰ ì—…ë°ì´íŠ¸: {last_update}</div>
                </div>
            """, unsafe_allow_html=True)
        else:
            st.markdown(f"""
                <div style="background-color: #f8f9fa; padding: 1.5rem; border-radius: 10px; border: 1px solid #e9ecef; text-align: center;">
                    <div style="font-weight: bold; font-size: 1.1rem; color: #333; margin-bottom: 0.5rem;">âš ï¸ Power ë¬¸ì„œ í•™ìŠµ ë¯¸ì™„ë£Œ</div>
                    <div style="color: #666; font-size: 0.9rem; margin-bottom: 0.3rem;">í•™ìŠµ ì™„ë£Œ ë¬¸ì„œ 0ê±´</div>
                    <div style="color: #999; font-size: 0.8rem;"></div>
                </div>
            """, unsafe_allow_html=True)
    
    # Other ì¸ë±ìŠ¤ ì¹´ë“œ
    with col3:
        if OTHER_INDEX_DIR.exists() and any(OTHER_INDEX_DIR.iterdir()):
            # ì €ì¥ëœ ë¬¸ì„œ ìˆ˜ ê³„ì‚°
            docs_dir = Path(__file__).parent.parent / "vectordb" / "docs"
            other_docs = [f for f in docs_dir.iterdir() if f.is_file() and classify_file(f.name) == 'other'] if docs_dir.exists() else []
            doc_count = len(other_docs)
            
            # ì‹¤ì œ ë§ˆì§€ë§‰ ì—…ë°ì´íŠ¸ ë‚ ì§œ ê³„ì‚°
            import datetime
            index_files = list(OTHER_INDEX_DIR.iterdir())
            if index_files:
                latest_time = max(f.stat().st_mtime for f in index_files)
                last_update = datetime.datetime.fromtimestamp(latest_time).strftime("%Y-%m-%d")
            else:
                last_update = "ì•Œ ìˆ˜ ì—†ìŒ"
            
            st.markdown(f"""
                <div style="background-color: #f8f9fa; padding: 1.5rem; border-radius: 10px; border: 1px solid #e9ecef; text-align: center;">
                    <div style="font-weight: bold; font-size: 1.1rem; color: #333; margin-bottom: 0.5rem;">âœ… Other ë¬¸ì„œ í•™ìŠµ ì™„ë£Œ</div>
                    <div style="color: #666; font-size: 0.9rem; margin-bottom: 0.3rem;">í•™ìŠµ ì™„ë£Œ ë¬¸ì„œ {doc_count}ê±´</div>
                    <div style="color: #999; font-size: 0.8rem;">ë§ˆì§€ë§‰ ì—…ë°ì´íŠ¸: {last_update}</div>
                </div>
            """, unsafe_allow_html=True)
        else:
            st.markdown(f"""
                <div style="background-color: #f8f9fa; padding: 1.5rem; border-radius: 10px; border: 1px solid #e9ecef; text-align: center;">
                    <div style="font-weight: bold; font-size: 1.1rem; color: #333; margin-bottom: 0.5rem;">âš ï¸ Other ë¬¸ì„œ í•™ìŠµ ë¯¸ì™„ë£Œ</div>
                    <div style="color: #666; font-size: 0.9rem; margin-bottom: 0.3rem;">í•™ìŠµ ì™„ë£Œ ë¬¸ì„œ 0ê±´</div>
                    <div style="color: #999; font-size: 0.8rem;"></div>
                </div>
            """, unsafe_allow_html=True)
    
    
    st.write("")  # ê³µë°± ì¶”ê°€
    
    # ì €ì¥ëœ ë¬¸ì„œ íŒŒì¼ ëª©ë¡ í‘œì‹œ
    docs_dir = Path(__file__).parent.parent / "vectordb" / "docs"
    if docs_dir.exists() and any(docs_dir.iterdir()):
        # ìˆ¨ê¹€ íŒŒì¼ ì œì™¸í•˜ê³  íŒŒì¼ ëª©ë¡ ê°€ì ¸ì˜¤ê¸°
        files = [f for f in docs_dir.iterdir() if f.is_file() and not f.name.startswith('.')]
        if files:
            # st.info(f"ğŸ“‚ ì´ {len(files)}ê°œ íŒŒì¼ì´ ì €ì¥ë˜ì–´ ìˆìŠµë‹ˆë‹¤:")
            
            # íŒŒì¼ ë°ì´í„°ë¥¼ í‘œ í˜•íƒœë¡œ ì¤€ë¹„
            file_data = []
            for file in sorted(files):
                file_size = file.stat().st_size
                file_size_kb = file_size / 1024
                category = classify_file(file.name)
                
                file_data.append({
                    "íŒŒì¼ëª…": file.name,
                    "ì‚¬ì´ì¦ˆ": f"{file_size_kb:.1f} KB" if file_size_kb < 1024 else f"{file_size_kb/1024:.2f} MB",
                    "ë¶„ë¥˜": category
                })
            
            df = pd.DataFrame(file_data)
            st.table(df)
        else:
            st.info("ğŸ“‚ ì €ì¥ëœ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤.")
    else:
        st.info("ğŸ“‚ ë¬¸ì„œ ì €ì¥ ë””ë ‰í† ë¦¬ê°€ ì—†ìŠµë‹ˆë‹¤.")

    # ìˆ˜ë™ ë¦¬ë¡œë“œ ë²„íŠ¼
    if st.button("ğŸ”„ í•™ìŠµ ë¬¸ì„œ ì •ë³´ ë¦¬ë¡œë“œ", type="secondary"):
        reload_backend_indexes()

def show_chat_page():
    """ì±„íŒ… í˜ì´ì§€ë¥¼ í‘œì‹œí•©ë‹ˆë‹¤."""
    # ë‚´ë¶€ ë¡œê³  ì˜ì—­
    st.markdown("""
        <div style="background-color:#ffffff; padding: 1rem 2rem; border-radius: 6px; margin-bottom: 1rem; display: flex; align-items: center; border: 1px solid #e5e7eb; box-shadow: 0 2px 4px rgba(0,0,0,0.04);">
            <img src="https://img.icons8.com/ios-filled/50/2d9bf0/document--v1.png" width="24px" style="margin-right: 10px;" />
            <span style="font-size: 1.3rem; font-weight: bold; color: #111827;">DocInsight AI</span>
        </div>
        <div style="color: #666666; font-size: 0.95rem; margin-bottom: 1.5rem;">
           ë‹¤ì–‘í•œ ë¬¸ì„œë¥¼ í•™ìŠµí•œ AIê°€ ì§ˆë¬¸ì— ë‹µë³€í•©ë‹ˆë‹¤.
        </div>
    """, unsafe_allow_html=True)

    display_chat_history()

    user_input = st.chat_input("ì„œìš¸ì‹œ ë„ì‹œê°€ìŠ¤ ìš”ê¸ˆ ì‚°ì • ë°©ì‹ì€?")
    if user_input:
        send_message(user_input)
        st.rerun()

def main():
    initialize_session_state()

    with st.sidebar:
        st.markdown("""
            <style>
            .sidebar-title {
                font-size: 1.3rem;
                font-weight: bold;
                margin-bottom: 1rem;
                color: white !important;
            }
            </style>
            <div class="sidebar-title">ë¬¸ì„œê´€ë¦¬</div>
        """, unsafe_allow_html=True)
        
        # API ì²˜ë¦¬ ì¤‘ì¸ì§€ í™•ì¸
        is_processing = st.session_state.get("api_processing", False)
        
        # ë¬¸ì„œì—…ë¡œë“œ ë²„íŠ¼
        upload_clicked = st.button(
            "ğŸ“‚ ë¬¸ì„œ ì—…ë¡œë“œ", 
            use_container_width=True,
            disabled=is_processing,
            key="upload_button"
        )
        
        if upload_clicked and not is_processing:
            st.session_state.current_page = "upload"
            st.rerun()
        
        # ë’¤ë¡œê°€ê¸° ë²„íŠ¼ì€ ë¬¸ì„œ ì—…ë¡œë“œ í™”ë©´ì—ì„œë§Œ í‘œì‹œ
        if st.session_state.current_page == "upload":
            # ë¬¸ì„œ ì—…ë¡œë“œ ë²„íŠ¼ ë°”ë¡œ ì•„ë˜ì— ë’¤ë¡œê°€ê¸° ë²„íŠ¼ ë°°ì¹˜
            back_clicked = st.button(
                "â† ë’¤ë¡œê°€ê¸°", 
                use_container_width=True,
                disabled=is_processing,
                key="back_button"
            )
            
            if back_clicked and not is_processing:
                st.session_state.current_page = "chat"
                st.rerun()

    # ìƒë‹¨ ì „ì²´ í—¤ë” ì˜ì—­ ì–´ë‘¡ê²Œ ì²˜ë¦¬
    st.markdown("""
        <style>
        header[data-testid="stHeader"] {
            background-color: #111827;
        }

        section[data-testid="stSidebar"] {
            width: 187px !important;
            background-color: #111827 !important;
            color: white !important;
            overflow-y: hidden !important;  /* ì‚¬ì´ë“œë°” ì„¸ë¡œ ìŠ¤í¬ë¡¤ ë¹„í™œì„±í™” */
            overflow-x: hidden !important;  /* ì‚¬ì´ë“œë°” ê°€ë¡œ ìŠ¤í¬ë¡¤ ë¹„í™œì„±í™” */
            height: 100vh !important;       /* ì‚¬ì´ë“œë°” ë†’ì´ë¥¼ í™”ë©´ ë†’ì´ë¡œ ê³ ì • */
            max-height: 100vh !important;   /* ìµœëŒ€ ë†’ì´ ì œí•œ */
        }

        /* ì‚¬ì´ë“œë°” ë‚´ë¶€ ì»¨í…Œì´ë„ˆë„ ìŠ¤í¬ë¡¤ ë°©ì§€ */
        section[data-testid="stSidebar"] > div {
            overflow-y: hidden !important;
            overflow-x: hidden !important;
            height: 100vh !important;
            max-height: 100vh !important;
        }

        /* ì‚¬ì´ë“œë°” ë‚´ë¶€ ëª¨ë“  ìš”ì†Œë“¤ì˜ ìŠ¤í¬ë¡¤ ë°©ì§€ */
        section[data-testid="stSidebar"] * {
            overflow-y: visible !important;
            overflow-x: visible !important;
        }

        /* ì‚¬ì´ë“œë°” ë²„íŠ¼ì´ disabled ìƒíƒœì—ì„œë„ ìƒ‰ìƒ ìœ ì§€ */
        section[data-testid="stSidebar"] .stButton > button {
            background-color: #333333 !important;
            color: white !important;
            border-radius: 8px !important;
            padding: 0.5rem 1rem !important;
        }
        section[data-testid="stSidebar"] .stButton > button:hover {
            background-color: #555555 !important;
        }
        section[data-testid="stSidebar"] .stButton > button:disabled {
            background-color: #333333 !important;
            color: white !important;
            opacity: 0.6 !important;
            cursor: not-allowed !important;
        }

        /* ì‚¬ì´ë“œë°” í† ê¸€ ë²„íŠ¼ ê°•ì¡° */
        button[data-testid="collapsedControl"] {
            border: 2px solid #9ca3af !important;
            border-radius: 50% !important;
            background-color: #f3f4f6 !important;
            width: 36px !important;
            height: 36px !important;
            display: flex;
            align-items: center;
            justify-content: center;
            position: fixed;
            top: 1.25rem;
            left: 0.75rem;
            z-index: 10000;
        }
        button[data-testid="collapsedControl"] svg {
            stroke: #374151 !important;
            width: 20px !important;
            height: 20px !important;
        }
        </style>
    """, unsafe_allow_html=True)

    # í˜„ì¬ í˜ì´ì§€ì— ë”°ë¼ ë‹¤ë¥¸ ë‚´ìš© í‘œì‹œ
    if st.session_state.current_page == "upload":
        show_upload_page()
    else:
        show_chat_page()

    st.markdown("""
        <style>
        html, body, [class*="css"]  {
            font-family: 'Pretendard', sans-serif !important;
        }
        /* ë©”ì¸ ì½˜í…ì¸  ì˜ì—­ì˜ ë²„íŠ¼ë§Œ ìŠ¤íƒ€ì¼ ì ìš© (ì‚¬ì´ë“œë°” ì œì™¸) */
        .main .stButton button {
            background-color: #333333 !important;
            color: white !important;
            border-radius: 8px !important;
            padding: 0.5rem 1rem !important;
        }
        .main .stButton button:hover {
            background-color: #555555 !important;
        }
        </style>
    """, unsafe_allow_html=True)

if __name__ == "__main__":
    main()
